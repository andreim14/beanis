{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Beanis - \"Beanie for Redis\" - is an asynchronous Python object-document mapper (ODM) for Redis. Data models are based on Pydantic . When using Beanis each document has a corresponding Document class that is used to interact with Redis. In addition to retrieving data, Beanis allows you to add, update, or delete documents as well. Beanis saves you time by removing boilerplate code, and it helps you focus on the parts of your app that actually matter. Works with vanilla Redis - no RedisJSON or RediSearch modules required! Installation PIP pip install beanis Poetry poetry add beanis Quick Example import asyncio from typing import Optional from redis.asyncio import Redis from pydantic import BaseModel from beanis import Document, init_beanis class Category(BaseModel): name: str description: str class Product(Document): name: str # You can use normal types just like in pydantic description: Optional[str] = None price: float category: Category # You can include pydantic models as well stock: int = 0 class Settings: name = \"products\" # This is an asynchronous example, so we will access it from an async function async def example(): # Beanis uses Redis async client client = Redis(host=\"localhost\", port=6379, db=0, decode_responses=True) # Initialize beanis with the Product document class await init_beanis(database=client, document_models=[Product]) chocolate = Category( name=\"Chocolate\", description=\"A preparation of roasted and ground cacao seeds.\" ) # Beanis documents work just like pydantic models product = Product(name=\"Tony's Chocolonely\", price=5.95, category=chocolate, stock=100) # And can be inserted into Redis await product.insert() # You can retrieve documents by ID found = await Product.get(product.id) # Update documents await product.update(price=6.95, stock=150) # Get all products all_products = await Product.all() # Clean up await client.close() if __name__ == \"__main__\": asyncio.run(example()) Key Features \u2705 Beanie-like API - Familiar interface for MongoDB/Beanie developers \u2705 Works with vanilla Redis - No modules required \u2705 Type Safety - Full Pydantic validation \u2705 Fast - Only 8% overhead vs raw Redis \u2705 Custom Encoders - Store NumPy arrays, PyTorch tensors, any type \u2705 TTL Support - Built-in expiration \u2705 Batch Operations - Efficient pipelines \u2705 Event Hooks - Before/after insert, update, delete Documentation For detailed documentation, visit the main README or check out: Custom Encoders Guide - Store any Python type Getting Started - Complete tutorial Tutorial - Step-by-step guides Credits Beanis is inspired by Beanie - the amazing MongoDB ODM. We took the Beanie philosophy and adapted it for Redis, creating a simple yet powerful ODM that works with vanilla Redis. License Apache License 2.0","title":"Overview"},{"location":"#overview","text":"Beanis - \"Beanie for Redis\" - is an asynchronous Python object-document mapper (ODM) for Redis. Data models are based on Pydantic . When using Beanis each document has a corresponding Document class that is used to interact with Redis. In addition to retrieving data, Beanis allows you to add, update, or delete documents as well. Beanis saves you time by removing boilerplate code, and it helps you focus on the parts of your app that actually matter. Works with vanilla Redis - no RedisJSON or RediSearch modules required!","title":"Overview"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pip","text":"pip install beanis","title":"PIP"},{"location":"#poetry","text":"poetry add beanis","title":"Poetry"},{"location":"#quick-example","text":"import asyncio from typing import Optional from redis.asyncio import Redis from pydantic import BaseModel from beanis import Document, init_beanis class Category(BaseModel): name: str description: str class Product(Document): name: str # You can use normal types just like in pydantic description: Optional[str] = None price: float category: Category # You can include pydantic models as well stock: int = 0 class Settings: name = \"products\" # This is an asynchronous example, so we will access it from an async function async def example(): # Beanis uses Redis async client client = Redis(host=\"localhost\", port=6379, db=0, decode_responses=True) # Initialize beanis with the Product document class await init_beanis(database=client, document_models=[Product]) chocolate = Category( name=\"Chocolate\", description=\"A preparation of roasted and ground cacao seeds.\" ) # Beanis documents work just like pydantic models product = Product(name=\"Tony's Chocolonely\", price=5.95, category=chocolate, stock=100) # And can be inserted into Redis await product.insert() # You can retrieve documents by ID found = await Product.get(product.id) # Update documents await product.update(price=6.95, stock=150) # Get all products all_products = await Product.all() # Clean up await client.close() if __name__ == \"__main__\": asyncio.run(example())","title":"Quick Example"},{"location":"#key-features","text":"\u2705 Beanie-like API - Familiar interface for MongoDB/Beanie developers \u2705 Works with vanilla Redis - No modules required \u2705 Type Safety - Full Pydantic validation \u2705 Fast - Only 8% overhead vs raw Redis \u2705 Custom Encoders - Store NumPy arrays, PyTorch tensors, any type \u2705 TTL Support - Built-in expiration \u2705 Batch Operations - Efficient pipelines \u2705 Event Hooks - Before/after insert, update, delete","title":"Key Features"},{"location":"#documentation","text":"For detailed documentation, visit the main README or check out: Custom Encoders Guide - Store any Python type Getting Started - Complete tutorial Tutorial - Step-by-step guides","title":"Documentation"},{"location":"#credits","text":"Beanis is inspired by Beanie - the amazing MongoDB ODM. We took the Beanie philosophy and adapted it for Redis, creating a simple yet powerful ODM that works with vanilla Redis.","title":"Credits"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"changelog/","text":"Changelog Beanis - Redis ODM for Python 0.1.0 - 2025-01-15 Major Release - Complete Redis Refactor Complete refactor from MongoDB (Beanie fork) to Redis-native implementation. Core Features \u2705 Redis Hash Storage - Documents stored as Redis Hashes \u2705 Automatic Indexing - Sorted Sets for numeric fields, Sets for categorical fields \u2705 Type Safety - Full Pydantic v2 validation \u2705 Async/Await - Built on redis.asyncio \u2705 TTL Support - Built-in document expiration \u2705 Event Hooks - Before/after insert, update, delete, save Custom Encoders System Author - Claude Code Assistant Custom type encoding/decoding registry Decorator and function APIs Auto-registration for NumPy and PyTorch types Type metadata storage for runtime type resolution Example: Store NumPy arrays, PyTorch tensors, custom classes Performance Optimizations msgspec for JSON serialization (2x faster than orjson) Redis pipelines for batch operations Lazy validation - Skip validation on reads by default 8% overhead vs vanilla Redis (benchmarked) Documentation Complete tutorial system (8 core tutorials) Getting started guide Custom encoders guide Side-by-side code comparisons (vanilla Redis vs Beanis) Removed MongoDB-only features documentation API Changes init_beanis() instead of init_beanie() Redis client instead of Motor client Indexed(type) for indexable fields Removed: Link, BackLink, migrations, aggregations, views Tests 72 passing tests Comprehensive document operations tests Custom encoder tests FastAPI integration tests Migration tests (legacy, kept for reference) 0.0.8 - 2024-06-05 Initial Fork from Beanie Forked from Beanie ODM Changed package name to Beanis Updated imports and references Initial PyPI release Credits Beanis is inspired by Beanie - the amazing MongoDB ODM by Roman Right . We took the Beanie philosophy (elegant API, Pydantic models, async/await) and adapted it for Redis, creating a simple yet powerful ODM that works with vanilla Redis. Migration from Beanie If you're migrating from Beanie (MongoDB) to Beanis (Redis), here are the key changes: Initialization # Before (Beanie) from motor.motor_asyncio import AsyncIOMotorClient from beanie import init_beanie client = AsyncIOMotorClient(\"mongodb://localhost:27017\") await init_beanie(database=client.db_name, document_models=[Product]) # After (Beanis) from redis.asyncio import Redis from beanis import init_beanis client = Redis(decode_responses=True) await init_beanis(database=client, document_models=[Product]) Document Definition # Before (Beanie) from beanie import Document import pymongo class Product(Document): name: str price: Indexed(float, index_type=pymongo.DESCENDING) class Settings: name = \"products\" # After (Beanis) from beanis import Document, Indexed class Product(Document): name: str price: Indexed(float) # Sorted Set index class Settings: name = \"products\" Features Not Available \u274c Relations (Link/BackLink) - Use embedded documents \u274c Aggregation pipelines - Use Python for data processing \u274c Migrations - Not needed (schema-less) \u274c Views - Not applicable to Redis \u274c Time Series - Use Redis TimeSeries module or TTL New Features \u2705 TTL support - Document expiration \u2705 Custom encoders - Store any Python type \u2705 Atomic operations - increment_field() for counters \u2705 Performance - 8% overhead vs vanilla Redis","title":"Changelog"},{"location":"changelog/#changelog","text":"Beanis - Redis ODM for Python","title":"Changelog"},{"location":"changelog/#010-2025-01-15","text":"","title":"0.1.0 - 2025-01-15"},{"location":"changelog/#major-release-complete-redis-refactor","text":"Complete refactor from MongoDB (Beanie fork) to Redis-native implementation.","title":"Major Release - Complete Redis Refactor"},{"location":"changelog/#core-features","text":"\u2705 Redis Hash Storage - Documents stored as Redis Hashes \u2705 Automatic Indexing - Sorted Sets for numeric fields, Sets for categorical fields \u2705 Type Safety - Full Pydantic v2 validation \u2705 Async/Await - Built on redis.asyncio \u2705 TTL Support - Built-in document expiration \u2705 Event Hooks - Before/after insert, update, delete, save","title":"Core Features"},{"location":"changelog/#custom-encoders-system","text":"Author - Claude Code Assistant Custom type encoding/decoding registry Decorator and function APIs Auto-registration for NumPy and PyTorch types Type metadata storage for runtime type resolution Example: Store NumPy arrays, PyTorch tensors, custom classes","title":"Custom Encoders System"},{"location":"changelog/#performance-optimizations","text":"msgspec for JSON serialization (2x faster than orjson) Redis pipelines for batch operations Lazy validation - Skip validation on reads by default 8% overhead vs vanilla Redis (benchmarked)","title":"Performance Optimizations"},{"location":"changelog/#documentation","text":"Complete tutorial system (8 core tutorials) Getting started guide Custom encoders guide Side-by-side code comparisons (vanilla Redis vs Beanis) Removed MongoDB-only features documentation","title":"Documentation"},{"location":"changelog/#api-changes","text":"init_beanis() instead of init_beanie() Redis client instead of Motor client Indexed(type) for indexable fields Removed: Link, BackLink, migrations, aggregations, views","title":"API Changes"},{"location":"changelog/#tests","text":"72 passing tests Comprehensive document operations tests Custom encoder tests FastAPI integration tests Migration tests (legacy, kept for reference)","title":"Tests"},{"location":"changelog/#008-2024-06-05","text":"","title":"0.0.8 - 2024-06-05"},{"location":"changelog/#initial-fork-from-beanie","text":"Forked from Beanie ODM Changed package name to Beanis Updated imports and references Initial PyPI release","title":"Initial Fork from Beanie"},{"location":"changelog/#credits","text":"Beanis is inspired by Beanie - the amazing MongoDB ODM by Roman Right . We took the Beanie philosophy (elegant API, Pydantic models, async/await) and adapted it for Redis, creating a simple yet powerful ODM that works with vanilla Redis.","title":"Credits"},{"location":"changelog/#migration-from-beanie","text":"If you're migrating from Beanie (MongoDB) to Beanis (Redis), here are the key changes:","title":"Migration from Beanie"},{"location":"changelog/#initialization","text":"# Before (Beanie) from motor.motor_asyncio import AsyncIOMotorClient from beanie import init_beanie client = AsyncIOMotorClient(\"mongodb://localhost:27017\") await init_beanie(database=client.db_name, document_models=[Product]) # After (Beanis) from redis.asyncio import Redis from beanis import init_beanis client = Redis(decode_responses=True) await init_beanis(database=client, document_models=[Product])","title":"Initialization"},{"location":"changelog/#document-definition","text":"# Before (Beanie) from beanie import Document import pymongo class Product(Document): name: str price: Indexed(float, index_type=pymongo.DESCENDING) class Settings: name = \"products\" # After (Beanis) from beanis import Document, Indexed class Product(Document): name: str price: Indexed(float) # Sorted Set index class Settings: name = \"products\"","title":"Document Definition"},{"location":"changelog/#features-not-available","text":"\u274c Relations (Link/BackLink) - Use embedded documents \u274c Aggregation pipelines - Use Python for data processing \u274c Migrations - Not needed (schema-less) \u274c Views - Not applicable to Redis \u274c Time Series - Use Redis TimeSeries module or TTL","title":"Features Not Available"},{"location":"changelog/#new-features","text":"\u2705 TTL support - Document expiration \u2705 Custom encoders - Store any Python type \u2705 Atomic operations - increment_field() for counters \u2705 Performance - 8% overhead vs vanilla Redis","title":"New Features"},{"location":"code-of-conduct/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at roman-right@protonmail.com. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.","title":"Code of conduct"},{"location":"code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code-of-conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"code-of-conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code-of-conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"code-of-conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at roman-right@protonmail.com. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"code-of-conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"code-of-conduct/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"code-of-conduct/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"code-of-conduct/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"code-of-conduct/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.","title":"Attribution"},{"location":"development/","text":"Development Hopefully, you have landed here because you would like to help out with the development of Beanis. Whether through adding new features, fixing bugs, or extending documentation, your help is really appreciated! Please read this page carefully. If you have any questions, open an issue on GitHub . Also, please read the Code of Conduct . Setting up the development environment We assume you are familiar with the general forking and pull request workflow for submitting to open-source projects. If not, don't worry, there are plenty of good guides available. Maybe check out this one . All the dependencies and build configs are set in the pyproject.toml file. There are two main dependency sections there: dependencies: for the dependencies required to run Beanis test: for the dependencies required to run tests To install all required dependencies, including test dependencies, in a virtual environment, run the following command in the root directory of the Beanis project: pip install -e .[test] Redis connection To run tests and use Beanis in general, you will need an accessible Redis database. All tests assume that the database is hosted locally on port 6379 and do not require authentication. You can run Redis locally using Docker: docker run -d -p 6379:6379 redis:latest Or install Redis natively: - macOS : brew install redis && brew services start redis - Linux : sudo apt-get install redis-server && sudo service redis-server start - Windows : Use Redis on Windows Testing Beanis uses pytest for unit testing. To ensure the stability of Beanis, each added feature must be tested in a separate unit test, even if it looks like other tests are covering it now. This strategy guarantees that: All the features will be covered and stay covered. There is independence from other features and test cases. To run the test suite, make sure that you have Redis running and run pytest : pytest To run tests with coverage: pytest --cov=beanis --cov-report=html Submitting new code You can submit your changes through a pull request on GitHub. Please take into account the following sections. Use pre-commit To ensure code consistency, Beanis uses Black and Ruff through pre-commit. To set it up, run: pre-commit install This will add the pre-commit command to your git's pre-commit hooks and make sure you never forget to run these. Single commit To make the pull request reviewing easier and keep the version tree clean, your pull request should consist of a single commit. It is natural that your branch might contain multiple commits, so you will need to squash these into a single commit. Instructions can be found here or here . Add documentation Please write clear documentation for any new functionality you add. Docstrings will be converted to the API documentation, but more human-friendly documentation might also be needed! See the section below. Add tests All new features and bug fixes must include tests. Tests should: - Be isolated and independent - Cover both success and failure cases - Use descriptive names - Clean up after themselves (delete test data) Working on the documentation The documentation is located in the docs/ folder and written in Markdown. Documentation structure docs/index.md - Main documentation landing page docs/getting-started.md - Getting started guide docs/tutorial/ - Step-by-step tutorials docs/api/ - API reference (auto-generated) Regenerating API documentation The API documentation in docs/api/ is auto-generated from docstrings in the source code using pydoc-markdown . To regenerate the API documentation after updating docstrings: pydoc-markdown This will: 1. Extract docstrings from the beanis package 2. Generate markdown files in docs/build/content/api-documentation/ 3. Copy the files to docs/api/ The configuration is in pydoc-markdown.yml . If you add new modules, update this file accordingly. Preview documentation locally You can preview documentation changes by serving the docs/ folder with any static file server: # Using Python's built-in server cd docs && python -m http.server 8000 Then visit http://localhost:8000 in your browser. Project structure beanis/ \u251c\u2500\u2500 beanis/ # Main package \u2502 \u251c\u2500\u2500 odm/ # ODM core \u2502 \u2502 \u251c\u2500\u2500 documents.py # Document base class \u2502 \u2502 \u251c\u2500\u2500 fields.py # Field types \u2502 \u2502 \u251c\u2500\u2500 operators/ # Query operators \u2502 \u2502 \u251c\u2500\u2500 queries/ # Query builders \u2502 \u2502 \u2514\u2500\u2500 utils/ # Utilities (encoder, decoder) \u2502 \u2514\u2500\u2500 executors/ # Background executors \u251c\u2500\u2500 tests/ # Test suite \u2502 \u251c\u2500\u2500 odm/ # ODM tests \u2502 \u251c\u2500\u2500 migrations/ # Migration tests (legacy) \u2502 \u2514\u2500\u2500 fastapi/ # FastAPI integration tests \u251c\u2500\u2500 docs/ # Documentation \u2514\u2500\u2500 benchmarks/ # Performance benchmarks Contributing guidelines Code style Follow PEP 8 Use type hints for all functions Write docstrings for public APIs Keep functions small and focused Git commit messages Use present tense (\"Add feature\" not \"Added feature\") Use imperative mood (\"Move cursor to...\" not \"Moves cursor to...\") First line should be 50 characters or less Reference issues and pull requests liberally Example: Add custom encoder support for NumPy arrays - Register encoders/decoders for custom types - Store type metadata with encoded values - Auto-register NumPy and PyTorch types Fixes #123 Pull request process Fork the repository Create a feature branch ( git checkout -b feature/amazing-feature ) Make your changes Add tests for your changes Run tests ( pytest ) Run pre-commit checks ( pre-commit run --all-files ) Commit your changes Push to your fork ( git push origin feature/amazing-feature ) Open a pull request Performance considerations When contributing, keep these performance tips in mind: Use Redis pipelines - Batch operations when possible Minimize Redis round trips - Fetch data in bulk Profile changes - Run benchmarks for performance-critical code Avoid unnecessary validation - Skip validation on reads when safe Use msgspec - It's 2x faster than orjson for serialization Run benchmarks: python benchmarks/benchmark_vs_plain_redis.py Questions? If you have questions about contributing, please: Check existing GitHub issues Open a new issue with the question label Provide context and examples Thank you for contributing to Beanis! \ud83c\udf89","title":"Development"},{"location":"development/#development","text":"Hopefully, you have landed here because you would like to help out with the development of Beanis. Whether through adding new features, fixing bugs, or extending documentation, your help is really appreciated! Please read this page carefully. If you have any questions, open an issue on GitHub . Also, please read the Code of Conduct .","title":"Development"},{"location":"development/#setting-up-the-development-environment","text":"We assume you are familiar with the general forking and pull request workflow for submitting to open-source projects. If not, don't worry, there are plenty of good guides available. Maybe check out this one . All the dependencies and build configs are set in the pyproject.toml file. There are two main dependency sections there: dependencies: for the dependencies required to run Beanis test: for the dependencies required to run tests To install all required dependencies, including test dependencies, in a virtual environment, run the following command in the root directory of the Beanis project: pip install -e .[test]","title":"Setting up the development environment"},{"location":"development/#redis-connection","text":"To run tests and use Beanis in general, you will need an accessible Redis database. All tests assume that the database is hosted locally on port 6379 and do not require authentication. You can run Redis locally using Docker: docker run -d -p 6379:6379 redis:latest Or install Redis natively: - macOS : brew install redis && brew services start redis - Linux : sudo apt-get install redis-server && sudo service redis-server start - Windows : Use Redis on Windows","title":"Redis connection"},{"location":"development/#testing","text":"Beanis uses pytest for unit testing. To ensure the stability of Beanis, each added feature must be tested in a separate unit test, even if it looks like other tests are covering it now. This strategy guarantees that: All the features will be covered and stay covered. There is independence from other features and test cases. To run the test suite, make sure that you have Redis running and run pytest : pytest To run tests with coverage: pytest --cov=beanis --cov-report=html","title":"Testing"},{"location":"development/#submitting-new-code","text":"You can submit your changes through a pull request on GitHub. Please take into account the following sections.","title":"Submitting new code"},{"location":"development/#use-pre-commit","text":"To ensure code consistency, Beanis uses Black and Ruff through pre-commit. To set it up, run: pre-commit install This will add the pre-commit command to your git's pre-commit hooks and make sure you never forget to run these.","title":"Use pre-commit"},{"location":"development/#single-commit","text":"To make the pull request reviewing easier and keep the version tree clean, your pull request should consist of a single commit. It is natural that your branch might contain multiple commits, so you will need to squash these into a single commit. Instructions can be found here or here .","title":"Single commit"},{"location":"development/#add-documentation","text":"Please write clear documentation for any new functionality you add. Docstrings will be converted to the API documentation, but more human-friendly documentation might also be needed! See the section below.","title":"Add documentation"},{"location":"development/#add-tests","text":"All new features and bug fixes must include tests. Tests should: - Be isolated and independent - Cover both success and failure cases - Use descriptive names - Clean up after themselves (delete test data)","title":"Add tests"},{"location":"development/#working-on-the-documentation","text":"The documentation is located in the docs/ folder and written in Markdown.","title":"Working on the documentation"},{"location":"development/#documentation-structure","text":"docs/index.md - Main documentation landing page docs/getting-started.md - Getting started guide docs/tutorial/ - Step-by-step tutorials docs/api/ - API reference (auto-generated)","title":"Documentation structure"},{"location":"development/#regenerating-api-documentation","text":"The API documentation in docs/api/ is auto-generated from docstrings in the source code using pydoc-markdown . To regenerate the API documentation after updating docstrings: pydoc-markdown This will: 1. Extract docstrings from the beanis package 2. Generate markdown files in docs/build/content/api-documentation/ 3. Copy the files to docs/api/ The configuration is in pydoc-markdown.yml . If you add new modules, update this file accordingly.","title":"Regenerating API documentation"},{"location":"development/#preview-documentation-locally","text":"You can preview documentation changes by serving the docs/ folder with any static file server: # Using Python's built-in server cd docs && python -m http.server 8000 Then visit http://localhost:8000 in your browser.","title":"Preview documentation locally"},{"location":"development/#project-structure","text":"beanis/ \u251c\u2500\u2500 beanis/ # Main package \u2502 \u251c\u2500\u2500 odm/ # ODM core \u2502 \u2502 \u251c\u2500\u2500 documents.py # Document base class \u2502 \u2502 \u251c\u2500\u2500 fields.py # Field types \u2502 \u2502 \u251c\u2500\u2500 operators/ # Query operators \u2502 \u2502 \u251c\u2500\u2500 queries/ # Query builders \u2502 \u2502 \u2514\u2500\u2500 utils/ # Utilities (encoder, decoder) \u2502 \u2514\u2500\u2500 executors/ # Background executors \u251c\u2500\u2500 tests/ # Test suite \u2502 \u251c\u2500\u2500 odm/ # ODM tests \u2502 \u251c\u2500\u2500 migrations/ # Migration tests (legacy) \u2502 \u2514\u2500\u2500 fastapi/ # FastAPI integration tests \u251c\u2500\u2500 docs/ # Documentation \u2514\u2500\u2500 benchmarks/ # Performance benchmarks","title":"Project structure"},{"location":"development/#contributing-guidelines","text":"","title":"Contributing guidelines"},{"location":"development/#code-style","text":"Follow PEP 8 Use type hints for all functions Write docstrings for public APIs Keep functions small and focused","title":"Code style"},{"location":"development/#git-commit-messages","text":"Use present tense (\"Add feature\" not \"Added feature\") Use imperative mood (\"Move cursor to...\" not \"Moves cursor to...\") First line should be 50 characters or less Reference issues and pull requests liberally Example: Add custom encoder support for NumPy arrays - Register encoders/decoders for custom types - Store type metadata with encoded values - Auto-register NumPy and PyTorch types Fixes #123","title":"Git commit messages"},{"location":"development/#pull-request-process","text":"Fork the repository Create a feature branch ( git checkout -b feature/amazing-feature ) Make your changes Add tests for your changes Run tests ( pytest ) Run pre-commit checks ( pre-commit run --all-files ) Commit your changes Push to your fork ( git push origin feature/amazing-feature ) Open a pull request","title":"Pull request process"},{"location":"development/#performance-considerations","text":"When contributing, keep these performance tips in mind: Use Redis pipelines - Batch operations when possible Minimize Redis round trips - Fetch data in bulk Profile changes - Run benchmarks for performance-critical code Avoid unnecessary validation - Skip validation on reads when safe Use msgspec - It's 2x faster than orjson for serialization Run benchmarks: python benchmarks/benchmark_vs_plain_redis.py","title":"Performance considerations"},{"location":"development/#questions","text":"If you have questions about contributing, please: Check existing GitHub issues Open a new issue with the question label Provide context and examples Thank you for contributing to Beanis! \ud83c\udf89","title":"Questions?"},{"location":"getting-started/","text":"Getting Started Installing Beanis You can simply install Beanis from PyPI : PIP pip install beanis Poetry poetry add beanis Initialization Getting Beanis setup in your code is really easy: Write your database model as a Pydantic class but use beanis.Document instead of pydantic.BaseModel Initialize Redis async client Call beanis.init_beanis with the Redis client and list of Beanis models The code below should get you started and shows some of the field types that you can use with Beanis. import asyncio from typing import Optional from redis.asyncio import Redis from pydantic import BaseModel from beanis import Document, Indexed, init_beanis class Category(BaseModel): name: str description: str # This is the model that will be saved to Redis class Product(Document): name: str # You can use normal types just like in pydantic description: Optional[str] = None price: Indexed(float) # Indexed for range queries category: Category # You can include pydantic models as well stock: int = 0 class Settings: name = \"products\" # Redis key prefix async def main(): # Initialize Redis client # IMPORTANT: decode_responses=True is required! client = Redis( host=\"localhost\", port=6379, db=0, decode_responses=True ) # Initialize Beanis with the Product document await init_beanis(database=client, document_models=[Product]) # Now you can use your models! chocolate = Category( name=\"Chocolate\", description=\"A preparation of roasted and ground cacao seeds.\" ) product = Product( name=\"Tony's Chocolonely\", price=5.95, category=chocolate, stock=100 ) # Insert the document await product.insert() print(f\"Inserted product with ID: {product.id}\") # Retrieve by ID found = await Product.get(product.id) print(f\"Found: {found.name} - ${found.price}\") # Query by price affordable = await Product.find(Product.price < 10.0).to_list() print(f\"Affordable products: {len(affordable)}\") # Update await product.update(price=6.95, stock=150) print(f\"Updated price to ${product.price}\") # Get all products all_products = await Product.all() print(f\"Total products: {len(all_products)}\") # Delete await product.delete_self() print(\"Product deleted\") # Clean up await client.close() if __name__ == \"__main__\": asyncio.run(main()) What's Next? Now that you have Beanis set up, you can explore: Defining Documents - Learn about document models Insert Operations - How to create documents Find Operations - How to query documents Update Operations - How to modify documents Custom Encoders - Store NumPy arrays, custom types Key Differences from Beanie (MongoDB) If you're coming from Beanie, here are the key differences: Redis client instead of Motor : Use redis.asyncio.Redis instead of motor.AsyncIOMotorClient decode_responses=True required : Redis needs this to return strings instead of bytes Hash storage : Documents are stored as Redis Hashes, not MongoDB documents Indexed fields : Uses Redis Sorted Sets/Sets for indexes, not MongoDB indexes No aggregation pipelines : Use Redis native operations or query methods Custom encoders : Built-in system for storing complex types like NumPy arrays Common Patterns Insert with TTL # Document expires after 1 hour await product.insert(ttl=3600) Batch Operations # Insert many documents efficiently products = [ Product(name=f\"Product {i}\", price=float(i * 10)) for i in range(100) ] await Product.insert_many(products) # Get many documents ids = [p.id for p in products] found = await Product.get_many(ids) Event Hooks from beanis import before_event, after_event, Insert, Update class Product(Document): name: str price: float @before_event(Insert) async def validate_price(self): if self.price < 0: raise ValueError(\"Price cannot be negative\") @after_event(Insert) async def log_creation(self): print(f\"Created product: {self.name}\") Troubleshooting \"All keys must be strings\" error Make sure you set decode_responses=True when creating the Redis client: client = Redis(decode_responses=True) # \u2705 Correct client = Redis() # \u274c Will cause errors Type validation errors Beanis validates data using Pydantic. If you get validation errors, check that your data matches the model: # \u274c This will fail - price must be float product = Product(name=\"Test\", price=\"not a number\") # \u2705 This works product = Product(name=\"Test\", price=9.99) Performance tips Use batch operations ( insert_many , get_many ) for multiple documents Skip validation on reads for better performance (default behavior) Use TTL to automatically expire old data Use indexed fields for range queries Next Steps Explore the tutorial for in-depth guides Check out custom encoders for storing complex types Read the README for feature comparisons","title":"Getting started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#installing-beanis","text":"You can simply install Beanis from PyPI :","title":"Installing Beanis"},{"location":"getting-started/#pip","text":"pip install beanis","title":"PIP"},{"location":"getting-started/#poetry","text":"poetry add beanis","title":"Poetry"},{"location":"getting-started/#initialization","text":"Getting Beanis setup in your code is really easy: Write your database model as a Pydantic class but use beanis.Document instead of pydantic.BaseModel Initialize Redis async client Call beanis.init_beanis with the Redis client and list of Beanis models The code below should get you started and shows some of the field types that you can use with Beanis. import asyncio from typing import Optional from redis.asyncio import Redis from pydantic import BaseModel from beanis import Document, Indexed, init_beanis class Category(BaseModel): name: str description: str # This is the model that will be saved to Redis class Product(Document): name: str # You can use normal types just like in pydantic description: Optional[str] = None price: Indexed(float) # Indexed for range queries category: Category # You can include pydantic models as well stock: int = 0 class Settings: name = \"products\" # Redis key prefix async def main(): # Initialize Redis client # IMPORTANT: decode_responses=True is required! client = Redis( host=\"localhost\", port=6379, db=0, decode_responses=True ) # Initialize Beanis with the Product document await init_beanis(database=client, document_models=[Product]) # Now you can use your models! chocolate = Category( name=\"Chocolate\", description=\"A preparation of roasted and ground cacao seeds.\" ) product = Product( name=\"Tony's Chocolonely\", price=5.95, category=chocolate, stock=100 ) # Insert the document await product.insert() print(f\"Inserted product with ID: {product.id}\") # Retrieve by ID found = await Product.get(product.id) print(f\"Found: {found.name} - ${found.price}\") # Query by price affordable = await Product.find(Product.price < 10.0).to_list() print(f\"Affordable products: {len(affordable)}\") # Update await product.update(price=6.95, stock=150) print(f\"Updated price to ${product.price}\") # Get all products all_products = await Product.all() print(f\"Total products: {len(all_products)}\") # Delete await product.delete_self() print(\"Product deleted\") # Clean up await client.close() if __name__ == \"__main__\": asyncio.run(main())","title":"Initialization"},{"location":"getting-started/#whats-next","text":"Now that you have Beanis set up, you can explore: Defining Documents - Learn about document models Insert Operations - How to create documents Find Operations - How to query documents Update Operations - How to modify documents Custom Encoders - Store NumPy arrays, custom types","title":"What's Next?"},{"location":"getting-started/#key-differences-from-beanie-mongodb","text":"If you're coming from Beanie, here are the key differences: Redis client instead of Motor : Use redis.asyncio.Redis instead of motor.AsyncIOMotorClient decode_responses=True required : Redis needs this to return strings instead of bytes Hash storage : Documents are stored as Redis Hashes, not MongoDB documents Indexed fields : Uses Redis Sorted Sets/Sets for indexes, not MongoDB indexes No aggregation pipelines : Use Redis native operations or query methods Custom encoders : Built-in system for storing complex types like NumPy arrays","title":"Key Differences from Beanie (MongoDB)"},{"location":"getting-started/#common-patterns","text":"","title":"Common Patterns"},{"location":"getting-started/#insert-with-ttl","text":"# Document expires after 1 hour await product.insert(ttl=3600)","title":"Insert with TTL"},{"location":"getting-started/#batch-operations","text":"# Insert many documents efficiently products = [ Product(name=f\"Product {i}\", price=float(i * 10)) for i in range(100) ] await Product.insert_many(products) # Get many documents ids = [p.id for p in products] found = await Product.get_many(ids)","title":"Batch Operations"},{"location":"getting-started/#event-hooks","text":"from beanis import before_event, after_event, Insert, Update class Product(Document): name: str price: float @before_event(Insert) async def validate_price(self): if self.price < 0: raise ValueError(\"Price cannot be negative\") @after_event(Insert) async def log_creation(self): print(f\"Created product: {self.name}\")","title":"Event Hooks"},{"location":"getting-started/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/#all-keys-must-be-strings-error","text":"Make sure you set decode_responses=True when creating the Redis client: client = Redis(decode_responses=True) # \u2705 Correct client = Redis() # \u274c Will cause errors","title":"\"All keys must be strings\" error"},{"location":"getting-started/#type-validation-errors","text":"Beanis validates data using Pydantic. If you get validation errors, check that your data matches the model: # \u274c This will fail - price must be float product = Product(name=\"Test\", price=\"not a number\") # \u2705 This works product = Product(name=\"Test\", price=9.99)","title":"Type validation errors"},{"location":"getting-started/#performance-tips","text":"Use batch operations ( insert_many , get_many ) for multiple documents Skip validation on reads for better performance (default behavior) Use TTL to automatically expire old data Use indexed fields for range queries","title":"Performance tips"},{"location":"getting-started/#next-steps","text":"Explore the tutorial for in-depth guides Check out custom encoders for storing complex types Read the README for feature comparisons","title":"Next Steps"},{"location":"api-documentation/actions/","text":"beanis.odm.actions ActionRegistry class ActionRegistry() ActionRegistry.add_action @classmethod def add_action(cls, document_class: Type[\"Document\"], event_types: List[EventTypes], action_direction: ActionDirections, funct: Callable) Add action to the action registry Arguments : document_class : document class event_types : List[EventTypes] action_direction : ActionDirections - before or after funct : Callable - function ActionRegistry.get_action_list @classmethod def get_action_list(cls, document_class: Type[\"Document\"], event_type: EventTypes, action_direction: ActionDirections) -> List[Callable] Get stored action list Arguments : document_class : Type - document class event_type : EventTypes - type of needed event action_direction : ActionDirections - before or after Returns : List[Callable] - list of stored methods ActionRegistry.run_actions @classmethod async def run_actions(cls, instance: \"Document\", event_type: EventTypes, action_direction: ActionDirections, exclude: List[Union[ActionDirections, str]]) Run actions Arguments : instance : Document - object of the Document subclass event_type : EventTypes - event types action_direction : ActionDirections - before or after register_action def register_action(event_types: Tuple[Union[List[EventTypes], EventTypes], ...], action_direction: ActionDirections) -> Callable[[F], F] Decorator. Base registration method. Used inside before_event and after_event Arguments : event_types : Union[List[EventTypes], EventTypes] - event types action_direction : ActionDirections - before or after before_event def before_event( *args: Union[List[EventTypes], EventTypes]) -> Callable[[F], F] Decorator. It adds action, which should run before mentioned one or many events happen Arguments : args : Union[List[EventTypes], EventTypes] - event types Returns : None after_event def after_event( *args: Union[List[EventTypes], EventTypes]) -> Callable[[F], F] Decorator. It adds action, which should run after mentioned one or many events happen Arguments : args : Union[List[EventTypes], EventTypes] - event types Returns : None wrap_with_actions def wrap_with_actions( event_type: EventTypes ) -> Callable[[\"AsyncDocMethod[DocType, P, R]\"], \"AsyncDocMethod[DocType, P, R]\"] Helper function to wrap Document methods with before and after event listeners Arguments : event_type : EventTypes - event types Returns : None","title":"Actions"},{"location":"api-documentation/actions/#beanisodmactions","text":"","title":"beanis.odm.actions"},{"location":"api-documentation/actions/#actionregistry","text":"class ActionRegistry()","title":"ActionRegistry"},{"location":"api-documentation/actions/#actionregistryadd_action","text":"@classmethod def add_action(cls, document_class: Type[\"Document\"], event_types: List[EventTypes], action_direction: ActionDirections, funct: Callable) Add action to the action registry Arguments : document_class : document class event_types : List[EventTypes] action_direction : ActionDirections - before or after funct : Callable - function","title":"ActionRegistry.add_action"},{"location":"api-documentation/actions/#actionregistryget_action_list","text":"@classmethod def get_action_list(cls, document_class: Type[\"Document\"], event_type: EventTypes, action_direction: ActionDirections) -> List[Callable] Get stored action list Arguments : document_class : Type - document class event_type : EventTypes - type of needed event action_direction : ActionDirections - before or after Returns : List[Callable] - list of stored methods","title":"ActionRegistry.get_action_list"},{"location":"api-documentation/actions/#actionregistryrun_actions","text":"@classmethod async def run_actions(cls, instance: \"Document\", event_type: EventTypes, action_direction: ActionDirections, exclude: List[Union[ActionDirections, str]]) Run actions Arguments : instance : Document - object of the Document subclass event_type : EventTypes - event types action_direction : ActionDirections - before or after","title":"ActionRegistry.run_actions"},{"location":"api-documentation/actions/#register_action","text":"def register_action(event_types: Tuple[Union[List[EventTypes], EventTypes], ...], action_direction: ActionDirections) -> Callable[[F], F] Decorator. Base registration method. Used inside before_event and after_event Arguments : event_types : Union[List[EventTypes], EventTypes] - event types action_direction : ActionDirections - before or after","title":"register_action"},{"location":"api-documentation/actions/#before_event","text":"def before_event( *args: Union[List[EventTypes], EventTypes]) -> Callable[[F], F] Decorator. It adds action, which should run before mentioned one or many events happen Arguments : args : Union[List[EventTypes], EventTypes] - event types Returns : None","title":"before_event"},{"location":"api-documentation/actions/#after_event","text":"def after_event( *args: Union[List[EventTypes], EventTypes]) -> Callable[[F], F] Decorator. It adds action, which should run after mentioned one or many events happen Arguments : args : Union[List[EventTypes], EventTypes] - event types Returns : None","title":"after_event"},{"location":"api-documentation/actions/#wrap_with_actions","text":"def wrap_with_actions( event_type: EventTypes ) -> Callable[[\"AsyncDocMethod[DocType, P, R]\"], \"AsyncDocMethod[DocType, P, R]\"] Helper function to wrap Document methods with before and after event listeners Arguments : event_type : EventTypes - event types Returns : None","title":"wrap_with_actions"},{"location":"api-documentation/custom-encoders/","text":"beanis.odm.custom_encoders Custom encoder/decoder registration system for Beanis Allows users to register custom serialization logic for any Python type. beanis.odm.custom_encoders.EncoderFunc Converts object to string for Redis beanis.odm.custom_encoders.DecoderFunc Converts string from Redis to object CustomEncoderRegistry class CustomEncoderRegistry() Global registry for custom type encoders/decoders Example usage: from beanis import register_encoder import numpy as np import base64 import pickle @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return base64.b64encode(pickle.dumps(arr)).decode('utf-8') @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: return pickle.loads(base64.b64decode(data)) CustomEncoderRegistry.register_encoder @classmethod def register_encoder(cls, type_: Type, encoder: EncoderFunc) -> None Register an encoder for a specific type Arguments : type_ - The Python type to encode encoder - Function that converts the type to a string CustomEncoderRegistry.register_decoder @classmethod def register_decoder(cls, type_: Type, decoder: DecoderFunc) -> None Register a decoder for a specific type Arguments : type_ - The Python type to decode decoder - Function that converts a string to the type CustomEncoderRegistry.register_pair @classmethod def register_pair(cls, type_: Type, encoder: EncoderFunc, decoder: DecoderFunc) -> None Register both encoder and decoder for a type Arguments : type_ - The Python type encoder - Function that converts the type to a string decoder - Function that converts a string to the type CustomEncoderRegistry.get_encoder @classmethod def get_encoder(cls, type_: Type) -> Optional[EncoderFunc] Get encoder for a type, or None if not registered CustomEncoderRegistry.get_decoder @classmethod def get_decoder(cls, type_: Type) -> Optional[DecoderFunc] Get decoder for a type, or None if not registered CustomEncoderRegistry.clear @classmethod def clear(cls) -> None Clear all registered encoders/decoders (mainly for testing) register_encoder def register_encoder(type_: Type) -> Callable[[EncoderFunc], EncoderFunc] Decorator to register a custom encoder for a type Example : @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return base64.b64encode(pickle.dumps(arr)).decode('utf-8') register_decoder def register_decoder(type_: Type) -> Callable[[DecoderFunc], DecoderFunc] Decorator to register a custom decoder for a type Example : @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: return pickle.loads(base64.b64decode(data)) register_type def register_type(type_: Type, encoder: EncoderFunc, decoder: DecoderFunc) -> None Register both encoder and decoder for a type (non-decorator version) Example : register_type( np.ndarray, encoder=lambda arr: base64.b64encode(pickle.dumps(arr)).decode(), decoder=lambda s: pickle.loads(base64.b64decode(s)) )","title":"Custom Encoders"},{"location":"api-documentation/custom-encoders/#beanisodmcustom_encoders","text":"Custom encoder/decoder registration system for Beanis Allows users to register custom serialization logic for any Python type.","title":"beanis.odm.custom_encoders"},{"location":"api-documentation/custom-encoders/#beanisodmcustom_encodersencoderfunc","text":"Converts object to string for Redis","title":"beanis.odm.custom_encoders.EncoderFunc"},{"location":"api-documentation/custom-encoders/#beanisodmcustom_encodersdecoderfunc","text":"Converts string from Redis to object","title":"beanis.odm.custom_encoders.DecoderFunc"},{"location":"api-documentation/custom-encoders/#customencoderregistry","text":"class CustomEncoderRegistry() Global registry for custom type encoders/decoders Example usage: from beanis import register_encoder import numpy as np import base64 import pickle @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return base64.b64encode(pickle.dumps(arr)).decode('utf-8') @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: return pickle.loads(base64.b64decode(data))","title":"CustomEncoderRegistry"},{"location":"api-documentation/custom-encoders/#customencoderregistryregister_encoder","text":"@classmethod def register_encoder(cls, type_: Type, encoder: EncoderFunc) -> None Register an encoder for a specific type Arguments : type_ - The Python type to encode encoder - Function that converts the type to a string","title":"CustomEncoderRegistry.register_encoder"},{"location":"api-documentation/custom-encoders/#customencoderregistryregister_decoder","text":"@classmethod def register_decoder(cls, type_: Type, decoder: DecoderFunc) -> None Register a decoder for a specific type Arguments : type_ - The Python type to decode decoder - Function that converts a string to the type","title":"CustomEncoderRegistry.register_decoder"},{"location":"api-documentation/custom-encoders/#customencoderregistryregister_pair","text":"@classmethod def register_pair(cls, type_: Type, encoder: EncoderFunc, decoder: DecoderFunc) -> None Register both encoder and decoder for a type Arguments : type_ - The Python type encoder - Function that converts the type to a string decoder - Function that converts a string to the type","title":"CustomEncoderRegistry.register_pair"},{"location":"api-documentation/custom-encoders/#customencoderregistryget_encoder","text":"@classmethod def get_encoder(cls, type_: Type) -> Optional[EncoderFunc] Get encoder for a type, or None if not registered","title":"CustomEncoderRegistry.get_encoder"},{"location":"api-documentation/custom-encoders/#customencoderregistryget_decoder","text":"@classmethod def get_decoder(cls, type_: Type) -> Optional[DecoderFunc] Get decoder for a type, or None if not registered","title":"CustomEncoderRegistry.get_decoder"},{"location":"api-documentation/custom-encoders/#customencoderregistryclear","text":"@classmethod def clear(cls) -> None Clear all registered encoders/decoders (mainly for testing)","title":"CustomEncoderRegistry.clear"},{"location":"api-documentation/custom-encoders/#register_encoder","text":"def register_encoder(type_: Type) -> Callable[[EncoderFunc], EncoderFunc] Decorator to register a custom encoder for a type Example : @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return base64.b64encode(pickle.dumps(arr)).decode('utf-8')","title":"register_encoder"},{"location":"api-documentation/custom-encoders/#register_decoder","text":"def register_decoder(type_: Type) -> Callable[[DecoderFunc], DecoderFunc] Decorator to register a custom decoder for a type Example : @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: return pickle.loads(base64.b64decode(data))","title":"register_decoder"},{"location":"api-documentation/custom-encoders/#register_type","text":"def register_type(type_: Type, encoder: EncoderFunc, decoder: DecoderFunc) -> None Register both encoder and decoder for a type (non-decorator version) Example : register_type( np.ndarray, encoder=lambda arr: base64.b64encode(pickle.dumps(arr)).decode(), decoder=lambda s: pickle.loads(base64.b64decode(s)) )","title":"register_type"},{"location":"api-documentation/custom-types/","text":"beanis.odm.custom_types.decimal","title":"Custom Types"},{"location":"api-documentation/custom-types/#beanisodmcustom_typesdecimal","text":"","title":"beanis.odm.custom_types.decimal"},{"location":"api-documentation/document/","text":"beanis.odm.documents Document class Document(LazyModel, SettersInterface, InheritanceInterface, OtherGettersInterface) Document Mapping class for Redis. Uses Redis Hashes for storage by default, with support for secondary indexes, TTL, and batch operations. Document.get @classmethod async def get(cls: Type[\"DocType\"], document_id: Any) -> Optional[\"DocType\"] Get document by id, returns None if document does not exist Arguments : document_id : str - document id Returns : Union[\"Document\", None] Document.exists @classmethod async def exists(cls: Type[\"DocType\"], document_id: Any) -> bool Check if a document exists by ID Arguments : document_id : str - document id Returns : bool Document.insert @wrap_with_actions(EventTypes.INSERT) async def insert(ttl: Optional[int] = None, skip_actions=None) -> DocType Insert the document (self) to Redis Arguments : ttl : Optional[int] - TTL in seconds skip_actions : Optional[List] - actions to skip Returns : Document Document.insert_one @classmethod async def insert_one(cls: Type[DocType], document: DocType, ttl: Optional[int] = None) -> Optional[DocType] Insert one document to Redis Arguments : document : Document - document to insert ttl : Optional[int] - TTL in seconds Returns : DocType Document.insert_many @classmethod async def insert_many(cls: Type[DocType], documents: Iterable[DocType], ttl: Optional[int] = None) -> List[DocType] Insert many documents to Redis using pipeline Arguments : documents : List[\"Document\"] - documents to insert ttl : Optional[int] - TTL in seconds for all documents Returns : List[DocType] Document.get_many @classmethod async def get_many(cls: Type[DocType], document_ids: List[Any]) -> List[Optional[DocType]] Get many documents by IDs using pipeline Optimized with msgspec (2x faster than orjson) and model_construct() (skip validation) Performance: 3-4x faster than orjson + model_validate approach Arguments : document_ids : List[str] - list of document IDs Returns : List[Optional[DocType]] Document.save def save() -> DocType Update an existing model in Redis or insert it if it does not yet exist. Returns : Document Document.update @wrap_with_actions(EventTypes.UPDATE) async def update(skip_actions=None, **fields) -> DocType Update specific fields of the document Arguments : skip_actions : Optional[List] - actions to skip fields : Field names and values to update Returns : Document Document.get_field async def get_field(field_name: str) -> Any Get a specific field value from Redis without loading the entire document Arguments : field_name : Name of the field Returns : Field value Document.set_field async def set_field(field_name: str, value: Any) -> None Set a specific field value in Redis Arguments : field_name : Name of the field value : Value to set Document.increment_field async def increment_field(field_name: str, amount: Union[int, float] = 1) -> Union[int, float] Increment a numeric field atomically Arguments : field_name : Name of the field amount : Amount to increment by Returns : New value Document.set_ttl async def set_ttl(seconds: int) -> bool Set TTL (time to live) for this document Arguments : seconds : TTL in seconds Returns : bool - True if TTL was set Document.get_ttl async def get_ttl() -> Optional[int] Get the remaining TTL for this document Returns : Optional[int] - TTL in seconds, -1 if no TTL, -2 if key doesn't exist Document.persist async def persist() -> bool Remove TTL from this document (make it persistent) Returns : bool - True if TTL was removed Document.delete_self @wrap_with_actions(EventTypes.DELETE) async def delete_self(skip_actions=None) Delete the document Document.delete @classmethod async def delete(cls, document_id) Delete a document by ID Arguments : document_id : str - document id Document.delete_many @classmethod async def delete_many(cls, document_ids: List[Any]) -> int Delete many documents by IDs Arguments : document_ids : List[str] - list of document IDs Returns : int - number of documents deleted Document.delete_all @classmethod async def delete_all(cls) -> int Delete all documents of this class Returns : int - number of documents deleted Document.count @classmethod async def count(cls) -> int Count all documents of this class Returns : int - number of documents Document.find @classmethod async def find(cls: Type[DocType], **filters) -> List[DocType] Find documents by indexed fields Examples: # Exact match on indexed field products = await Product.find(category=\"electronics\") # Range query on numeric indexed field products = await Product.find(price__gte=10, price__lte=100) Arguments : filters : Field filters (supports __gte, __lte for numeric fields) Returns : List[DocType] Document.all @classmethod async def all(cls: Type[DocType], skip: int = 0, limit: Optional[int] = None, sort_desc: bool = False) -> List[DocType] Get all documents of this class Arguments : skip : Number of documents to skip limit : Maximum number of documents to return sort_desc : Sort by insertion time descending Returns : List[DocType] Document.use_state_management @classmethod def use_state_management(cls) -> bool Is state management turned on Returns : bool Document.state_management_save_previous @classmethod def state_management_save_previous(cls) -> bool Should we save the previous state after a commit to database Returns : bool Document.state_management_replace_objects @classmethod def state_management_replace_objects(cls) -> bool Should objects be replaced when using state management Returns : bool Document.get_saved_state def get_saved_state() -> Optional[Dict[str, Any]] Saved state getter. It is protected property. Returns : Optional[Dict[str, Any]] - saved state Document.get_previous_saved_state def get_previous_saved_state() -> Optional[Dict[str, Any]] Previous state getter. It is a protected property. Returns : Optional[Dict[str, Any]] - previous state Document.get_settings @classmethod def get_settings(cls) -> ItemSettings Get document settings, which was created on the initialization step Returns : ItemSettings class","title":"Document"},{"location":"api-documentation/document/#beanisodmdocuments","text":"","title":"beanis.odm.documents"},{"location":"api-documentation/document/#document","text":"class Document(LazyModel, SettersInterface, InheritanceInterface, OtherGettersInterface) Document Mapping class for Redis. Uses Redis Hashes for storage by default, with support for secondary indexes, TTL, and batch operations.","title":"Document"},{"location":"api-documentation/document/#documentget","text":"@classmethod async def get(cls: Type[\"DocType\"], document_id: Any) -> Optional[\"DocType\"] Get document by id, returns None if document does not exist Arguments : document_id : str - document id Returns : Union[\"Document\", None]","title":"Document.get"},{"location":"api-documentation/document/#documentexists","text":"@classmethod async def exists(cls: Type[\"DocType\"], document_id: Any) -> bool Check if a document exists by ID Arguments : document_id : str - document id Returns : bool","title":"Document.exists"},{"location":"api-documentation/document/#documentinsert","text":"@wrap_with_actions(EventTypes.INSERT) async def insert(ttl: Optional[int] = None, skip_actions=None) -> DocType Insert the document (self) to Redis Arguments : ttl : Optional[int] - TTL in seconds skip_actions : Optional[List] - actions to skip Returns : Document","title":"Document.insert"},{"location":"api-documentation/document/#documentinsert_one","text":"@classmethod async def insert_one(cls: Type[DocType], document: DocType, ttl: Optional[int] = None) -> Optional[DocType] Insert one document to Redis Arguments : document : Document - document to insert ttl : Optional[int] - TTL in seconds Returns : DocType","title":"Document.insert_one"},{"location":"api-documentation/document/#documentinsert_many","text":"@classmethod async def insert_many(cls: Type[DocType], documents: Iterable[DocType], ttl: Optional[int] = None) -> List[DocType] Insert many documents to Redis using pipeline Arguments : documents : List[\"Document\"] - documents to insert ttl : Optional[int] - TTL in seconds for all documents Returns : List[DocType]","title":"Document.insert_many"},{"location":"api-documentation/document/#documentget_many","text":"@classmethod async def get_many(cls: Type[DocType], document_ids: List[Any]) -> List[Optional[DocType]] Get many documents by IDs using pipeline Optimized with msgspec (2x faster than orjson) and model_construct() (skip validation) Performance: 3-4x faster than orjson + model_validate approach Arguments : document_ids : List[str] - list of document IDs Returns : List[Optional[DocType]]","title":"Document.get_many"},{"location":"api-documentation/document/#documentsave","text":"def save() -> DocType Update an existing model in Redis or insert it if it does not yet exist. Returns : Document","title":"Document.save"},{"location":"api-documentation/document/#documentupdate","text":"@wrap_with_actions(EventTypes.UPDATE) async def update(skip_actions=None, **fields) -> DocType Update specific fields of the document Arguments : skip_actions : Optional[List] - actions to skip fields : Field names and values to update Returns : Document","title":"Document.update"},{"location":"api-documentation/document/#documentget_field","text":"async def get_field(field_name: str) -> Any Get a specific field value from Redis without loading the entire document Arguments : field_name : Name of the field Returns : Field value","title":"Document.get_field"},{"location":"api-documentation/document/#documentset_field","text":"async def set_field(field_name: str, value: Any) -> None Set a specific field value in Redis Arguments : field_name : Name of the field value : Value to set","title":"Document.set_field"},{"location":"api-documentation/document/#documentincrement_field","text":"async def increment_field(field_name: str, amount: Union[int, float] = 1) -> Union[int, float] Increment a numeric field atomically Arguments : field_name : Name of the field amount : Amount to increment by Returns : New value","title":"Document.increment_field"},{"location":"api-documentation/document/#documentset_ttl","text":"async def set_ttl(seconds: int) -> bool Set TTL (time to live) for this document Arguments : seconds : TTL in seconds Returns : bool - True if TTL was set","title":"Document.set_ttl"},{"location":"api-documentation/document/#documentget_ttl","text":"async def get_ttl() -> Optional[int] Get the remaining TTL for this document Returns : Optional[int] - TTL in seconds, -1 if no TTL, -2 if key doesn't exist","title":"Document.get_ttl"},{"location":"api-documentation/document/#documentpersist","text":"async def persist() -> bool Remove TTL from this document (make it persistent) Returns : bool - True if TTL was removed","title":"Document.persist"},{"location":"api-documentation/document/#documentdelete_self","text":"@wrap_with_actions(EventTypes.DELETE) async def delete_self(skip_actions=None) Delete the document","title":"Document.delete_self"},{"location":"api-documentation/document/#documentdelete","text":"@classmethod async def delete(cls, document_id) Delete a document by ID Arguments : document_id : str - document id","title":"Document.delete"},{"location":"api-documentation/document/#documentdelete_many","text":"@classmethod async def delete_many(cls, document_ids: List[Any]) -> int Delete many documents by IDs Arguments : document_ids : List[str] - list of document IDs Returns : int - number of documents deleted","title":"Document.delete_many"},{"location":"api-documentation/document/#documentdelete_all","text":"@classmethod async def delete_all(cls) -> int Delete all documents of this class Returns : int - number of documents deleted","title":"Document.delete_all"},{"location":"api-documentation/document/#documentcount","text":"@classmethod async def count(cls) -> int Count all documents of this class Returns : int - number of documents","title":"Document.count"},{"location":"api-documentation/document/#documentfind","text":"@classmethod async def find(cls: Type[DocType], **filters) -> List[DocType] Find documents by indexed fields Examples: # Exact match on indexed field products = await Product.find(category=\"electronics\") # Range query on numeric indexed field products = await Product.find(price__gte=10, price__lte=100) Arguments : filters : Field filters (supports __gte, __lte for numeric fields) Returns : List[DocType]","title":"Document.find"},{"location":"api-documentation/document/#documentall","text":"@classmethod async def all(cls: Type[DocType], skip: int = 0, limit: Optional[int] = None, sort_desc: bool = False) -> List[DocType] Get all documents of this class Arguments : skip : Number of documents to skip limit : Maximum number of documents to return sort_desc : Sort by insertion time descending Returns : List[DocType]","title":"Document.all"},{"location":"api-documentation/document/#documentuse_state_management","text":"@classmethod def use_state_management(cls) -> bool Is state management turned on Returns : bool","title":"Document.use_state_management"},{"location":"api-documentation/document/#documentstate_management_save_previous","text":"@classmethod def state_management_save_previous(cls) -> bool Should we save the previous state after a commit to database Returns : bool","title":"Document.state_management_save_previous"},{"location":"api-documentation/document/#documentstate_management_replace_objects","text":"@classmethod def state_management_replace_objects(cls) -> bool Should objects be replaced when using state management Returns : bool","title":"Document.state_management_replace_objects"},{"location":"api-documentation/document/#documentget_saved_state","text":"def get_saved_state() -> Optional[Dict[str, Any]] Saved state getter. It is protected property. Returns : Optional[Dict[str, Any]] - saved state","title":"Document.get_saved_state"},{"location":"api-documentation/document/#documentget_previous_saved_state","text":"def get_previous_saved_state() -> Optional[Dict[str, Any]] Previous state getter. It is a protected property. Returns : Optional[Dict[str, Any]] - previous state","title":"Document.get_previous_saved_state"},{"location":"api-documentation/document/#documentget_settings","text":"@classmethod def get_settings(cls) -> ItemSettings Get document settings, which was created on the initialization step Returns : ItemSettings class","title":"Document.get_settings"},{"location":"api-documentation/fields/","text":"beanis.odm.fields ExpressionField class ExpressionField(str) Simple field expression for Redis ODM Removed query operator support (use indexing instead) ExpressionField.__getitem__ def __getitem__(item) Get sub field Arguments : item : name of the subfield Returns : ExpressionField ExpressionField.__getattr__ def __getattr__(item) Get sub field Arguments : item : name of the subfield Returns : ExpressionField","title":"Fields"},{"location":"api-documentation/fields/#beanisodmfields","text":"","title":"beanis.odm.fields"},{"location":"api-documentation/fields/#expressionfield","text":"class ExpressionField(str) Simple field expression for Redis ODM Removed query operator support (use indexing instead)","title":"ExpressionField"},{"location":"api-documentation/fields/#expressionfield__getitem__","text":"def __getitem__(item) Get sub field Arguments : item : name of the subfield Returns : ExpressionField","title":"ExpressionField.__getitem__"},{"location":"api-documentation/fields/#expressionfield__getattr__","text":"def __getattr__(item) Get sub field Arguments : item : name of the subfield Returns : ExpressionField","title":"ExpressionField.__getattr__"},{"location":"api-documentation/indexes/","text":"beanis.odm.indexes Redis-based indexing system using Sets and Sorted Sets This module provides secondary indexing capabilities for Beanis documents using native Redis data structures: - Sets for categorical/string fields (exact match lookups) - Sorted Sets for numeric fields (range queries) GeoPoint class GeoPoint(BaseModel) Represents a geographic point with longitude and latitude Usage: class Store(Document): name: str location: Indexed[GeoPoint] # Geo index store = Store(name=\"HQ\", location=GeoPoint(longitude=-122.4, latitude=37.8)) GeoPoint.as_tuple def as_tuple() -> Tuple[float, float] Return (longitude, latitude) tuple for Redis GEOADD IndexType class IndexType() Types of indexes supported IndexType.SET For categorical/string fields (exact match) IndexType.SORTED_SET For numeric fields (range queries) IndexType.GEO For geo-spatial fields (location-based queries) IndexType.VECTOR For vector similarity search (embeddings) IndexedField class IndexedField() Marks a field as indexed for secondary index support Usage: class Product(Document): category: Annotated[str, IndexedField()] # Set index price: Annotated[float, IndexedField()] # Sorted Set index location: Annotated[GeoPoint, IndexedField()] # Geo index IndexedField.__init__ def __init__(index_type: Optional[str] = None) Arguments : index_type : Type of index (\"set\", \"zset\", or \"geo\"). If None, auto-detect based on field type VectorField def VectorField(dimensions: int, algorithm: str = \"HNSW\", distance_metric: str = \"COSINE\", m: int = 16, ef_construction: int = 200) -> IndexedField Helper to create a vector field for similarity search Usage: class Document(Document): embedding: Annotated[List[float], VectorField(dimensions=1024)] Arguments : dimensions - Vector dimensionality (e.g., 1024 for Jina v4) algorithm - \"HNSW\" (default) or \"FLAT\" distance_metric - \"COSINE\" (default), \"L2\", or \"IP\" (inner product) m - HNSW M parameter (connections per node) ef_construction - HNSW ef_construction parameter IndexManager class IndexManager() Manages secondary indexes for documents using Redis Sets and Sorted Sets IndexManager.get_index_key @staticmethod def get_index_key(document_class: Type, field_name: str, value: Any = None) -> str Generate Redis key for an index For Set indexes: idx:Product:category:electronics For Sorted Set indexes: idx:Product:price IndexManager.get_indexed_fields @staticmethod def get_indexed_fields(document_class: Type) -> Dict[str, IndexedField] Extract all indexed fields from a document class Returns dict: {field_name: IndexedField} IndexManager.determine_index_type @staticmethod def determine_index_type(document_class: Type, field_name: str, indexed_field: IndexedField) -> str Determine the index type based on field type Numeric types (int, float) -> Sorted Set (zset) GeoPoint types -> Geo index String/categorical types -> Set IndexManager.add_to_index @staticmethod async def add_to_index(redis_client, document_class: Type, document_id: str, field_name: str, value: Any, index_type: str) Add document ID to the appropriate index IndexManager.remove_from_index @staticmethod async def remove_from_index(redis_client, document_class: Type, document_id: str, field_name: str, value: Any, index_type: str) Remove document ID from the appropriate index IndexManager.update_indexes @staticmethod async def update_indexes(redis_client, document_class: Type, document_id: str, old_values: Optional[Dict[str, Any]], new_values: Dict[str, Any]) Update all indexes when a document changes Uses Redis pipeline for batch operations (performance optimization) Arguments : old_values : Previous field values (for removal from old indexes) new_values : New field values (for adding to new indexes) IndexManager.remove_all_indexes @staticmethod async def remove_all_indexes(redis_client, document_class: Type, document_id: str, values: Dict[str, Any]) Remove document from all indexes (for deletion) Uses Redis pipeline for batch operations (performance optimization) IndexManager.find_by_index @staticmethod async def find_by_index(redis_client, document_class: Type, field_name: str, value: Any = None, min_value: Any = None, max_value: Any = None) -> List[str] Find document IDs using an index For Set indexes (categorical): find_by_index(redis, Product, \"category\", value=\"electronics\") For Sorted Set indexes (numeric range): find_by_index(redis, Product, \"price\", min_value=10, max_value=100) IndexManager.find_by_geo_radius @staticmethod async def find_by_geo_radius(redis_client, document_class: Type, field_name: str, longitude: float, latitude: float, radius: float, unit: str = \"km\") -> List[str] Find document IDs within a radius of a geo location Arguments : field_name : Name of the geo-indexed field longitude : Center point longitude latitude : Center point latitude radius : Search radius unit : Distance unit - 'm', 'km', 'mi', 'ft' (default: 'km') Usage: nearby = await IndexManager.find_by_geo_radius( redis_client, Store, \"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" ) IndexManager.find_by_geo_radius_with_distance @staticmethod async def find_by_geo_radius_with_distance( redis_client, document_class: Type, field_name: str, longitude: float, latitude: float, radius: float, unit: str = \"km\") -> List[Tuple[str, float]] Find document IDs within a radius with their distances Returns list of (document_id, distance) tuples Usage: nearby = await IndexManager.find_by_geo_radius_with_distance( redis_client, Store, \"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" ) for doc_id, distance in nearby: print(f\"{doc_id}: {distance} km away\") IndexManager.find_by_vector_similarity @staticmethod async def find_by_vector_similarity( redis_client, document_class: Type, field_name: str, query_vector: List[float], k: int = 10, ef_runtime: Optional[int] = None) -> List[Tuple[str, float]] Find document IDs by vector similarity (KNN search) Returns list of (document_id, similarity_score) tuples sorted by similarity Usage: query_embedding = model.encode([\"search text\"])[0].tolist() results = await IndexManager.find_by_vector_similarity( redis_client, Document, \"embedding\", query_vector=query_embedding, k=5 ) for doc_id, score in results: doc = await Document.get(doc_id) print(f\"{doc.text}: {score}\") Arguments : redis_client - Redis client instance document_class - Document class to search field_name - Name of the vector field query_vector - Query vector as list of floats k - Number of results to return ef_runtime - HNSW ef_runtime parameter (optional, for tuning) Indexed def Indexed(field_type: Type, **kwargs) -> Type Helper function to create an indexed field Usage: class Product(Document): category: Indexed[str] # Set index price: Indexed[float] # Sorted Set index","title":"Indexes"},{"location":"api-documentation/indexes/#beanisodmindexes","text":"Redis-based indexing system using Sets and Sorted Sets This module provides secondary indexing capabilities for Beanis documents using native Redis data structures: - Sets for categorical/string fields (exact match lookups) - Sorted Sets for numeric fields (range queries)","title":"beanis.odm.indexes"},{"location":"api-documentation/indexes/#geopoint","text":"class GeoPoint(BaseModel) Represents a geographic point with longitude and latitude Usage: class Store(Document): name: str location: Indexed[GeoPoint] # Geo index store = Store(name=\"HQ\", location=GeoPoint(longitude=-122.4, latitude=37.8))","title":"GeoPoint"},{"location":"api-documentation/indexes/#geopointas_tuple","text":"def as_tuple() -> Tuple[float, float] Return (longitude, latitude) tuple for Redis GEOADD","title":"GeoPoint.as_tuple"},{"location":"api-documentation/indexes/#indextype","text":"class IndexType() Types of indexes supported","title":"IndexType"},{"location":"api-documentation/indexes/#indextypeset","text":"For categorical/string fields (exact match)","title":"IndexType.SET"},{"location":"api-documentation/indexes/#indextypesorted_set","text":"For numeric fields (range queries)","title":"IndexType.SORTED_SET"},{"location":"api-documentation/indexes/#indextypegeo","text":"For geo-spatial fields (location-based queries)","title":"IndexType.GEO"},{"location":"api-documentation/indexes/#indextypevector","text":"For vector similarity search (embeddings)","title":"IndexType.VECTOR"},{"location":"api-documentation/indexes/#indexedfield","text":"class IndexedField() Marks a field as indexed for secondary index support Usage: class Product(Document): category: Annotated[str, IndexedField()] # Set index price: Annotated[float, IndexedField()] # Sorted Set index location: Annotated[GeoPoint, IndexedField()] # Geo index","title":"IndexedField"},{"location":"api-documentation/indexes/#indexedfield__init__","text":"def __init__(index_type: Optional[str] = None) Arguments : index_type : Type of index (\"set\", \"zset\", or \"geo\"). If None, auto-detect based on field type","title":"IndexedField.__init__"},{"location":"api-documentation/indexes/#vectorfield","text":"def VectorField(dimensions: int, algorithm: str = \"HNSW\", distance_metric: str = \"COSINE\", m: int = 16, ef_construction: int = 200) -> IndexedField Helper to create a vector field for similarity search Usage: class Document(Document): embedding: Annotated[List[float], VectorField(dimensions=1024)] Arguments : dimensions - Vector dimensionality (e.g., 1024 for Jina v4) algorithm - \"HNSW\" (default) or \"FLAT\" distance_metric - \"COSINE\" (default), \"L2\", or \"IP\" (inner product) m - HNSW M parameter (connections per node) ef_construction - HNSW ef_construction parameter","title":"VectorField"},{"location":"api-documentation/indexes/#indexmanager","text":"class IndexManager() Manages secondary indexes for documents using Redis Sets and Sorted Sets","title":"IndexManager"},{"location":"api-documentation/indexes/#indexmanagerget_index_key","text":"@staticmethod def get_index_key(document_class: Type, field_name: str, value: Any = None) -> str Generate Redis key for an index For Set indexes: idx:Product:category:electronics For Sorted Set indexes: idx:Product:price","title":"IndexManager.get_index_key"},{"location":"api-documentation/indexes/#indexmanagerget_indexed_fields","text":"@staticmethod def get_indexed_fields(document_class: Type) -> Dict[str, IndexedField] Extract all indexed fields from a document class Returns dict: {field_name: IndexedField}","title":"IndexManager.get_indexed_fields"},{"location":"api-documentation/indexes/#indexmanagerdetermine_index_type","text":"@staticmethod def determine_index_type(document_class: Type, field_name: str, indexed_field: IndexedField) -> str Determine the index type based on field type Numeric types (int, float) -> Sorted Set (zset) GeoPoint types -> Geo index String/categorical types -> Set","title":"IndexManager.determine_index_type"},{"location":"api-documentation/indexes/#indexmanageradd_to_index","text":"@staticmethod async def add_to_index(redis_client, document_class: Type, document_id: str, field_name: str, value: Any, index_type: str) Add document ID to the appropriate index","title":"IndexManager.add_to_index"},{"location":"api-documentation/indexes/#indexmanagerremove_from_index","text":"@staticmethod async def remove_from_index(redis_client, document_class: Type, document_id: str, field_name: str, value: Any, index_type: str) Remove document ID from the appropriate index","title":"IndexManager.remove_from_index"},{"location":"api-documentation/indexes/#indexmanagerupdate_indexes","text":"@staticmethod async def update_indexes(redis_client, document_class: Type, document_id: str, old_values: Optional[Dict[str, Any]], new_values: Dict[str, Any]) Update all indexes when a document changes Uses Redis pipeline for batch operations (performance optimization) Arguments : old_values : Previous field values (for removal from old indexes) new_values : New field values (for adding to new indexes)","title":"IndexManager.update_indexes"},{"location":"api-documentation/indexes/#indexmanagerremove_all_indexes","text":"@staticmethod async def remove_all_indexes(redis_client, document_class: Type, document_id: str, values: Dict[str, Any]) Remove document from all indexes (for deletion) Uses Redis pipeline for batch operations (performance optimization)","title":"IndexManager.remove_all_indexes"},{"location":"api-documentation/indexes/#indexmanagerfind_by_index","text":"@staticmethod async def find_by_index(redis_client, document_class: Type, field_name: str, value: Any = None, min_value: Any = None, max_value: Any = None) -> List[str] Find document IDs using an index For Set indexes (categorical): find_by_index(redis, Product, \"category\", value=\"electronics\") For Sorted Set indexes (numeric range): find_by_index(redis, Product, \"price\", min_value=10, max_value=100)","title":"IndexManager.find_by_index"},{"location":"api-documentation/indexes/#indexmanagerfind_by_geo_radius","text":"@staticmethod async def find_by_geo_radius(redis_client, document_class: Type, field_name: str, longitude: float, latitude: float, radius: float, unit: str = \"km\") -> List[str] Find document IDs within a radius of a geo location Arguments : field_name : Name of the geo-indexed field longitude : Center point longitude latitude : Center point latitude radius : Search radius unit : Distance unit - 'm', 'km', 'mi', 'ft' (default: 'km') Usage: nearby = await IndexManager.find_by_geo_radius( redis_client, Store, \"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" )","title":"IndexManager.find_by_geo_radius"},{"location":"api-documentation/indexes/#indexmanagerfind_by_geo_radius_with_distance","text":"@staticmethod async def find_by_geo_radius_with_distance( redis_client, document_class: Type, field_name: str, longitude: float, latitude: float, radius: float, unit: str = \"km\") -> List[Tuple[str, float]] Find document IDs within a radius with their distances Returns list of (document_id, distance) tuples Usage: nearby = await IndexManager.find_by_geo_radius_with_distance( redis_client, Store, \"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" ) for doc_id, distance in nearby: print(f\"{doc_id}: {distance} km away\")","title":"IndexManager.find_by_geo_radius_with_distance"},{"location":"api-documentation/indexes/#indexmanagerfind_by_vector_similarity","text":"@staticmethod async def find_by_vector_similarity( redis_client, document_class: Type, field_name: str, query_vector: List[float], k: int = 10, ef_runtime: Optional[int] = None) -> List[Tuple[str, float]] Find document IDs by vector similarity (KNN search) Returns list of (document_id, similarity_score) tuples sorted by similarity Usage: query_embedding = model.encode([\"search text\"])[0].tolist() results = await IndexManager.find_by_vector_similarity( redis_client, Document, \"embedding\", query_vector=query_embedding, k=5 ) for doc_id, score in results: doc = await Document.get(doc_id) print(f\"{doc.text}: {score}\") Arguments : redis_client - Redis client instance document_class - Document class to search field_name - Name of the vector field query_vector - Query vector as list of floats k - Number of results to return ef_runtime - HNSW ef_runtime parameter (optional, for tuning)","title":"IndexManager.find_by_vector_similarity"},{"location":"api-documentation/indexes/#indexed","text":"def Indexed(field_type: Type, **kwargs) -> Type Helper function to create an indexed field Usage: class Product(Document): category: Indexed[str] # Set index price: Indexed[float] # Sorted Set index","title":"Indexed"},{"location":"api-documentation/interfaces/","text":"beanis.odm.interfaces.detector beanis.odm.interfaces.setters SettersInterface class SettersInterface() SettersInterface.set_database @classmethod def set_database(cls, database) Redis client setter SettersInterface.set_collection_name @classmethod def set_collection_name(cls, name: str) Key prefix setter (replaces collection name) beanis.odm.interfaces.clone beanis.odm.interfaces.inheritance beanis.odm.interfaces.getters OtherGettersInterface class OtherGettersInterface() OtherGettersInterface.get_redis_client @classmethod def get_redis_client(cls) -> \"Redis\" Get the Redis async client OtherGettersInterface.get_collection_name @classmethod def get_collection_name(cls) Get the key prefix (replaces collection name) OtherGettersInterface.get_bson_encoders @classmethod def get_bson_encoders(cls) Legacy method - kept for backward compatibility OtherGettersInterface.get_link_fields @classmethod def get_link_fields(cls) Legacy method - links not supported in Redis ODM","title":"Interfaces"},{"location":"api-documentation/interfaces/#beanisodminterfacesdetector","text":"","title":"beanis.odm.interfaces.detector"},{"location":"api-documentation/interfaces/#beanisodminterfacessetters","text":"","title":"beanis.odm.interfaces.setters"},{"location":"api-documentation/interfaces/#settersinterface","text":"class SettersInterface()","title":"SettersInterface"},{"location":"api-documentation/interfaces/#settersinterfaceset_database","text":"@classmethod def set_database(cls, database) Redis client setter","title":"SettersInterface.set_database"},{"location":"api-documentation/interfaces/#settersinterfaceset_collection_name","text":"@classmethod def set_collection_name(cls, name: str) Key prefix setter (replaces collection name)","title":"SettersInterface.set_collection_name"},{"location":"api-documentation/interfaces/#beanisodminterfacesclone","text":"","title":"beanis.odm.interfaces.clone"},{"location":"api-documentation/interfaces/#beanisodminterfacesinheritance","text":"","title":"beanis.odm.interfaces.inheritance"},{"location":"api-documentation/interfaces/#beanisodminterfacesgetters","text":"","title":"beanis.odm.interfaces.getters"},{"location":"api-documentation/interfaces/#othergettersinterface","text":"class OtherGettersInterface()","title":"OtherGettersInterface"},{"location":"api-documentation/interfaces/#othergettersinterfaceget_redis_client","text":"@classmethod def get_redis_client(cls) -> \"Redis\" Get the Redis async client","title":"OtherGettersInterface.get_redis_client"},{"location":"api-documentation/interfaces/#othergettersinterfaceget_collection_name","text":"@classmethod def get_collection_name(cls) Get the key prefix (replaces collection name)","title":"OtherGettersInterface.get_collection_name"},{"location":"api-documentation/interfaces/#othergettersinterfaceget_bson_encoders","text":"@classmethod def get_bson_encoders(cls) Legacy method - kept for backward compatibility","title":"OtherGettersInterface.get_bson_encoders"},{"location":"api-documentation/interfaces/#othergettersinterfaceget_link_fields","text":"@classmethod def get_link_fields(cls) Legacy method - links not supported in Redis ODM","title":"OtherGettersInterface.get_link_fields"},{"location":"api-documentation/utils/","text":"beanis.odm.utils.init Initializer class Initializer() Initializer.__init__ def __init__(database: \"Redis\" = None, document_models: Optional[List[Union[Type[\"DocType\"], str]]] = None) Beanis initializer Arguments : database : redis.asyncio.Redis - Redis async client instance document_models : List[Union[Type[DocType], str]] - model classes or strings with dot separated paths Returns : None Initializer.get_model @staticmethod def get_model(dot_path: str) -> Type[\"DocType\"] Get the model by the path in format bar.foo.Model Arguments : dot_path : str - dot seprated path to the model Returns : Type[DocType] - class of the model Initializer.init_settings def init_settings(cls: Union[Type[Document]]) Init Settings Arguments : cls : Union[Type[Document], Type[View], Type[UnionDoc]] - Class to init settings Returns : None Initializer.set_default_class_vars @staticmethod def set_default_class_vars(cls: Type[Document]) Set default class variables. Arguments : cls : Union[Type[Document], Type[View], Type[UnionDoc]] - Class to init settings Initializer.init_document_fields def init_document_fields(cls) -> None Init class fields Returns : None Initializer.init_actions @staticmethod def init_actions(cls) Init event-based actions Initializer.init_vector_indexes async def init_vector_indexes(cls) Create vector indexes for fields with VectorField Arguments : cls : Initializer.init_document_collection def init_document_collection(cls) Init Redis client for the Document-based class Arguments : cls : Initializer.init_document async def init_document(cls: Type[Document]) -> Optional[Output] Init Document-based class Arguments : cls : Initializer.init_class async def init_class(cls: Union[Type[Document]]) Init Document, View or UnionDoc based class. Arguments : cls : init_beanis async def init_beanis(database: \"Redis\" = None, document_models: Optional[List[Union[Type[Document], str]]] = None) Beanis initialization Arguments : database : redis.asyncio.Redis - Redis async client instance document_models : List[Union[Type[DocType], str]] - model classes or strings with dot separated paths Returns : None","title":"Utils"},{"location":"api-documentation/utils/#beanisodmutilsinit","text":"","title":"beanis.odm.utils.init"},{"location":"api-documentation/utils/#initializer","text":"class Initializer()","title":"Initializer"},{"location":"api-documentation/utils/#initializer__init__","text":"def __init__(database: \"Redis\" = None, document_models: Optional[List[Union[Type[\"DocType\"], str]]] = None) Beanis initializer Arguments : database : redis.asyncio.Redis - Redis async client instance document_models : List[Union[Type[DocType], str]] - model classes or strings with dot separated paths Returns : None","title":"Initializer.__init__"},{"location":"api-documentation/utils/#initializerget_model","text":"@staticmethod def get_model(dot_path: str) -> Type[\"DocType\"] Get the model by the path in format bar.foo.Model Arguments : dot_path : str - dot seprated path to the model Returns : Type[DocType] - class of the model","title":"Initializer.get_model"},{"location":"api-documentation/utils/#initializerinit_settings","text":"def init_settings(cls: Union[Type[Document]]) Init Settings Arguments : cls : Union[Type[Document], Type[View], Type[UnionDoc]] - Class to init settings Returns : None","title":"Initializer.init_settings"},{"location":"api-documentation/utils/#initializerset_default_class_vars","text":"@staticmethod def set_default_class_vars(cls: Type[Document]) Set default class variables. Arguments : cls : Union[Type[Document], Type[View], Type[UnionDoc]] - Class to init settings","title":"Initializer.set_default_class_vars"},{"location":"api-documentation/utils/#initializerinit_document_fields","text":"def init_document_fields(cls) -> None Init class fields Returns : None","title":"Initializer.init_document_fields"},{"location":"api-documentation/utils/#initializerinit_actions","text":"@staticmethod def init_actions(cls) Init event-based actions","title":"Initializer.init_actions"},{"location":"api-documentation/utils/#initializerinit_vector_indexes","text":"async def init_vector_indexes(cls) Create vector indexes for fields with VectorField Arguments : cls :","title":"Initializer.init_vector_indexes"},{"location":"api-documentation/utils/#initializerinit_document_collection","text":"def init_document_collection(cls) Init Redis client for the Document-based class Arguments : cls :","title":"Initializer.init_document_collection"},{"location":"api-documentation/utils/#initializerinit_document","text":"async def init_document(cls: Type[Document]) -> Optional[Output] Init Document-based class Arguments : cls :","title":"Initializer.init_document"},{"location":"api-documentation/utils/#initializerinit_class","text":"async def init_class(cls: Union[Type[Document]]) Init Document, View or UnionDoc based class. Arguments : cls :","title":"Initializer.init_class"},{"location":"api-documentation/utils/#init_beanis","text":"async def init_beanis(database: \"Redis\" = None, document_models: Optional[List[Union[Type[Document], str]]] = None) Beanis initialization Arguments : database : redis.asyncio.Redis - Redis async client instance document_models : List[Union[Type[DocType], str]] - model classes or strings with dot separated paths Returns : None","title":"init_beanis"},{"location":"tutorial/custom-encoders/","text":"Custom Encoders Beanis provides a custom encoder/decoder system to serialize and deserialize complex Python types to and from Redis. Overview Redis stores data as strings, but you often need to work with complex Python types like NumPy arrays, PyTorch tensors, or custom objects. Custom encoders solve this problem: Encoder - Converts Python object \u2192 Redis string Decoder - Converts Redis string \u2192 Python object Quick Start from beanis import Document, register_encoder, register_decoder import numpy as np import base64 import pickle # Register encoder @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return base64.b64encode(pickle.dumps(arr)).decode('utf-8') # Register decoder @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: return pickle.loads(base64.b64decode(data.encode('utf-8'))) # Use in document class MLModel(Document): name: str weights: np.ndarray # Automatically encoded/decoded # Works seamlessly model = MLModel(name=\"my_model\", weights=np.array([1, 2, 3])) await model.insert() # Weights encoded to string retrieved = await MLModel.get(model.id) print(retrieved.weights) # np.array([1, 2, 3]) - decoded! Why Custom Encoders? Default Serialization Beanis uses msgspec for fast JSON serialization, which handles: - Basic types: str , int , float , bool , None - Collections: list , dict , set , tuple - Pydantic models: Nested documents - Standard types: datetime , UUID , Decimal When You Need Custom Encoders Use custom encoders for: - Scientific libraries : NumPy, PyTorch, TensorFlow - Binary data : Images, audio, compressed data - Custom classes : Your own Python classes - Specialized formats : Protocol buffers, MessagePack Registration Methods Method 1: Decorator (Recommended) from beanis import register_encoder, register_decoder @register_encoder(MyType) def encode_my_type(obj: MyType) -> str: return str(obj) # Convert to string @register_decoder(MyType) def decode_my_type(data: str) -> MyType: return MyType(data) # Convert back Method 2: Function Call from beanis import register_type register_type( MyType, encoder=lambda obj: str(obj), decoder=lambda data: MyType(data) ) Method 3: Registry Class from beanis import CustomEncoderRegistry CustomEncoderRegistry.register_encoder(MyType, encode_func) CustomEncoderRegistry.register_decoder(MyType, decode_func) # Or both at once CustomEncoderRegistry.register_pair(MyType, encode_func, decode_func) Common Use Cases NumPy Arrays import numpy as np import base64 import pickle from beanis import register_type register_type( np.ndarray, encoder=lambda arr: base64.b64encode(pickle.dumps(arr)).decode('utf-8'), decoder=lambda s: pickle.loads(base64.b64decode(s.encode('utf-8'))) ) class DataScience(Document): experiment: str results: np.ndarray # Usage doc = DataScience( experiment=\"test_1\", results=np.array([[1, 2], [3, 4]]) ) await doc.insert() PyTorch Tensors import torch import base64 import pickle from beanis import register_type register_type( torch.Tensor, encoder=lambda tensor: base64.b64encode(pickle.dumps(tensor)).decode('utf-8'), decoder=lambda s: pickle.loads(base64.b64decode(s.encode('utf-8'))) ) class NeuralNet(Document): model_name: str weights: torch.Tensor # Usage weights = torch.randn(10, 10) model = NeuralNet(model_name=\"v1\", weights=weights) await model.insert() Pandas DataFrames import pandas as pd from beanis import register_type register_type( pd.DataFrame, encoder=lambda df: df.to_json(orient='split'), decoder=lambda s: pd.read_json(s, orient='split') ) class Analysis(Document): dataset_name: str data: pd.DataFrame # Usage df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) analysis = Analysis(dataset_name=\"sales\", data=df) await analysis.insert() PIL Images from PIL import Image import io import base64 from beanis import register_type def encode_image(img: Image.Image) -> str: buffer = io.BytesIO() img.save(buffer, format='PNG') return base64.b64encode(buffer.getvalue()).decode('utf-8') def decode_image(data: str) -> Image.Image: buffer = io.BytesIO(base64.b64decode(data.encode('utf-8'))) return Image.open(buffer) register_type(Image.Image, encoder=encode_image, decoder=decode_image) class ImageDoc(Document): title: str image: Image.Image # Usage img = Image.new('RGB', (100, 100), color='red') doc = ImageDoc(title=\"test\", image=img) await doc.insert() Custom Classes from dataclasses import dataclass import json from beanis import register_type @dataclass class Point: x: float y: float def to_dict(self): return {'x': self.x, 'y': self.y} @classmethod def from_dict(cls, data): return cls(**data) register_type( Point, encoder=lambda p: json.dumps(p.to_dict()), decoder=lambda s: Point.from_dict(json.loads(s)) ) class Location(Document): name: str coordinates: Point # Usage loc = Location(name=\"Home\", coordinates=Point(37.7749, -122.4194)) await loc.insert() Auto-Registration Beanis automatically registers encoders for NumPy and PyTorch if installed: # No need to register manually! import numpy as np from beanis import Document class Model(Document): weights: np.ndarray # Auto-registered # Just works model = Model(weights=np.array([1, 2, 3])) await model.insert() To disable auto-registration: # In beanis/odm/custom_encoders.py _AUTO_REGISTER = False # Set before importing Advanced Patterns Conditional Encoding from beanis import register_encoder, register_decoder @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: # Use compression for large arrays if arr.size > 1000: import zlib compressed = zlib.compress(pickle.dumps(arr)) return \"compressed:\" + base64.b64encode(compressed).decode('utf-8') else: return base64.b64encode(pickle.dumps(arr)).decode('utf-8') @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: if data.startswith(\"compressed:\"): import zlib data = data[11:] # Remove prefix compressed = base64.b64decode(data.encode('utf-8')) return pickle.loads(zlib.decompress(compressed)) else: return pickle.loads(base64.b64decode(data.encode('utf-8'))) Type Variants from typing import Union from beanis import register_encoder, register_decoder import numpy as np import torch # Can't register Union directly, so register each type @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return \"numpy:\" + base64.b64encode(pickle.dumps(arr)).decode('utf-8') @register_encoder(torch.Tensor) def encode_torch(tensor: torch.Tensor) -> str: return \"torch:\" + base64.b64encode(pickle.dumps(tensor)).decode('utf-8') # Single decoder that handles both @register_decoder(np.ndarray) def decode_array(data: str) -> np.ndarray: if data.startswith(\"numpy:\"): data = data[6:] return pickle.loads(base64.b64decode(data.encode('utf-8'))) @register_decoder(torch.Tensor) def decode_tensor(data: str) -> torch.Tensor: if data.startswith(\"torch:\"): data = data[6:] return pickle.loads(base64.b64decode(data.encode('utf-8'))) Versioned Encoding @register_encoder(MyClass) def encode_v2(obj: MyClass) -> str: data = { 'version': 2, 'data': obj.to_dict() } return json.dumps(data) @register_decoder(MyClass) def decode_versioned(data: str) -> MyClass: parsed = json.loads(data) version = parsed.get('version', 1) if version == 1: # Old format return MyClass.from_old_format(parsed) elif version == 2: # New format return MyClass.from_dict(parsed['data']) else: raise ValueError(f\"Unknown version: {version}\") Performance Considerations Encoding Speed Different serialization methods have different speeds: # Fastest - Simple JSON register_type( Point, encoder=lambda p: json.dumps([p.x, p.y]), decoder=lambda s: Point(*json.loads(s)) ) # Fast - msgspec (if available) import msgspec register_type( Point, encoder=lambda p: msgspec.json.encode([p.x, p.y]).decode(), decoder=lambda s: Point(*msgspec.json.decode(s.encode())) ) # Slower - Pickle (but handles more types) register_type( ComplexType, encoder=lambda obj: base64.b64encode(pickle.dumps(obj)).decode(), decoder=lambda s: pickle.loads(base64.b64decode(s.encode())) ) Storage Size Compression can reduce storage: import zlib @register_encoder(LargeObject) def encode_compressed(obj: LargeObject) -> str: serialized = pickle.dumps(obj) compressed = zlib.compress(serialized, level=6) return base64.b64encode(compressed).decode('utf-8') @register_decoder(LargeObject) def decode_compressed(data: str) -> LargeObject: compressed = base64.b64decode(data.encode('utf-8')) serialized = zlib.decompress(compressed) return pickle.loads(serialized) Error Handling @register_encoder(MyType) def safe_encode(obj: MyType) -> str: try: return json.dumps(obj.to_dict()) except Exception as e: # Log error or use fallback print(f\"Encoding error: {e}\") return json.dumps({'error': str(e)}) @register_decoder(MyType) def safe_decode(data: str) -> MyType: try: parsed = json.loads(data) if 'error' in parsed: raise ValueError(f\"Encoded error: {parsed['error']}\") return MyType.from_dict(parsed) except json.JSONDecodeError as e: # Handle corrupt data print(f\"Decoding error: {e}\") return MyType() # Return default Testing Encoders import pytest from beanis import register_type def test_custom_encoder(): # Register encoder register_type( Point, encoder=lambda p: f\"{p.x},{p.y}\", decoder=lambda s: Point(*map(float, s.split(','))) ) # Test encoding p = Point(1.5, 2.5) encoded = encode_point(p) assert encoded == \"1.5,2.5\" # Test decoding decoded = decode_point(encoded) assert decoded.x == 1.5 assert decoded.y == 2.5 # Test round-trip with document class Doc(Document): location: Point doc = Doc(location=Point(3.0, 4.0)) # Would need actual Redis for full test Debugging Check registered encoders: from beanis import CustomEncoderRegistry # Check if type has encoder encoder = CustomEncoderRegistry.get_encoder(MyType) print(f\"Encoder: {encoder}\") # Check if type has decoder decoder = CustomEncoderRegistry.get_decoder(MyType) print(f\"Decoder: {decoder}\") # Clear all (for testing) CustomEncoderRegistry.clear() Limitations Cannot Index Custom Types class Model(Document): weights: np.ndarray # Cannot be indexed # weights: Indexed(np.ndarray) # \u274c Won't work # Workaround: Extract searchable fields class Model(Document): weights: np.ndarray weight_size: Indexed(int) # Can be indexed @before_event(Insert) async def set_metadata(self): self.weight_size = self.weights.size Type Hints Required # Must specify type hint class Doc(Document): data: np.ndarray # \u2705 Works # data = None # \u274c Type hint required for encoder Best Practices Use standard formats - JSON over pickle when possible Version your encoders - Include version field for compatibility Handle errors gracefully - Corrupt data shouldn't crash Test round-trips - Ensure encode\u2192decode recovers original Consider compression - For large objects Document your encoders - Explain format for future maintainers Benchmark performance - Profile with realistic data Comparison with Pydantic Validators Feature Custom Encoders Pydantic Validators Purpose Serialize to Redis Validate/transform on input When runs Save/load from DB Model instantiation Type support Any Python type Pydantic-compatible types Performance Encoder-dependent Very fast Use case Complex types Input validation Next Steps Defining Documents - Using custom types in documents Indexes - Index limitations with custom types Event Hooks - Derive searchable fields from custom types API Reference - Full encoder API Examples Repository Check tests/ for more examples: - test_custom_encoders.py - Encoder tests - test_numpy_integration.py - NumPy examples - test_complex_types.py - Advanced patterns","title":"Custom encoders"},{"location":"tutorial/custom-encoders/#custom-encoders","text":"Beanis provides a custom encoder/decoder system to serialize and deserialize complex Python types to and from Redis.","title":"Custom Encoders"},{"location":"tutorial/custom-encoders/#overview","text":"Redis stores data as strings, but you often need to work with complex Python types like NumPy arrays, PyTorch tensors, or custom objects. Custom encoders solve this problem: Encoder - Converts Python object \u2192 Redis string Decoder - Converts Redis string \u2192 Python object","title":"Overview"},{"location":"tutorial/custom-encoders/#quick-start","text":"from beanis import Document, register_encoder, register_decoder import numpy as np import base64 import pickle # Register encoder @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return base64.b64encode(pickle.dumps(arr)).decode('utf-8') # Register decoder @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: return pickle.loads(base64.b64decode(data.encode('utf-8'))) # Use in document class MLModel(Document): name: str weights: np.ndarray # Automatically encoded/decoded # Works seamlessly model = MLModel(name=\"my_model\", weights=np.array([1, 2, 3])) await model.insert() # Weights encoded to string retrieved = await MLModel.get(model.id) print(retrieved.weights) # np.array([1, 2, 3]) - decoded!","title":"Quick Start"},{"location":"tutorial/custom-encoders/#why-custom-encoders","text":"","title":"Why Custom Encoders?"},{"location":"tutorial/custom-encoders/#default-serialization","text":"Beanis uses msgspec for fast JSON serialization, which handles: - Basic types: str , int , float , bool , None - Collections: list , dict , set , tuple - Pydantic models: Nested documents - Standard types: datetime , UUID , Decimal","title":"Default Serialization"},{"location":"tutorial/custom-encoders/#when-you-need-custom-encoders","text":"Use custom encoders for: - Scientific libraries : NumPy, PyTorch, TensorFlow - Binary data : Images, audio, compressed data - Custom classes : Your own Python classes - Specialized formats : Protocol buffers, MessagePack","title":"When You Need Custom Encoders"},{"location":"tutorial/custom-encoders/#registration-methods","text":"","title":"Registration Methods"},{"location":"tutorial/custom-encoders/#method-1-decorator-recommended","text":"from beanis import register_encoder, register_decoder @register_encoder(MyType) def encode_my_type(obj: MyType) -> str: return str(obj) # Convert to string @register_decoder(MyType) def decode_my_type(data: str) -> MyType: return MyType(data) # Convert back","title":"Method 1: Decorator (Recommended)"},{"location":"tutorial/custom-encoders/#method-2-function-call","text":"from beanis import register_type register_type( MyType, encoder=lambda obj: str(obj), decoder=lambda data: MyType(data) )","title":"Method 2: Function Call"},{"location":"tutorial/custom-encoders/#method-3-registry-class","text":"from beanis import CustomEncoderRegistry CustomEncoderRegistry.register_encoder(MyType, encode_func) CustomEncoderRegistry.register_decoder(MyType, decode_func) # Or both at once CustomEncoderRegistry.register_pair(MyType, encode_func, decode_func)","title":"Method 3: Registry Class"},{"location":"tutorial/custom-encoders/#common-use-cases","text":"","title":"Common Use Cases"},{"location":"tutorial/custom-encoders/#numpy-arrays","text":"import numpy as np import base64 import pickle from beanis import register_type register_type( np.ndarray, encoder=lambda arr: base64.b64encode(pickle.dumps(arr)).decode('utf-8'), decoder=lambda s: pickle.loads(base64.b64decode(s.encode('utf-8'))) ) class DataScience(Document): experiment: str results: np.ndarray # Usage doc = DataScience( experiment=\"test_1\", results=np.array([[1, 2], [3, 4]]) ) await doc.insert()","title":"NumPy Arrays"},{"location":"tutorial/custom-encoders/#pytorch-tensors","text":"import torch import base64 import pickle from beanis import register_type register_type( torch.Tensor, encoder=lambda tensor: base64.b64encode(pickle.dumps(tensor)).decode('utf-8'), decoder=lambda s: pickle.loads(base64.b64decode(s.encode('utf-8'))) ) class NeuralNet(Document): model_name: str weights: torch.Tensor # Usage weights = torch.randn(10, 10) model = NeuralNet(model_name=\"v1\", weights=weights) await model.insert()","title":"PyTorch Tensors"},{"location":"tutorial/custom-encoders/#pandas-dataframes","text":"import pandas as pd from beanis import register_type register_type( pd.DataFrame, encoder=lambda df: df.to_json(orient='split'), decoder=lambda s: pd.read_json(s, orient='split') ) class Analysis(Document): dataset_name: str data: pd.DataFrame # Usage df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) analysis = Analysis(dataset_name=\"sales\", data=df) await analysis.insert()","title":"Pandas DataFrames"},{"location":"tutorial/custom-encoders/#pil-images","text":"from PIL import Image import io import base64 from beanis import register_type def encode_image(img: Image.Image) -> str: buffer = io.BytesIO() img.save(buffer, format='PNG') return base64.b64encode(buffer.getvalue()).decode('utf-8') def decode_image(data: str) -> Image.Image: buffer = io.BytesIO(base64.b64decode(data.encode('utf-8'))) return Image.open(buffer) register_type(Image.Image, encoder=encode_image, decoder=decode_image) class ImageDoc(Document): title: str image: Image.Image # Usage img = Image.new('RGB', (100, 100), color='red') doc = ImageDoc(title=\"test\", image=img) await doc.insert()","title":"PIL Images"},{"location":"tutorial/custom-encoders/#custom-classes","text":"from dataclasses import dataclass import json from beanis import register_type @dataclass class Point: x: float y: float def to_dict(self): return {'x': self.x, 'y': self.y} @classmethod def from_dict(cls, data): return cls(**data) register_type( Point, encoder=lambda p: json.dumps(p.to_dict()), decoder=lambda s: Point.from_dict(json.loads(s)) ) class Location(Document): name: str coordinates: Point # Usage loc = Location(name=\"Home\", coordinates=Point(37.7749, -122.4194)) await loc.insert()","title":"Custom Classes"},{"location":"tutorial/custom-encoders/#auto-registration","text":"Beanis automatically registers encoders for NumPy and PyTorch if installed: # No need to register manually! import numpy as np from beanis import Document class Model(Document): weights: np.ndarray # Auto-registered # Just works model = Model(weights=np.array([1, 2, 3])) await model.insert() To disable auto-registration: # In beanis/odm/custom_encoders.py _AUTO_REGISTER = False # Set before importing","title":"Auto-Registration"},{"location":"tutorial/custom-encoders/#advanced-patterns","text":"","title":"Advanced Patterns"},{"location":"tutorial/custom-encoders/#conditional-encoding","text":"from beanis import register_encoder, register_decoder @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: # Use compression for large arrays if arr.size > 1000: import zlib compressed = zlib.compress(pickle.dumps(arr)) return \"compressed:\" + base64.b64encode(compressed).decode('utf-8') else: return base64.b64encode(pickle.dumps(arr)).decode('utf-8') @register_decoder(np.ndarray) def decode_numpy(data: str) -> np.ndarray: if data.startswith(\"compressed:\"): import zlib data = data[11:] # Remove prefix compressed = base64.b64decode(data.encode('utf-8')) return pickle.loads(zlib.decompress(compressed)) else: return pickle.loads(base64.b64decode(data.encode('utf-8')))","title":"Conditional Encoding"},{"location":"tutorial/custom-encoders/#type-variants","text":"from typing import Union from beanis import register_encoder, register_decoder import numpy as np import torch # Can't register Union directly, so register each type @register_encoder(np.ndarray) def encode_numpy(arr: np.ndarray) -> str: return \"numpy:\" + base64.b64encode(pickle.dumps(arr)).decode('utf-8') @register_encoder(torch.Tensor) def encode_torch(tensor: torch.Tensor) -> str: return \"torch:\" + base64.b64encode(pickle.dumps(tensor)).decode('utf-8') # Single decoder that handles both @register_decoder(np.ndarray) def decode_array(data: str) -> np.ndarray: if data.startswith(\"numpy:\"): data = data[6:] return pickle.loads(base64.b64decode(data.encode('utf-8'))) @register_decoder(torch.Tensor) def decode_tensor(data: str) -> torch.Tensor: if data.startswith(\"torch:\"): data = data[6:] return pickle.loads(base64.b64decode(data.encode('utf-8')))","title":"Type Variants"},{"location":"tutorial/custom-encoders/#versioned-encoding","text":"@register_encoder(MyClass) def encode_v2(obj: MyClass) -> str: data = { 'version': 2, 'data': obj.to_dict() } return json.dumps(data) @register_decoder(MyClass) def decode_versioned(data: str) -> MyClass: parsed = json.loads(data) version = parsed.get('version', 1) if version == 1: # Old format return MyClass.from_old_format(parsed) elif version == 2: # New format return MyClass.from_dict(parsed['data']) else: raise ValueError(f\"Unknown version: {version}\")","title":"Versioned Encoding"},{"location":"tutorial/custom-encoders/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"tutorial/custom-encoders/#encoding-speed","text":"Different serialization methods have different speeds: # Fastest - Simple JSON register_type( Point, encoder=lambda p: json.dumps([p.x, p.y]), decoder=lambda s: Point(*json.loads(s)) ) # Fast - msgspec (if available) import msgspec register_type( Point, encoder=lambda p: msgspec.json.encode([p.x, p.y]).decode(), decoder=lambda s: Point(*msgspec.json.decode(s.encode())) ) # Slower - Pickle (but handles more types) register_type( ComplexType, encoder=lambda obj: base64.b64encode(pickle.dumps(obj)).decode(), decoder=lambda s: pickle.loads(base64.b64decode(s.encode())) )","title":"Encoding Speed"},{"location":"tutorial/custom-encoders/#storage-size","text":"Compression can reduce storage: import zlib @register_encoder(LargeObject) def encode_compressed(obj: LargeObject) -> str: serialized = pickle.dumps(obj) compressed = zlib.compress(serialized, level=6) return base64.b64encode(compressed).decode('utf-8') @register_decoder(LargeObject) def decode_compressed(data: str) -> LargeObject: compressed = base64.b64decode(data.encode('utf-8')) serialized = zlib.decompress(compressed) return pickle.loads(serialized)","title":"Storage Size"},{"location":"tutorial/custom-encoders/#error-handling","text":"@register_encoder(MyType) def safe_encode(obj: MyType) -> str: try: return json.dumps(obj.to_dict()) except Exception as e: # Log error or use fallback print(f\"Encoding error: {e}\") return json.dumps({'error': str(e)}) @register_decoder(MyType) def safe_decode(data: str) -> MyType: try: parsed = json.loads(data) if 'error' in parsed: raise ValueError(f\"Encoded error: {parsed['error']}\") return MyType.from_dict(parsed) except json.JSONDecodeError as e: # Handle corrupt data print(f\"Decoding error: {e}\") return MyType() # Return default","title":"Error Handling"},{"location":"tutorial/custom-encoders/#testing-encoders","text":"import pytest from beanis import register_type def test_custom_encoder(): # Register encoder register_type( Point, encoder=lambda p: f\"{p.x},{p.y}\", decoder=lambda s: Point(*map(float, s.split(','))) ) # Test encoding p = Point(1.5, 2.5) encoded = encode_point(p) assert encoded == \"1.5,2.5\" # Test decoding decoded = decode_point(encoded) assert decoded.x == 1.5 assert decoded.y == 2.5 # Test round-trip with document class Doc(Document): location: Point doc = Doc(location=Point(3.0, 4.0)) # Would need actual Redis for full test","title":"Testing Encoders"},{"location":"tutorial/custom-encoders/#debugging","text":"Check registered encoders: from beanis import CustomEncoderRegistry # Check if type has encoder encoder = CustomEncoderRegistry.get_encoder(MyType) print(f\"Encoder: {encoder}\") # Check if type has decoder decoder = CustomEncoderRegistry.get_decoder(MyType) print(f\"Decoder: {decoder}\") # Clear all (for testing) CustomEncoderRegistry.clear()","title":"Debugging"},{"location":"tutorial/custom-encoders/#limitations","text":"","title":"Limitations"},{"location":"tutorial/custom-encoders/#cannot-index-custom-types","text":"class Model(Document): weights: np.ndarray # Cannot be indexed # weights: Indexed(np.ndarray) # \u274c Won't work # Workaround: Extract searchable fields class Model(Document): weights: np.ndarray weight_size: Indexed(int) # Can be indexed @before_event(Insert) async def set_metadata(self): self.weight_size = self.weights.size","title":"Cannot Index Custom Types"},{"location":"tutorial/custom-encoders/#type-hints-required","text":"# Must specify type hint class Doc(Document): data: np.ndarray # \u2705 Works # data = None # \u274c Type hint required for encoder","title":"Type Hints Required"},{"location":"tutorial/custom-encoders/#best-practices","text":"Use standard formats - JSON over pickle when possible Version your encoders - Include version field for compatibility Handle errors gracefully - Corrupt data shouldn't crash Test round-trips - Ensure encode\u2192decode recovers original Consider compression - For large objects Document your encoders - Explain format for future maintainers Benchmark performance - Profile with realistic data","title":"Best Practices"},{"location":"tutorial/custom-encoders/#comparison-with-pydantic-validators","text":"Feature Custom Encoders Pydantic Validators Purpose Serialize to Redis Validate/transform on input When runs Save/load from DB Model instantiation Type support Any Python type Pydantic-compatible types Performance Encoder-dependent Very fast Use case Complex types Input validation","title":"Comparison with Pydantic Validators"},{"location":"tutorial/custom-encoders/#next-steps","text":"Defining Documents - Using custom types in documents Indexes - Index limitations with custom types Event Hooks - Derive searchable fields from custom types API Reference - Full encoder API","title":"Next Steps"},{"location":"tutorial/custom-encoders/#examples-repository","text":"Check tests/ for more examples: - test_custom_encoders.py - Encoder tests - test_numpy_integration.py - NumPy examples - test_complex_types.py - Advanced patterns","title":"Examples Repository"},{"location":"tutorial/defining-a-document/","text":"Defining a Document The Document class in Beanis is responsible for mapping and handling data in Redis. It is inherited from the BaseModel Pydantic class, so it follows the same data typing and parsing behavior. from typing import Optional from pydantic import BaseModel from beanis import Document, Indexed class Category(BaseModel): name: str description: str class Product(Document): # This is the model name: str description: Optional[str] = None price: Indexed(float) # Indexed for range queries category: Category stock: int = 0 class Settings: name = \"products\" # Redis key prefix Fields As mentioned before, the Document class is inherited from the Pydantic BaseModel class. It uses all the same patterns of BaseModel . But also it has special types of fields: id Indexed id The id field of the Document class reflects the unique identifier for the Redis document. Each object of the Document type has this field. The default type is str (UUID is auto-generated if not provided). class Sample(Document): num: int description: str foo = await Sample.find(Sample.num > 5).first_or_none() print(foo.id) # This will print the id bar = await Sample.get(foo.id) # get by id If you prefer another type, you can set it up too. For example, UUID: from uuid import UUID, uuid4 from pydantic import Field class Sample(Document): id: UUID = Field(default_factory=uuid4) num: int description: str Indexed To set up an index over a single field, the Indexed function can be used to wrap the type: from beanis import Document, Indexed class Sample(Document): num: Indexed(int) # Indexed for exact match queries price: Indexed(float) # Indexed for range queries description: str How indexing works in Redis: Numeric fields (int, float): Stored in Redis Sorted Sets for range queries String/categorical fields : Stored in Redis Sets for exact match queries Example queries: # Range query on indexed field (uses Redis Sorted Set) products = await Product.find( Product.price >= 10.0, Product.price <= 50.0 ).to_list() # Exact match on indexed field (uses Redis Set) electronics = await Product.find( Product.category == \"electronics\" ).to_list() Settings The inner Settings class is used to configure the document behavior: class Product(Document): name: str price: float class Settings: name = \"products\" # Redis key prefix (default: class name) key_prefix = \"prod\" # Alternative: custom prefix default_ttl = 3600 # Default TTL in seconds (optional) keep_nulls = False # Don't store None values (default: True) Available Settings name or key_prefix : Redis key prefix (e.g., \"Product:123\") default_ttl : Default expiration time in seconds keep_nulls : Whether to store fields with None values use_validation_on_fetch : Validate data when reading from Redis (default: False for performance) Complex Types Beanis automatically handles complex Pydantic types: from typing import List, Dict, Optional from pydantic import BaseModel from datetime import datetime from decimal import Decimal from uuid import UUID class Address(BaseModel): street: str city: str country: str class Product(Document): name: str price: Decimal # Precise decimal values tags: List[str] # Lists metadata: Dict[str, str] # Dictionaries category: Category # Nested Pydantic models address: Optional[Address] = None # Optional nested models created_at: datetime # Datetime fields product_id: UUID # UUID fields All these types are automatically serialized to/from Redis! Custom Types For types not natively supported (like NumPy arrays, PyTorch tensors), use custom encoders: from beanis import Document, register_type import numpy as np import base64 import pickle # Register custom encoder/decoder register_type( np.ndarray, encoder=lambda arr: base64.b64encode(pickle.dumps(arr)).decode(), decoder=lambda s: pickle.loads(base64.b64decode(s.encode())) ) class MLModel(Document): name: str weights: Any # Can store NumPy arrays! model = MLModel(name=\"model_v1\", weights=np.random.rand(100, 100)) await model.insert() # NumPy array is automatically encoded See the Custom Encoders Guide for more details. Document Storage Beanis stores documents as Redis Hashes , which provides: Field-level access ( HGET , HSET individual fields) Atomic increment/decrement operations Efficient memory usage Native Redis commands compatibility Example Redis structure for Product:123 : HGETALL Product:123 { \"name\": \"Tony's Chocolonely\", \"price\": \"5.95\", \"stock\": \"100\", \"category\": \"{\\\"name\\\":\\\"Chocolate\\\",\\\"description\\\":\\\"Roasted cacao\\\"}\", \"_class_name\": \"myapp.models.Product\" } Next Steps Initialization - Set up Beanis with Redis Insert Operations - Create documents Find Operations - Query documents Update Operations - Modify documents","title":"Defining a document"},{"location":"tutorial/defining-a-document/#defining-a-document","text":"The Document class in Beanis is responsible for mapping and handling data in Redis. It is inherited from the BaseModel Pydantic class, so it follows the same data typing and parsing behavior. from typing import Optional from pydantic import BaseModel from beanis import Document, Indexed class Category(BaseModel): name: str description: str class Product(Document): # This is the model name: str description: Optional[str] = None price: Indexed(float) # Indexed for range queries category: Category stock: int = 0 class Settings: name = \"products\" # Redis key prefix","title":"Defining a Document"},{"location":"tutorial/defining-a-document/#fields","text":"As mentioned before, the Document class is inherited from the Pydantic BaseModel class. It uses all the same patterns of BaseModel . But also it has special types of fields: id Indexed","title":"Fields"},{"location":"tutorial/defining-a-document/#id","text":"The id field of the Document class reflects the unique identifier for the Redis document. Each object of the Document type has this field. The default type is str (UUID is auto-generated if not provided). class Sample(Document): num: int description: str foo = await Sample.find(Sample.num > 5).first_or_none() print(foo.id) # This will print the id bar = await Sample.get(foo.id) # get by id If you prefer another type, you can set it up too. For example, UUID: from uuid import UUID, uuid4 from pydantic import Field class Sample(Document): id: UUID = Field(default_factory=uuid4) num: int description: str","title":"id"},{"location":"tutorial/defining-a-document/#indexed","text":"To set up an index over a single field, the Indexed function can be used to wrap the type: from beanis import Document, Indexed class Sample(Document): num: Indexed(int) # Indexed for exact match queries price: Indexed(float) # Indexed for range queries description: str How indexing works in Redis: Numeric fields (int, float): Stored in Redis Sorted Sets for range queries String/categorical fields : Stored in Redis Sets for exact match queries Example queries: # Range query on indexed field (uses Redis Sorted Set) products = await Product.find( Product.price >= 10.0, Product.price <= 50.0 ).to_list() # Exact match on indexed field (uses Redis Set) electronics = await Product.find( Product.category == \"electronics\" ).to_list()","title":"Indexed"},{"location":"tutorial/defining-a-document/#settings","text":"The inner Settings class is used to configure the document behavior: class Product(Document): name: str price: float class Settings: name = \"products\" # Redis key prefix (default: class name) key_prefix = \"prod\" # Alternative: custom prefix default_ttl = 3600 # Default TTL in seconds (optional) keep_nulls = False # Don't store None values (default: True)","title":"Settings"},{"location":"tutorial/defining-a-document/#available-settings","text":"name or key_prefix : Redis key prefix (e.g., \"Product:123\") default_ttl : Default expiration time in seconds keep_nulls : Whether to store fields with None values use_validation_on_fetch : Validate data when reading from Redis (default: False for performance)","title":"Available Settings"},{"location":"tutorial/defining-a-document/#complex-types","text":"Beanis automatically handles complex Pydantic types: from typing import List, Dict, Optional from pydantic import BaseModel from datetime import datetime from decimal import Decimal from uuid import UUID class Address(BaseModel): street: str city: str country: str class Product(Document): name: str price: Decimal # Precise decimal values tags: List[str] # Lists metadata: Dict[str, str] # Dictionaries category: Category # Nested Pydantic models address: Optional[Address] = None # Optional nested models created_at: datetime # Datetime fields product_id: UUID # UUID fields All these types are automatically serialized to/from Redis!","title":"Complex Types"},{"location":"tutorial/defining-a-document/#custom-types","text":"For types not natively supported (like NumPy arrays, PyTorch tensors), use custom encoders: from beanis import Document, register_type import numpy as np import base64 import pickle # Register custom encoder/decoder register_type( np.ndarray, encoder=lambda arr: base64.b64encode(pickle.dumps(arr)).decode(), decoder=lambda s: pickle.loads(base64.b64decode(s.encode())) ) class MLModel(Document): name: str weights: Any # Can store NumPy arrays! model = MLModel(name=\"model_v1\", weights=np.random.rand(100, 100)) await model.insert() # NumPy array is automatically encoded See the Custom Encoders Guide for more details.","title":"Custom Types"},{"location":"tutorial/defining-a-document/#document-storage","text":"Beanis stores documents as Redis Hashes , which provides: Field-level access ( HGET , HSET individual fields) Atomic increment/decrement operations Efficient memory usage Native Redis commands compatibility Example Redis structure for Product:123 : HGETALL Product:123 { \"name\": \"Tony's Chocolonely\", \"price\": \"5.95\", \"stock\": \"100\", \"category\": \"{\\\"name\\\":\\\"Chocolate\\\",\\\"description\\\":\\\"Roasted cacao\\\"}\", \"_class_name\": \"myapp.models.Product\" }","title":"Document Storage"},{"location":"tutorial/defining-a-document/#next-steps","text":"Initialization - Set up Beanis with Redis Insert Operations - Create documents Find Operations - Query documents Update Operations - Modify documents","title":"Next Steps"},{"location":"tutorial/deleting-documents/","text":"Delete Documents Beanis provides several methods for deleting documents from Redis. Delete Single Document Delete by Instance Use delete_self() to delete a document instance: from beanis import Document class Product(Document): name: str price: float class Settings: name = \"products\" # Get and delete product = await Product.get(\"product_id_123\") await product.delete_self() # Verify deletion exists = await Product.exists(\"product_id_123\") print(exists) # False Delete by ID Use the class method delete() to delete by ID without fetching: # Delete without fetching await Product.delete(\"product_id_123\") # More efficient than: # product = await Product.get(\"product_id_123\") # await product.delete_self() Delete Multiple Documents Use delete_many() to delete multiple documents by their IDs: # Delete multiple products ids = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"] await Product.delete_many(ids) # Verify deletions for id in ids: exists = await Product.exists(id) print(f\"{id} exists: {exists}\") # All False This uses Redis pipelines for efficient batch deletion. Delete All Documents Use delete_all() to remove all documents of a type: # Delete all products await Product.delete_all() # Verify count = await Product.count() print(count) # 0 Warning: This deletes ALL documents and cannot be undone! Check Before Delete Always check if a document exists before attempting deletion: # Safe deletion if await Product.exists(\"product_id_123\"): await Product.delete(\"product_id_123\") print(\"Deleted\") else: print(\"Product not found\") Delete with Event Hooks Run custom logic before/after deletions: from beanis import before_event, after_event, Delete from datetime import datetime class Product(Document): name: str price: float @before_event(Delete) async def backup_before_delete(self): \"\"\"Log deletion for audit trail\"\"\" print(f\"Deleting product: {self.name} at {datetime.now()}\") @after_event(Delete) async def cleanup_after_delete(self): \"\"\"Clean up related resources\"\"\" print(f\"Product {self.id} deleted successfully\") product = await Product.get(\"product_id_123\") await product.delete_self() # Output: Deleting product: Test Product at 2025-01-15 10:30:00 # Product product_id_123 deleted successfully What Gets Deleted When you delete a document, Beanis removes: The document Hash - Product:{id} Index entries - All sorted set/set entries for indexed fields Tracking entry - Entry in all:Product sorted set # For this document: class Product(Document): name: str price: Indexed(float) category: Indexed(str) # Deletion removes: # 1. Hash: Product:{id} # 2. Index: idx:Product:price (sorted set entry) # 3. Index: idx:Product:category:{value} (set entry) # 4. Tracking: all:Product (sorted set entry) Bulk Delete Pattern Efficient pattern for conditional bulk deletion: # Delete all products below $5 cheap_products = await Product.find(Product.price < 5.0).to_list() ids_to_delete = [p.id for p in cheap_products] if ids_to_delete: await Product.delete_many(ids_to_delete) print(f\"Deleted {len(ids_to_delete)} products\") Delete with TTL Alternative Instead of deleting, consider using TTL for automatic expiration: # Instead of deleting immediately await product.delete_self() # Consider expiring after 30 days await product.set_ttl(30 * 24 * 3600) # 30 days # Document auto-deletes after TTL expires This is useful for soft-deletes and compliance requirements. Delete vs Expire Delete (Immediate) : await product.delete_self() # Gone immediately Expire (Scheduled) : await product.set_ttl(3600) # Gone in 1 hour Choose based on your use case: - Delete : When you need immediate removal - Expire : For automatic cleanup, soft deletes, caching Transaction-Safe Deletion For critical operations, verify deletion succeeded: async def safe_delete(product_id: str) -> bool: \"\"\"Delete and verify\"\"\" if not await Product.exists(product_id): return False await Product.delete(product_id) # Verify deletion still_exists = await Product.exists(product_id) return not still_exists success = await safe_delete(\"product_id_123\") if success: print(\"Successfully deleted\") else: print(\"Deletion failed\") Cascade Deletion Pattern Beanis doesn't support automatic cascade deletes, but you can implement them: class Order(Document): product_id: str quantity: int class Settings: name = \"orders\" async def delete_product_cascade(product_id: str): \"\"\"Delete product and all related orders\"\"\" # Find all related orders # Note: This requires get_all and filter in memory # since we can't query on non-indexed product_id all_orders = await Order.all() related_orders = [o for o in all_orders if o.product_id == product_id] # Delete related orders if related_orders: order_ids = [o.id for o in related_orders] await Order.delete_many(order_ids) # Delete product await Product.delete(product_id) print(f\"Deleted product and {len(related_orders)} related orders\") await delete_product_cascade(\"product_id_123\") Delete Performance Deletion performance by method: delete(id) - O(1) - Fast, single Redis DEL command delete_self() - O(1) - Same as delete(id) delete_many(ids) - O(N) - Uses pipeline, efficient for bulk delete_all() - O(N) - Deletes all documents, can be slow for large collections Important Notes Deletion is permanent - No undo unless you have backups Indexes are cleaned up - All index entries automatically removed Use pipelines for bulk - delete_many() is optimized Consider TTL instead - For temporary data or soft deletes Event hooks run - Before/after delete hooks are triggered Examples Soft Delete with Flag class Product(Document): name: str price: float deleted: bool = False async def soft_delete(self): \"\"\"Mark as deleted instead of removing\"\"\" await self.update(deleted=True) # Optionally set TTL for eventual removal await self.set_ttl(30 * 24 * 3600) # Remove after 30 days @classmethod async def get_active(cls, product_id: str): \"\"\"Get only non-deleted products\"\"\" product = await cls.get(product_id) if product and not product.deleted: return product return None # Soft delete product = await Product.get(\"prod_123\") await product.soft_delete() # Won't find soft-deleted active = await Product.get_active(\"prod_123\") print(active) # None Batch Delete with Confirmation async def batch_delete_with_confirm(ids: list[str], model_class): \"\"\"Delete multiple documents with confirmation\"\"\" # Check how many exist existing = [] for id in ids: if await model_class.exists(id): existing.append(id) if not existing: print(\"No documents to delete\") return 0 print(f\"Found {len(existing)} documents to delete\") # Delete await model_class.delete_many(existing) # Verify deleted_count = 0 for id in existing: if not await model_class.exists(id): deleted_count += 1 print(f\"Successfully deleted {deleted_count}/{len(existing)} documents\") return deleted_count # Use it ids = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"] deleted = await batch_delete_with_confirm(ids, Product) Delete Old Records from datetime import datetime, timedelta class Product(Document): name: str created_at: datetime class Settings: name = \"products\" async def delete_old_products(days: int = 30): \"\"\"Delete products older than N days\"\"\" cutoff = datetime.now() - timedelta(days=days) # Get all products (Redis doesn't support date queries without custom indexing) all_products = await Product.all() # Filter in memory old_products = [ p for p in all_products if p.created_at < cutoff ] if old_products: ids = [p.id for p in old_products] await Product.delete_many(ids) print(f\"Deleted {len(ids)} old products\") else: print(\"No old products to delete\") # Delete products older than 90 days await delete_old_products(days=90) Next Steps Indexes - Learn about Redis indexing Event Hooks - Document lifecycle events Find Operations - Query documents before deletion","title":"Deleting documents"},{"location":"tutorial/deleting-documents/#delete-documents","text":"Beanis provides several methods for deleting documents from Redis.","title":"Delete Documents"},{"location":"tutorial/deleting-documents/#delete-single-document","text":"","title":"Delete Single Document"},{"location":"tutorial/deleting-documents/#delete-by-instance","text":"Use delete_self() to delete a document instance: from beanis import Document class Product(Document): name: str price: float class Settings: name = \"products\" # Get and delete product = await Product.get(\"product_id_123\") await product.delete_self() # Verify deletion exists = await Product.exists(\"product_id_123\") print(exists) # False","title":"Delete by Instance"},{"location":"tutorial/deleting-documents/#delete-by-id","text":"Use the class method delete() to delete by ID without fetching: # Delete without fetching await Product.delete(\"product_id_123\") # More efficient than: # product = await Product.get(\"product_id_123\") # await product.delete_self()","title":"Delete by ID"},{"location":"tutorial/deleting-documents/#delete-multiple-documents","text":"Use delete_many() to delete multiple documents by their IDs: # Delete multiple products ids = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"] await Product.delete_many(ids) # Verify deletions for id in ids: exists = await Product.exists(id) print(f\"{id} exists: {exists}\") # All False This uses Redis pipelines for efficient batch deletion.","title":"Delete Multiple Documents"},{"location":"tutorial/deleting-documents/#delete-all-documents","text":"Use delete_all() to remove all documents of a type: # Delete all products await Product.delete_all() # Verify count = await Product.count() print(count) # 0 Warning: This deletes ALL documents and cannot be undone!","title":"Delete All Documents"},{"location":"tutorial/deleting-documents/#check-before-delete","text":"Always check if a document exists before attempting deletion: # Safe deletion if await Product.exists(\"product_id_123\"): await Product.delete(\"product_id_123\") print(\"Deleted\") else: print(\"Product not found\")","title":"Check Before Delete"},{"location":"tutorial/deleting-documents/#delete-with-event-hooks","text":"Run custom logic before/after deletions: from beanis import before_event, after_event, Delete from datetime import datetime class Product(Document): name: str price: float @before_event(Delete) async def backup_before_delete(self): \"\"\"Log deletion for audit trail\"\"\" print(f\"Deleting product: {self.name} at {datetime.now()}\") @after_event(Delete) async def cleanup_after_delete(self): \"\"\"Clean up related resources\"\"\" print(f\"Product {self.id} deleted successfully\") product = await Product.get(\"product_id_123\") await product.delete_self() # Output: Deleting product: Test Product at 2025-01-15 10:30:00 # Product product_id_123 deleted successfully","title":"Delete with Event Hooks"},{"location":"tutorial/deleting-documents/#what-gets-deleted","text":"When you delete a document, Beanis removes: The document Hash - Product:{id} Index entries - All sorted set/set entries for indexed fields Tracking entry - Entry in all:Product sorted set # For this document: class Product(Document): name: str price: Indexed(float) category: Indexed(str) # Deletion removes: # 1. Hash: Product:{id} # 2. Index: idx:Product:price (sorted set entry) # 3. Index: idx:Product:category:{value} (set entry) # 4. Tracking: all:Product (sorted set entry)","title":"What Gets Deleted"},{"location":"tutorial/deleting-documents/#bulk-delete-pattern","text":"Efficient pattern for conditional bulk deletion: # Delete all products below $5 cheap_products = await Product.find(Product.price < 5.0).to_list() ids_to_delete = [p.id for p in cheap_products] if ids_to_delete: await Product.delete_many(ids_to_delete) print(f\"Deleted {len(ids_to_delete)} products\")","title":"Bulk Delete Pattern"},{"location":"tutorial/deleting-documents/#delete-with-ttl-alternative","text":"Instead of deleting, consider using TTL for automatic expiration: # Instead of deleting immediately await product.delete_self() # Consider expiring after 30 days await product.set_ttl(30 * 24 * 3600) # 30 days # Document auto-deletes after TTL expires This is useful for soft-deletes and compliance requirements.","title":"Delete with TTL Alternative"},{"location":"tutorial/deleting-documents/#delete-vs-expire","text":"Delete (Immediate) : await product.delete_self() # Gone immediately Expire (Scheduled) : await product.set_ttl(3600) # Gone in 1 hour Choose based on your use case: - Delete : When you need immediate removal - Expire : For automatic cleanup, soft deletes, caching","title":"Delete vs Expire"},{"location":"tutorial/deleting-documents/#transaction-safe-deletion","text":"For critical operations, verify deletion succeeded: async def safe_delete(product_id: str) -> bool: \"\"\"Delete and verify\"\"\" if not await Product.exists(product_id): return False await Product.delete(product_id) # Verify deletion still_exists = await Product.exists(product_id) return not still_exists success = await safe_delete(\"product_id_123\") if success: print(\"Successfully deleted\") else: print(\"Deletion failed\")","title":"Transaction-Safe Deletion"},{"location":"tutorial/deleting-documents/#cascade-deletion-pattern","text":"Beanis doesn't support automatic cascade deletes, but you can implement them: class Order(Document): product_id: str quantity: int class Settings: name = \"orders\" async def delete_product_cascade(product_id: str): \"\"\"Delete product and all related orders\"\"\" # Find all related orders # Note: This requires get_all and filter in memory # since we can't query on non-indexed product_id all_orders = await Order.all() related_orders = [o for o in all_orders if o.product_id == product_id] # Delete related orders if related_orders: order_ids = [o.id for o in related_orders] await Order.delete_many(order_ids) # Delete product await Product.delete(product_id) print(f\"Deleted product and {len(related_orders)} related orders\") await delete_product_cascade(\"product_id_123\")","title":"Cascade Deletion Pattern"},{"location":"tutorial/deleting-documents/#delete-performance","text":"Deletion performance by method: delete(id) - O(1) - Fast, single Redis DEL command delete_self() - O(1) - Same as delete(id) delete_many(ids) - O(N) - Uses pipeline, efficient for bulk delete_all() - O(N) - Deletes all documents, can be slow for large collections","title":"Delete Performance"},{"location":"tutorial/deleting-documents/#important-notes","text":"Deletion is permanent - No undo unless you have backups Indexes are cleaned up - All index entries automatically removed Use pipelines for bulk - delete_many() is optimized Consider TTL instead - For temporary data or soft deletes Event hooks run - Before/after delete hooks are triggered","title":"Important Notes"},{"location":"tutorial/deleting-documents/#examples","text":"","title":"Examples"},{"location":"tutorial/deleting-documents/#soft-delete-with-flag","text":"class Product(Document): name: str price: float deleted: bool = False async def soft_delete(self): \"\"\"Mark as deleted instead of removing\"\"\" await self.update(deleted=True) # Optionally set TTL for eventual removal await self.set_ttl(30 * 24 * 3600) # Remove after 30 days @classmethod async def get_active(cls, product_id: str): \"\"\"Get only non-deleted products\"\"\" product = await cls.get(product_id) if product and not product.deleted: return product return None # Soft delete product = await Product.get(\"prod_123\") await product.soft_delete() # Won't find soft-deleted active = await Product.get_active(\"prod_123\") print(active) # None","title":"Soft Delete with Flag"},{"location":"tutorial/deleting-documents/#batch-delete-with-confirmation","text":"async def batch_delete_with_confirm(ids: list[str], model_class): \"\"\"Delete multiple documents with confirmation\"\"\" # Check how many exist existing = [] for id in ids: if await model_class.exists(id): existing.append(id) if not existing: print(\"No documents to delete\") return 0 print(f\"Found {len(existing)} documents to delete\") # Delete await model_class.delete_many(existing) # Verify deleted_count = 0 for id in existing: if not await model_class.exists(id): deleted_count += 1 print(f\"Successfully deleted {deleted_count}/{len(existing)} documents\") return deleted_count # Use it ids = [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"] deleted = await batch_delete_with_confirm(ids, Product)","title":"Batch Delete with Confirmation"},{"location":"tutorial/deleting-documents/#delete-old-records","text":"from datetime import datetime, timedelta class Product(Document): name: str created_at: datetime class Settings: name = \"products\" async def delete_old_products(days: int = 30): \"\"\"Delete products older than N days\"\"\" cutoff = datetime.now() - timedelta(days=days) # Get all products (Redis doesn't support date queries without custom indexing) all_products = await Product.all() # Filter in memory old_products = [ p for p in all_products if p.created_at < cutoff ] if old_products: ids = [p.id for p in old_products] await Product.delete_many(ids) print(f\"Deleted {len(ids)} old products\") else: print(\"No old products to delete\") # Delete products older than 90 days await delete_old_products(days=90)","title":"Delete Old Records"},{"location":"tutorial/deleting-documents/#next-steps","text":"Indexes - Learn about Redis indexing Event Hooks - Document lifecycle events Find Operations - Query documents before deletion","title":"Next Steps"},{"location":"tutorial/event-based-actions/","text":"Event Hooks (Actions) Beanis provides event hooks that allow you to run custom logic before and after document operations. Overview Event hooks enable you to: - Validate data before saving - Transform data before insertion - Log operations - Trigger side effects - Maintain custom indexes - Send notifications Available Events Beanis supports hooks for these operations: Insert - When inserting new documents Update - When updating existing documents Delete - When deleting documents Save - When saving (insert or update) Basic Usage Before Event Hooks Run logic before an operation: from beanis import Document, before_event, Insert class Product(Document): name: str price: float @before_event(Insert) async def validate_price(self): \"\"\"Validate price before inserting\"\"\" if self.price < 0: raise ValueError(\"Price cannot be negative\") if self.price > 10000: raise ValueError(\"Price too high\") # This will raise ValueError product = Product(name=\"Test\", price=-5.0) await product.insert() # \u274c ValueError: Price cannot be negative After Event Hooks Run logic after an operation: from beanis import Document, after_event, Insert from datetime import datetime class Product(Document): name: str price: float @after_event(Insert) async def log_creation(self): \"\"\"Log after successful insert\"\"\" print(f\"Created product '{self.name}' at {datetime.now()}\") product = Product(name=\"Laptop\", price=999.99) await product.insert() # Output: Created product 'Laptop' at 2025-01-15 10:30:00 Multiple Hooks You can attach multiple hooks to the same event: from beanis import Document, before_event, after_event, Insert class Product(Document): name: str price: float created_at: datetime = None @before_event(Insert) async def set_timestamp(self): \"\"\"Set creation timestamp\"\"\" self.created_at = datetime.now() @before_event(Insert) async def validate_price(self): \"\"\"Validate price\"\"\" if self.price < 0: raise ValueError(\"Price must be positive\") @after_event(Insert) async def log_creation(self): \"\"\"Log after insert\"\"\" print(f\"Created: {self.name}\") @after_event(Insert) async def send_notification(self): \"\"\"Send notification\"\"\" # Send to analytics, message queue, etc. pass # All hooks run in order await product.insert() Execution order : Hooks run in the order they're defined in the class. Hook Types Insert Hooks Triggered when inserting new documents: from beanis import before_event, after_event, Insert class Product(Document): name: str slug: str = \"\" @before_event(Insert) async def generate_slug(self): \"\"\"Auto-generate slug from name\"\"\" self.slug = self.name.lower().replace(\" \", \"-\") @after_event(Insert) async def index_for_search(self): \"\"\"Add to external search index\"\"\" # Add to Elasticsearch, Algolia, etc. pass product = Product(name=\"Tony's Chocolonely\", price=5.95) await product.insert() print(product.slug) # \"tony's-chocolonely\" Update Hooks Triggered when updating existing documents: from beanis import before_event, after_event, Update class Product(Document): name: str price: float updated_at: datetime = None @before_event(Update) async def set_updated_timestamp(self): \"\"\"Update timestamp on every update\"\"\" self.updated_at = datetime.now() @after_event(Update) async def invalidate_cache(self): \"\"\"Clear cache after update\"\"\" # Clear Redis cache, CDN, etc. pass product = await Product.get(\"prod_123\") await product.update(price=7.99) print(product.updated_at) # 2025-01-15 10:35:00 Save Hooks Triggered by save() method (insert OR update): from beanis import before_event, after_event, Save class Product(Document): name: str price: float @before_event(Save) async def validate_data(self): \"\"\"Runs on both insert and update\"\"\" if self.price < 0: raise ValueError(\"Invalid price\") @after_event(Save) async def log_save(self): \"\"\"Log all saves\"\"\" print(f\"Saved product {self.id}\") # Works for both product = Product(name=\"New\", price=9.99) await product.save() # Insert - hooks run product.price = 12.99 await product.save() # Update - hooks run again Delete Hooks Triggered when deleting documents: from beanis import before_event, after_event, Delete class Product(Document): name: str @before_event(Delete) async def backup_before_delete(self): \"\"\"Backup before deletion\"\"\" # Save to backup storage print(f\"Backing up product: {self.name}\") @after_event(Delete) async def cleanup_resources(self): \"\"\"Clean up related resources\"\"\" # Delete images, clear cache, etc. print(f\"Cleaned up resources for {self.id}\") product = await Product.get(\"prod_123\") await product.delete_self() # Output: Backing up product: Laptop # Cleaned up resources for prod_123 Common Use Cases Auto-Timestamps from datetime import datetime from beanis import Document, before_event, Insert, Update class Product(Document): name: str created_at: datetime = None updated_at: datetime = None @before_event(Insert) async def set_created_at(self): self.created_at = datetime.now() self.updated_at = datetime.now() @before_event(Update) async def set_updated_at(self): self.updated_at = datetime.now() product = Product(name=\"Test\", price=9.99) await product.insert() print(product.created_at) # 2025-01-15 10:00:00 await asyncio.sleep(2) await product.update(price=12.99) print(product.updated_at) # 2025-01-15 10:00:02 Slug Generation import re from beanis import Document, before_event, Insert, Update class Product(Document): name: str slug: str = \"\" @before_event(Insert, Update) async def generate_slug(self): \"\"\"Auto-generate URL-safe slug\"\"\" slug = self.name.lower() slug = re.sub(r'[^a-z0-9]+', '-', slug) slug = slug.strip('-') self.slug = slug product = Product(name=\"Tony's Chocolonely!\", price=5.95) await product.insert() print(product.slug) # \"tony-s-chocolonely\" Data Validation from beanis import Document, before_event, Insert, Update class Product(Document): name: str price: float stock: int @before_event(Insert, Update) async def validate_business_rules(self): \"\"\"Complex validation logic\"\"\" if self.price < 0: raise ValueError(\"Price must be positive\") if self.stock < 0: raise ValueError(\"Stock cannot be negative\") if self.price > 10000 and self.stock > 1000: raise ValueError(\"High-value + high-stock needs approval\") # Normalize name self.name = self.name.strip().title() product = Product(name=\" laptop \", price=999, stock=50) await product.insert() print(product.name) # \"Laptop\" Audit Trail from datetime import datetime from typing import Optional class AuditLog(Document): entity_type: str entity_id: str action: str timestamp: datetime data: dict class Settings: name = \"audit_logs\" class Product(Document): name: str price: float @after_event(Insert) async def log_insert(self): await AuditLog( entity_type=\"Product\", entity_id=self.id, action=\"INSERT\", timestamp=datetime.now(), data={\"name\": self.name, \"price\": self.price} ).insert() @after_event(Update) async def log_update(self): await AuditLog( entity_type=\"Product\", entity_id=self.id, action=\"UPDATE\", timestamp=datetime.now(), data={\"name\": self.name, \"price\": self.price} ).insert() @after_event(Delete) async def log_delete(self): await AuditLog( entity_type=\"Product\", entity_id=self.id, action=\"DELETE\", timestamp=datetime.now(), data={} ).insert() # All operations are logged product = Product(name=\"Test\", price=9.99) await product.insert() # Creates audit log await product.update(price=12.99) # Creates audit log await product.delete_self() # Creates audit log Custom Secondary Indexes from beanis import Document, after_event, Insert, Update, Delete class Product(Document): name: str tags: list[str] @after_event(Insert, Update) async def index_tags(self): \"\"\"Maintain custom tag indexes\"\"\" # Add to Redis sets for each tag for tag in self.tags: await self._database.sadd(f\"tag_index:{tag}\", self.id) @after_event(Delete) async def cleanup_tag_indexes(self): \"\"\"Remove from tag indexes\"\"\" for tag in self.tags: await self._database.srem(f\"tag_index:{tag}\", self.id) # Tags are automatically indexed product = Product(name=\"Laptop\", tags=[\"electronics\", \"computers\"]) await product.insert() # Find products by tag (using custom index) laptop_ids = await Product._database.smembers(\"tag_index:electronics\") laptops = await Product.get_many(list(laptop_ids)) Notification System from beanis import Document, after_event, Insert, Update class Product(Document): name: str price: float stock: int @after_event(Update) async def check_low_stock(self): \"\"\"Send alert if stock is low\"\"\" if self.stock < 10: await self.send_alert( f\"Low stock alert: {self.name} has {self.stock} units\" ) @after_event(Update) async def check_price_drop(self): \"\"\"Notify customers of price drop\"\"\" # Get old price from Redis old_data = await self._database.hgetall(f\"Product:{self.id}\") old_price = float(old_data.get(\"price\", self.price)) if self.price < old_price * 0.9: # 10% drop await self.send_alert( f\"Price drop: {self.name} now ${self.price}\" ) async def send_alert(self, message: str): \"\"\"Send alert via your notification system\"\"\" print(f\"ALERT: {message}\") # Send to email, SMS, push notification, etc. product = await Product.get(\"prod_123\") await product.update(stock=5) # Output: ALERT: Low stock alert: Laptop has 5 units await product.update(price=499.99) # Down from $999 # Output: ALERT: Price drop: Laptop now $499.99 Hook Execution Order When multiple hooks are present: Before hooks run first (in definition order) Operation executes (insert/update/delete) After hooks run last (in definition order) class Product(Document): name: str @before_event(Insert) async def hook1(self): print(\"1. Before Insert - First\") @before_event(Insert) async def hook2(self): print(\"2. Before Insert - Second\") @after_event(Insert) async def hook3(self): print(\"4. After Insert - First\") @after_event(Insert) async def hook4(self): print(\"5. After Insert - Second\") await product.insert() # Output: # 1. Before Insert - First # 2. Before Insert - Second # 3. [INSERT OPERATION] # 4. After Insert - First # 5. After Insert - Second Error Handling If a before hook raises an exception, the operation is aborted : class Product(Document): price: float @before_event(Insert) async def validate_price(self): if self.price < 0: raise ValueError(\"Invalid price\") try: product = Product(price=-5.0) await product.insert() except ValueError as e: print(f\"Insert failed: {e}\") # Document was NOT inserted If an after hook raises an exception, the operation already completed : class Product(Document): name: str @after_event(Insert) async def send_notification(self): raise Exception(\"Notification failed\") try: product = Product(name=\"Test\") await product.insert() except Exception as e: print(f\"After hook failed: {e}\") # But document WAS inserted successfully Performance Considerations Keep hooks fast - They run synchronously with operations Use after hooks for slow tasks - Don't block the operation Consider background tasks - For heavy processing Avoid circular dependencies - Hook calling save() on same document Best Practices Use before hooks for validation - Prevent bad data from reaching Redis Use after hooks for side effects - Notifications, logging, etc. Keep hooks simple - One clear responsibility per hook Handle errors gracefully - Especially in after hooks Document hook behavior - Make it clear what hooks do Test hooks separately - Unit test hook logic Next Steps Insert Operations - Using hooks with inserts Update Operations - Using hooks with updates Delete Operations - Using hooks with deletes Custom Encoders - Transform data in hooks","title":"Event-based actions"},{"location":"tutorial/event-based-actions/#event-hooks-actions","text":"Beanis provides event hooks that allow you to run custom logic before and after document operations.","title":"Event Hooks (Actions)"},{"location":"tutorial/event-based-actions/#overview","text":"Event hooks enable you to: - Validate data before saving - Transform data before insertion - Log operations - Trigger side effects - Maintain custom indexes - Send notifications","title":"Overview"},{"location":"tutorial/event-based-actions/#available-events","text":"Beanis supports hooks for these operations: Insert - When inserting new documents Update - When updating existing documents Delete - When deleting documents Save - When saving (insert or update)","title":"Available Events"},{"location":"tutorial/event-based-actions/#basic-usage","text":"","title":"Basic Usage"},{"location":"tutorial/event-based-actions/#before-event-hooks","text":"Run logic before an operation: from beanis import Document, before_event, Insert class Product(Document): name: str price: float @before_event(Insert) async def validate_price(self): \"\"\"Validate price before inserting\"\"\" if self.price < 0: raise ValueError(\"Price cannot be negative\") if self.price > 10000: raise ValueError(\"Price too high\") # This will raise ValueError product = Product(name=\"Test\", price=-5.0) await product.insert() # \u274c ValueError: Price cannot be negative","title":"Before Event Hooks"},{"location":"tutorial/event-based-actions/#after-event-hooks","text":"Run logic after an operation: from beanis import Document, after_event, Insert from datetime import datetime class Product(Document): name: str price: float @after_event(Insert) async def log_creation(self): \"\"\"Log after successful insert\"\"\" print(f\"Created product '{self.name}' at {datetime.now()}\") product = Product(name=\"Laptop\", price=999.99) await product.insert() # Output: Created product 'Laptop' at 2025-01-15 10:30:00","title":"After Event Hooks"},{"location":"tutorial/event-based-actions/#multiple-hooks","text":"You can attach multiple hooks to the same event: from beanis import Document, before_event, after_event, Insert class Product(Document): name: str price: float created_at: datetime = None @before_event(Insert) async def set_timestamp(self): \"\"\"Set creation timestamp\"\"\" self.created_at = datetime.now() @before_event(Insert) async def validate_price(self): \"\"\"Validate price\"\"\" if self.price < 0: raise ValueError(\"Price must be positive\") @after_event(Insert) async def log_creation(self): \"\"\"Log after insert\"\"\" print(f\"Created: {self.name}\") @after_event(Insert) async def send_notification(self): \"\"\"Send notification\"\"\" # Send to analytics, message queue, etc. pass # All hooks run in order await product.insert() Execution order : Hooks run in the order they're defined in the class.","title":"Multiple Hooks"},{"location":"tutorial/event-based-actions/#hook-types","text":"","title":"Hook Types"},{"location":"tutorial/event-based-actions/#insert-hooks","text":"Triggered when inserting new documents: from beanis import before_event, after_event, Insert class Product(Document): name: str slug: str = \"\" @before_event(Insert) async def generate_slug(self): \"\"\"Auto-generate slug from name\"\"\" self.slug = self.name.lower().replace(\" \", \"-\") @after_event(Insert) async def index_for_search(self): \"\"\"Add to external search index\"\"\" # Add to Elasticsearch, Algolia, etc. pass product = Product(name=\"Tony's Chocolonely\", price=5.95) await product.insert() print(product.slug) # \"tony's-chocolonely\"","title":"Insert Hooks"},{"location":"tutorial/event-based-actions/#update-hooks","text":"Triggered when updating existing documents: from beanis import before_event, after_event, Update class Product(Document): name: str price: float updated_at: datetime = None @before_event(Update) async def set_updated_timestamp(self): \"\"\"Update timestamp on every update\"\"\" self.updated_at = datetime.now() @after_event(Update) async def invalidate_cache(self): \"\"\"Clear cache after update\"\"\" # Clear Redis cache, CDN, etc. pass product = await Product.get(\"prod_123\") await product.update(price=7.99) print(product.updated_at) # 2025-01-15 10:35:00","title":"Update Hooks"},{"location":"tutorial/event-based-actions/#save-hooks","text":"Triggered by save() method (insert OR update): from beanis import before_event, after_event, Save class Product(Document): name: str price: float @before_event(Save) async def validate_data(self): \"\"\"Runs on both insert and update\"\"\" if self.price < 0: raise ValueError(\"Invalid price\") @after_event(Save) async def log_save(self): \"\"\"Log all saves\"\"\" print(f\"Saved product {self.id}\") # Works for both product = Product(name=\"New\", price=9.99) await product.save() # Insert - hooks run product.price = 12.99 await product.save() # Update - hooks run again","title":"Save Hooks"},{"location":"tutorial/event-based-actions/#delete-hooks","text":"Triggered when deleting documents: from beanis import before_event, after_event, Delete class Product(Document): name: str @before_event(Delete) async def backup_before_delete(self): \"\"\"Backup before deletion\"\"\" # Save to backup storage print(f\"Backing up product: {self.name}\") @after_event(Delete) async def cleanup_resources(self): \"\"\"Clean up related resources\"\"\" # Delete images, clear cache, etc. print(f\"Cleaned up resources for {self.id}\") product = await Product.get(\"prod_123\") await product.delete_self() # Output: Backing up product: Laptop # Cleaned up resources for prod_123","title":"Delete Hooks"},{"location":"tutorial/event-based-actions/#common-use-cases","text":"","title":"Common Use Cases"},{"location":"tutorial/event-based-actions/#auto-timestamps","text":"from datetime import datetime from beanis import Document, before_event, Insert, Update class Product(Document): name: str created_at: datetime = None updated_at: datetime = None @before_event(Insert) async def set_created_at(self): self.created_at = datetime.now() self.updated_at = datetime.now() @before_event(Update) async def set_updated_at(self): self.updated_at = datetime.now() product = Product(name=\"Test\", price=9.99) await product.insert() print(product.created_at) # 2025-01-15 10:00:00 await asyncio.sleep(2) await product.update(price=12.99) print(product.updated_at) # 2025-01-15 10:00:02","title":"Auto-Timestamps"},{"location":"tutorial/event-based-actions/#slug-generation","text":"import re from beanis import Document, before_event, Insert, Update class Product(Document): name: str slug: str = \"\" @before_event(Insert, Update) async def generate_slug(self): \"\"\"Auto-generate URL-safe slug\"\"\" slug = self.name.lower() slug = re.sub(r'[^a-z0-9]+', '-', slug) slug = slug.strip('-') self.slug = slug product = Product(name=\"Tony's Chocolonely!\", price=5.95) await product.insert() print(product.slug) # \"tony-s-chocolonely\"","title":"Slug Generation"},{"location":"tutorial/event-based-actions/#data-validation","text":"from beanis import Document, before_event, Insert, Update class Product(Document): name: str price: float stock: int @before_event(Insert, Update) async def validate_business_rules(self): \"\"\"Complex validation logic\"\"\" if self.price < 0: raise ValueError(\"Price must be positive\") if self.stock < 0: raise ValueError(\"Stock cannot be negative\") if self.price > 10000 and self.stock > 1000: raise ValueError(\"High-value + high-stock needs approval\") # Normalize name self.name = self.name.strip().title() product = Product(name=\" laptop \", price=999, stock=50) await product.insert() print(product.name) # \"Laptop\"","title":"Data Validation"},{"location":"tutorial/event-based-actions/#audit-trail","text":"from datetime import datetime from typing import Optional class AuditLog(Document): entity_type: str entity_id: str action: str timestamp: datetime data: dict class Settings: name = \"audit_logs\" class Product(Document): name: str price: float @after_event(Insert) async def log_insert(self): await AuditLog( entity_type=\"Product\", entity_id=self.id, action=\"INSERT\", timestamp=datetime.now(), data={\"name\": self.name, \"price\": self.price} ).insert() @after_event(Update) async def log_update(self): await AuditLog( entity_type=\"Product\", entity_id=self.id, action=\"UPDATE\", timestamp=datetime.now(), data={\"name\": self.name, \"price\": self.price} ).insert() @after_event(Delete) async def log_delete(self): await AuditLog( entity_type=\"Product\", entity_id=self.id, action=\"DELETE\", timestamp=datetime.now(), data={} ).insert() # All operations are logged product = Product(name=\"Test\", price=9.99) await product.insert() # Creates audit log await product.update(price=12.99) # Creates audit log await product.delete_self() # Creates audit log","title":"Audit Trail"},{"location":"tutorial/event-based-actions/#custom-secondary-indexes","text":"from beanis import Document, after_event, Insert, Update, Delete class Product(Document): name: str tags: list[str] @after_event(Insert, Update) async def index_tags(self): \"\"\"Maintain custom tag indexes\"\"\" # Add to Redis sets for each tag for tag in self.tags: await self._database.sadd(f\"tag_index:{tag}\", self.id) @after_event(Delete) async def cleanup_tag_indexes(self): \"\"\"Remove from tag indexes\"\"\" for tag in self.tags: await self._database.srem(f\"tag_index:{tag}\", self.id) # Tags are automatically indexed product = Product(name=\"Laptop\", tags=[\"electronics\", \"computers\"]) await product.insert() # Find products by tag (using custom index) laptop_ids = await Product._database.smembers(\"tag_index:electronics\") laptops = await Product.get_many(list(laptop_ids))","title":"Custom Secondary Indexes"},{"location":"tutorial/event-based-actions/#notification-system","text":"from beanis import Document, after_event, Insert, Update class Product(Document): name: str price: float stock: int @after_event(Update) async def check_low_stock(self): \"\"\"Send alert if stock is low\"\"\" if self.stock < 10: await self.send_alert( f\"Low stock alert: {self.name} has {self.stock} units\" ) @after_event(Update) async def check_price_drop(self): \"\"\"Notify customers of price drop\"\"\" # Get old price from Redis old_data = await self._database.hgetall(f\"Product:{self.id}\") old_price = float(old_data.get(\"price\", self.price)) if self.price < old_price * 0.9: # 10% drop await self.send_alert( f\"Price drop: {self.name} now ${self.price}\" ) async def send_alert(self, message: str): \"\"\"Send alert via your notification system\"\"\" print(f\"ALERT: {message}\") # Send to email, SMS, push notification, etc. product = await Product.get(\"prod_123\") await product.update(stock=5) # Output: ALERT: Low stock alert: Laptop has 5 units await product.update(price=499.99) # Down from $999 # Output: ALERT: Price drop: Laptop now $499.99","title":"Notification System"},{"location":"tutorial/event-based-actions/#hook-execution-order","text":"When multiple hooks are present: Before hooks run first (in definition order) Operation executes (insert/update/delete) After hooks run last (in definition order) class Product(Document): name: str @before_event(Insert) async def hook1(self): print(\"1. Before Insert - First\") @before_event(Insert) async def hook2(self): print(\"2. Before Insert - Second\") @after_event(Insert) async def hook3(self): print(\"4. After Insert - First\") @after_event(Insert) async def hook4(self): print(\"5. After Insert - Second\") await product.insert() # Output: # 1. Before Insert - First # 2. Before Insert - Second # 3. [INSERT OPERATION] # 4. After Insert - First # 5. After Insert - Second","title":"Hook Execution Order"},{"location":"tutorial/event-based-actions/#error-handling","text":"If a before hook raises an exception, the operation is aborted : class Product(Document): price: float @before_event(Insert) async def validate_price(self): if self.price < 0: raise ValueError(\"Invalid price\") try: product = Product(price=-5.0) await product.insert() except ValueError as e: print(f\"Insert failed: {e}\") # Document was NOT inserted If an after hook raises an exception, the operation already completed : class Product(Document): name: str @after_event(Insert) async def send_notification(self): raise Exception(\"Notification failed\") try: product = Product(name=\"Test\") await product.insert() except Exception as e: print(f\"After hook failed: {e}\") # But document WAS inserted successfully","title":"Error Handling"},{"location":"tutorial/event-based-actions/#performance-considerations","text":"Keep hooks fast - They run synchronously with operations Use after hooks for slow tasks - Don't block the operation Consider background tasks - For heavy processing Avoid circular dependencies - Hook calling save() on same document","title":"Performance Considerations"},{"location":"tutorial/event-based-actions/#best-practices","text":"Use before hooks for validation - Prevent bad data from reaching Redis Use after hooks for side effects - Notifications, logging, etc. Keep hooks simple - One clear responsibility per hook Handle errors gracefully - Especially in after hooks Document hook behavior - Make it clear what hooks do Test hooks separately - Unit test hook logic","title":"Best Practices"},{"location":"tutorial/event-based-actions/#next-steps","text":"Insert Operations - Using hooks with inserts Update Operations - Using hooks with updates Delete Operations - Using hooks with deletes Custom Encoders - Transform data in hooks","title":"Next Steps"},{"location":"tutorial/finding-documents/","text":"Find Documents Beanis provides a powerful query interface for finding documents in Redis. Find One Get a single document by ID: product = await Product.get(\"product_id_123\") if product: print(product.name) Find with Conditions Query documents using indexed fields: from beanis import Document, Indexed class Product(Document): name: str price: Indexed(float) # Must be indexed for queries category: Indexed(str) class Settings: name = \"products\" # Range query on numeric field products = await Product.find( Product.price >= 10.0, Product.price <= 50.0 ).to_list() # Exact match on categorical field electronics = await Product.find( Product.category == \"electronics\" ).to_list() Query Methods to_list() Get all matching documents as a list: products = await Product.find(Product.price < 100).to_list() first_or_none() Get the first matching document or None: product = await Product.find(Product.price > 1000).first_or_none() if product: print(product.name) limit() Limit the number of results: # Get first 10 products products = await Product.find(Product.price > 10).limit(10).to_list() Get All Documents Retrieve all documents (uses sorted set tracking): # Get all products all_products = await Product.all() # With pagination page1 = await Product.all(limit=10) page2 = await Product.all(skip=10, limit=10) # Sort descending (newest first) recent = await Product.all(sort_desc=True, limit=5) Count Documents Count total documents: count = await Product.count() print(f\"Total products: {count}\") Check Existence Check if a document exists: exists = await Product.exists(\"product_id_123\") if exists: print(\"Product exists\") Batch Get Get multiple documents by their IDs: ids = [\"id1\", \"id2\", \"id3\"] products = await Product.get_many(ids) for product in products: if product: # Some IDs might not exist print(product.name) Important Notes Indexed fields required for queries - Only indexed fields support find() queries Exact match vs range queries : Numeric fields (int, float): Support range queries (>, <, >=, <=, ==) String fields: Support exact match only (==) all() uses document tracking - Returns documents in insertion order Query Examples Find by price range: affordable = await Product.find( Product.price >= 5.0, Product.price <= 20.0 ).to_list() Find by category: books = await Product.find(Product.category == \"books\").to_list() Find with limit: top_10 = await Product.find(Product.price > 0).limit(10).to_list() Find first match: expensive = await Product.find(Product.price > 1000).first_or_none() Next Steps Update Operations - Modify documents Delete Operations - Remove documents Indexes - Learn about indexing","title":"Finding documents"},{"location":"tutorial/finding-documents/#find-documents","text":"Beanis provides a powerful query interface for finding documents in Redis.","title":"Find Documents"},{"location":"tutorial/finding-documents/#find-one","text":"Get a single document by ID: product = await Product.get(\"product_id_123\") if product: print(product.name)","title":"Find One"},{"location":"tutorial/finding-documents/#find-with-conditions","text":"Query documents using indexed fields: from beanis import Document, Indexed class Product(Document): name: str price: Indexed(float) # Must be indexed for queries category: Indexed(str) class Settings: name = \"products\" # Range query on numeric field products = await Product.find( Product.price >= 10.0, Product.price <= 50.0 ).to_list() # Exact match on categorical field electronics = await Product.find( Product.category == \"electronics\" ).to_list()","title":"Find with Conditions"},{"location":"tutorial/finding-documents/#query-methods","text":"","title":"Query Methods"},{"location":"tutorial/finding-documents/#to_list","text":"Get all matching documents as a list: products = await Product.find(Product.price < 100).to_list()","title":"to_list()"},{"location":"tutorial/finding-documents/#first_or_none","text":"Get the first matching document or None: product = await Product.find(Product.price > 1000).first_or_none() if product: print(product.name)","title":"first_or_none()"},{"location":"tutorial/finding-documents/#limit","text":"Limit the number of results: # Get first 10 products products = await Product.find(Product.price > 10).limit(10).to_list()","title":"limit()"},{"location":"tutorial/finding-documents/#get-all-documents","text":"Retrieve all documents (uses sorted set tracking): # Get all products all_products = await Product.all() # With pagination page1 = await Product.all(limit=10) page2 = await Product.all(skip=10, limit=10) # Sort descending (newest first) recent = await Product.all(sort_desc=True, limit=5)","title":"Get All Documents"},{"location":"tutorial/finding-documents/#count-documents","text":"Count total documents: count = await Product.count() print(f\"Total products: {count}\")","title":"Count Documents"},{"location":"tutorial/finding-documents/#check-existence","text":"Check if a document exists: exists = await Product.exists(\"product_id_123\") if exists: print(\"Product exists\")","title":"Check Existence"},{"location":"tutorial/finding-documents/#batch-get","text":"Get multiple documents by their IDs: ids = [\"id1\", \"id2\", \"id3\"] products = await Product.get_many(ids) for product in products: if product: # Some IDs might not exist print(product.name)","title":"Batch Get"},{"location":"tutorial/finding-documents/#important-notes","text":"Indexed fields required for queries - Only indexed fields support find() queries Exact match vs range queries : Numeric fields (int, float): Support range queries (>, <, >=, <=, ==) String fields: Support exact match only (==) all() uses document tracking - Returns documents in insertion order","title":"Important Notes"},{"location":"tutorial/finding-documents/#query-examples","text":"","title":"Query Examples"},{"location":"tutorial/finding-documents/#find-by-price-range","text":"affordable = await Product.find( Product.price >= 5.0, Product.price <= 20.0 ).to_list()","title":"Find by price range:"},{"location":"tutorial/finding-documents/#find-by-category","text":"books = await Product.find(Product.category == \"books\").to_list()","title":"Find by category:"},{"location":"tutorial/finding-documents/#find-with-limit","text":"top_10 = await Product.find(Product.price > 0).limit(10).to_list()","title":"Find with limit:"},{"location":"tutorial/finding-documents/#find-first-match","text":"expensive = await Product.find(Product.price > 1000).first_or_none()","title":"Find first match:"},{"location":"tutorial/finding-documents/#next-steps","text":"Update Operations - Modify documents Delete Operations - Remove documents Indexes - Learn about indexing","title":"Next Steps"},{"location":"tutorial/geo-spatial-indexing/","text":"Geo-Spatial Indexing Build location-based features like store locators, delivery radius checks, and real-time tracking with Redis geo-spatial indexes. Overview Beanis provides built-in support for geo-spatial data through the GeoPoint type and Redis's GEOADD/GEORADIUS commands. This enables fast proximity searches with sub-millisecond query times. When to Use Geo-Spatial Indexes Perfect for: - Store/restaurant locators (\"find stores near me\") - Delivery radius validation - Real-time vehicle/device tracking - Geo-fencing applications - Nearby user discovery Performance: - Query time: ~0.2ms average (sub-millisecond) - Scalability: O(log N) - barely increases with dataset size - Insert overhead: ~90-120% slower than non-indexed inserts - Distance calc overhead: ~7% additional time Quick Start from beanis import Document, GeoPoint, init_beanis from beanis.odm.indexes import IndexedField, IndexManager from typing_extensions import Annotated from redis.asyncio import Redis # Define document with geo-indexed location class Store(Document): name: str address: str location: Annotated[GeoPoint, IndexedField()] class Settings: name = \"stores\" # Initialize redis = Redis(decode_responses=True) await init_beanis(database=redis, document_models=[Store]) # Create store with location store = Store( name=\"Downtown Coffee\", address=\"123 Main St, San Francisco, CA\", location=GeoPoint(longitude=-122.4194, latitude=37.7749) ) await store.insert() # Find stores within 5km nearby_ids = await IndexManager.find_by_geo_radius( redis_client=redis, document_class=Store, field_name=\"location\", longitude=-122.4200, latitude=37.7750, radius=5, unit=\"km\" ) # Get full store documents nearby_stores = await Store.get_many(nearby_ids) for store in nearby_stores: print(f\"{store.name} - {store.address}\") GeoPoint Type Creating GeoPoint from beanis import GeoPoint # Standard format location = GeoPoint(longitude=-122.4194, latitude=37.7749) # From dict location = GeoPoint(**{\"longitude\": -122.4194, \"latitude\": 37.7749}) # Access values print(f\"Lat: {location.latitude}, Lon: {location.longitude}\") Validation GeoPoint automatically validates coordinates: # Valid ranges # Longitude: -180 to 180 # Latitude: -90 to 90 # This raises ValidationError try: invalid = GeoPoint(longitude=200, latitude=37.7) except ValueError as e: print(e) # \"Longitude must be between -180 and 180\" Radius Queries Basic Proximity Search # Find all stores within 10km nearby = await IndexManager.find_by_geo_radius( redis_client=redis, document_class=Store, field_name=\"location\", longitude=-122.4194, latitude=37.7749, radius=10, unit=\"km\" # Options: 'm', 'km', 'mi', 'ft' ) # Returns list of document IDs print(f\"Found {len(nearby)} stores\") Query with Distances Get results sorted by distance with actual distance values: # Find stores with distances results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=Store, field_name=\"location\", longitude=-122.4194, latitude=37.7749, radius=10, unit=\"km\" ) # Returns list of (doc_id, distance) tuples for store_id, distance in results: store = await Store.get(store_id) print(f\"{store.name}: {distance:.2f} km away\") Supported Distance Units # Meters await IndexManager.find_by_geo_radius(..., radius=1000, unit=\"m\") # Kilometers (default) await IndexManager.find_by_geo_radius(..., radius=10, unit=\"km\") # Miles await IndexManager.find_by_geo_radius(..., radius=5, unit=\"mi\") # Feet await IndexManager.find_by_geo_radius(..., radius=5000, unit=\"ft\") Real-World Example: Food Delivery Service Let's build a complete food delivery radius checker: from beanis import Document, GeoPoint from beanis.odm.indexes import IndexedField, IndexManager from typing_extensions import Annotated from typing import List, Optional from pydantic import BaseModel # Models class DeliveryZone(BaseModel): name: str max_radius_km: float class Restaurant(Document): name: str cuisine: str location: Annotated[GeoPoint, IndexedField()] delivery_zones: List[DeliveryZone] is_open: bool class Settings: name = \"restaurants\" class DeliveryAddress(BaseModel): location: GeoPoint formatted_address: str # Business logic class DeliveryService: def __init__(self, redis_client): self.redis = redis_client async def find_available_restaurants( self, address: DeliveryAddress, max_distance_km: float = 10 ) -> List[tuple[Restaurant, float]]: \"\"\" Find restaurants that deliver to given address Returns list of (restaurant, distance) sorted by distance \"\"\" # Find all restaurants within max distance results = await IndexManager.find_by_geo_radius_with_distance( redis_client=self.redis, document_class=Restaurant, field_name=\"location\", longitude=address.location.longitude, latitude=address.location.latitude, radius=max_distance_km, unit=\"km\" ) # Fetch restaurant details and filter by delivery zones available = [] for restaurant_id, distance in results: restaurant = await Restaurant.get(restaurant_id) # Skip if closed if not restaurant.is_open: continue # Check if address is in any delivery zone for zone in restaurant.delivery_zones: if distance <= zone.max_radius_km: available.append((restaurant, distance)) break # Sort by distance available.sort(key=lambda x: x[1]) return available async def check_delivery_available( self, restaurant_id: str, address: DeliveryAddress ) -> tuple[bool, Optional[float]]: \"\"\" Check if restaurant delivers to address Returns (is_available, distance_km) \"\"\" restaurant = await Restaurant.get(restaurant_id) if not restaurant or not restaurant.is_open: return (False, None) # Calculate distance results = await IndexManager.find_by_geo_radius_with_distance( redis_client=self.redis, document_class=Restaurant, field_name=\"location\", longitude=address.location.longitude, latitude=address.location.latitude, radius=max(zone.max_radius_km for zone in restaurant.delivery_zones), unit=\"km\" ) # Find this restaurant in results for doc_id, distance in results: if doc_id == restaurant_id: # Check if in any delivery zone for zone in restaurant.delivery_zones: if distance <= zone.max_radius_km: return (True, distance) return (False, distance) return (False, None) async def get_delivery_estimate( self, restaurant: Restaurant, distance_km: float ) -> dict: \"\"\" Calculate delivery time and fee based on distance \"\"\" # Base delivery time: 20 minutes + 3 min per km delivery_time_min = 20 + (distance_km * 3) # Delivery fee: $2.99 base + $0.50 per km delivery_fee = 2.99 + (distance_km * 0.50) return { \"restaurant_name\": restaurant.name, \"distance_km\": round(distance_km, 2), \"estimated_delivery_min\": round(delivery_time_min), \"delivery_fee\": round(delivery_fee, 2) } # Usage example async def main(): from redis.asyncio import Redis from beanis import init_beanis redis = Redis(decode_responses=True) await init_beanis(database=redis, document_models=[Restaurant]) # Create sample restaurants restaurants = [ Restaurant( name=\"Pizza Palace\", cuisine=\"Italian\", location=GeoPoint(longitude=-122.4194, latitude=37.7749), delivery_zones=[ DeliveryZone(name=\"Downtown\", max_radius_km=5), DeliveryZone(name=\"Extended\", max_radius_km=10) ], is_open=True ), Restaurant( name=\"Sushi Express\", cuisine=\"Japanese\", location=GeoPoint(longitude=-122.4100, latitude=37.7850), delivery_zones=[ DeliveryZone(name=\"Local\", max_radius_km=3) ], is_open=True ), Restaurant( name=\"Burger Joint\", cuisine=\"American\", location=GeoPoint(longitude=-122.4300, latitude=37.7650), delivery_zones=[ DeliveryZone(name=\"Wide\", max_radius_km=15) ], is_open=True ) ] for restaurant in restaurants: await restaurant.insert() # User's delivery address user_address = DeliveryAddress( location=GeoPoint(longitude=-122.4150, latitude=37.7800), formatted_address=\"456 Market St, San Francisco, CA\" ) # Find available restaurants service = DeliveryService(redis) available = await service.find_available_restaurants(user_address) print(f\"Restaurants delivering to {user_address.formatted_address}:\\n\") for restaurant, distance in available: estimate = await service.get_delivery_estimate(restaurant, distance) print(f\"{estimate['restaurant_name']} ({restaurant.cuisine})\") print(f\" Distance: {estimate['distance_km']} km\") print(f\" Delivery time: ~{estimate['estimated_delivery_min']} min\") print(f\" Delivery fee: ${estimate['delivery_fee']}\") print() await redis.aclose() # Run import asyncio asyncio.run(main()) Output: Restaurants delivering to 456 Market St, San Francisco, CA: Pizza Palace (Italian) Distance: 0.73 km Delivery time: ~22 min Delivery fee: $3.36 Sushi Express (Japanese) Distance: 1.12 km Delivery time: ~23 min Delivery fee: $3.55 Burger Joint (American) Distance: 2.21 km Delivery time: ~27 min Delivery fee: $4.10 More Use Cases Store Locator class Store(Document): name: str address: str phone: str location: Annotated[GeoPoint, IndexedField()] store_hours: dict class Settings: name = \"stores\" async def find_nearest_store(user_lat: float, user_lon: float, limit: int = 5): \"\"\"Find nearest stores to user\"\"\" results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=Store, field_name=\"location\", longitude=user_lon, latitude=user_lat, radius=50, # Within 50km unit=\"km\" ) # Get top N nearest nearest = results[:limit] stores_with_distance = [] for store_id, distance in nearest: store = await Store.get(store_id) stores_with_distance.append({ \"store\": store, \"distance_km\": distance, \"distance_mi\": distance * 0.621371 # Convert to miles }) return stores_with_distance Real-Time Vehicle Tracking from datetime import datetime class Vehicle(Document): vehicle_id: str driver_name: str location: Annotated[GeoPoint, IndexedField()] last_updated: datetime is_available: bool class Settings: name = \"vehicles\" async def find_nearest_available_driver( pickup_location: GeoPoint, max_distance_km: float = 5 ) -> Optional[tuple[Vehicle, float]]: \"\"\"Find nearest available driver for ride-hailing\"\"\" results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=Vehicle, field_name=\"location\", longitude=pickup_location.longitude, latitude=pickup_location.latitude, radius=max_distance_km, unit=\"km\" ) # Find first available driver for vehicle_id, distance in results: vehicle = await Vehicle.get(vehicle_id) if vehicle.is_available: return (vehicle, distance) return None async def update_vehicle_location(vehicle_id: str, new_location: GeoPoint): \"\"\"Update driver location in real-time\"\"\" vehicle = await Vehicle.get(vehicle_id) vehicle.location = new_location vehicle.last_updated = datetime.utcnow() await vehicle.save() # Geo index automatically updated Geo-Fencing / Zone Detection class GeofenceZone(Document): name: str center: Annotated[GeoPoint, IndexedField()] radius_km: float zone_type: str # \"delivery\", \"restricted\", \"premium\" class Settings: name = \"geofence_zones\" async def check_if_in_zone( location: GeoPoint, zone_type: Optional[str] = None ) -> List[GeofenceZone]: \"\"\"Check if location is within any geofence zones\"\"\" # Query with large radius to get all potential zones results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=GeofenceZone, field_name=\"center\", longitude=location.longitude, latitude=location.latitude, radius=100, # Max zone size unit=\"km\" ) zones_containing_location = [] for zone_id, distance in results: zone = await GeofenceZone.get(zone_id) # Check if location is within zone's radius if distance <= zone.radius_km: if zone_type is None or zone.zone_type == zone_type: zones_containing_location.append(zone) return zones_containing_location Performance Tips 1. Choose Appropriate Radius Smaller radius = faster queries: # Fast: ~0.19ms nearby = await find_by_geo_radius(..., radius=1, unit=\"km\") # Still fast: ~0.23ms nearby = await find_by_geo_radius(..., radius=10, unit=\"km\") # Slower: ~0.27ms (more results to process) nearby = await find_by_geo_radius(..., radius=50, unit=\"km\") 2. Batch Queries When Possible # \u274c Slow: Multiple round-trips for user in users: nearby = await find_by_geo_radius(user.location, ...) # \u2705 Better: Batch processing user_locations = [user.location for user in users] # Process in batches or use async gather 3. Cache Frequent Queries from functools import lru_cache @lru_cache(maxsize=1000) def get_cached_nearby_stores(lat: float, lon: float, radius: int): # Round coordinates to reduce cache misses lat_rounded = round(lat, 3) lon_rounded = round(lon, 3) # ... query logic 4. Use Distance Only When Needed # If you only need IDs (7% faster) ids = await find_by_geo_radius(...) # If you need distances for sorting/display results = await find_by_geo_radius_with_distance(...) Limitations Cannot Combine with Other Indexes Geo queries are separate from other index queries: # \u274c Cannot do this in one query # \"Find Italian restaurants within 5km\" # \u2705 Do this instead: # 1. Find by geo proximity nearby_ids = await find_by_geo_radius(..., radius=5) # 2. Filter by other criteria nearby_restaurants = await Restaurant.get_many(nearby_ids) italian = [r for r in nearby_restaurants if r.cuisine == \"Italian\"] One GeoPoint Per Document Each document can only have one geo-indexed location: # \u274c Multiple geo indexes not supported class Business(Document): main_location: Annotated[GeoPoint, IndexedField()] warehouse_location: Annotated[GeoPoint, IndexedField()] # Won't work well # \u2705 Use separate documents class BusinessLocation(Document): business_id: str location_type: str # \"main\" or \"warehouse\" location: Annotated[GeoPoint, IndexedField()] Benchmark Results Based on our comprehensive benchmarks: Metric Value Notes Query time (avg) 0.2ms Sub-millisecond Query time (P95) 0.21ms Very consistent Insert overhead 90-120% ~2x slower with geo index Distance overhead 7% Minimal cost Scalability O(log N) Time barely increases Dataset tested: 10,000 stores, 25km radius queries Comparison with MongoDB Feature MongoDB (2dsphere) Beanis (Redis GEO) Query time 5-20ms 0.2ms (10-100x faster) Index size Larger Smaller (sorted set) Complex queries \u2705 $near + filters \u26a0\ufe0f Filter after query Distance units \u2705 All units \u2705 m, km, mi, ft Polygon queries \u2705 Supported \u274c Radius only Max distance Unlimited Practical: ~500km Best Practices Always validate coordinates - GeoPoint does this automatically Use appropriate radius - Start small, increase if needed Cache frequent locations - User's home, work addresses Update locations efficiently - Geo index updates automatically on save Consider data distribution - Works best with evenly distributed points Monitor query times - Should stay under 1ms for most use cases Troubleshooting Queries Return No Results # Check if documents have geo indexes members = await redis.zrange(\"idx:Store:location\", 0, -1) print(f\"Indexed stores: {len(members)}\") # Verify location is saved correctly store = await Store.get(store_id) print(f\"Location: {store.location}\") # Check radius is reasonable # 1 degree \u2248 111km, so 500km = ~4.5 degrees Slow Insert Performance # If geo indexing is not needed for all documents: class Store(Document): location: GeoPoint # No index # Manually index only important stores await IndexManager.add_to_index( redis_client, Store, store.id, \"location\", store.location, \"geo\" ) Next Steps Indexes Overview - Learn about other index types Custom Encoders - Extend GeoPoint with custom fields API Reference - Full IndexManager documentation Further Reading Redis GEOADD Redis GEORADIUS Haversine Formula - Distance calculation method used by Redis","title":"Geo-spatial indexing"},{"location":"tutorial/geo-spatial-indexing/#geo-spatial-indexing","text":"Build location-based features like store locators, delivery radius checks, and real-time tracking with Redis geo-spatial indexes.","title":"Geo-Spatial Indexing"},{"location":"tutorial/geo-spatial-indexing/#overview","text":"Beanis provides built-in support for geo-spatial data through the GeoPoint type and Redis's GEOADD/GEORADIUS commands. This enables fast proximity searches with sub-millisecond query times.","title":"Overview"},{"location":"tutorial/geo-spatial-indexing/#when-to-use-geo-spatial-indexes","text":"Perfect for: - Store/restaurant locators (\"find stores near me\") - Delivery radius validation - Real-time vehicle/device tracking - Geo-fencing applications - Nearby user discovery Performance: - Query time: ~0.2ms average (sub-millisecond) - Scalability: O(log N) - barely increases with dataset size - Insert overhead: ~90-120% slower than non-indexed inserts - Distance calc overhead: ~7% additional time","title":"When to Use Geo-Spatial Indexes"},{"location":"tutorial/geo-spatial-indexing/#quick-start","text":"from beanis import Document, GeoPoint, init_beanis from beanis.odm.indexes import IndexedField, IndexManager from typing_extensions import Annotated from redis.asyncio import Redis # Define document with geo-indexed location class Store(Document): name: str address: str location: Annotated[GeoPoint, IndexedField()] class Settings: name = \"stores\" # Initialize redis = Redis(decode_responses=True) await init_beanis(database=redis, document_models=[Store]) # Create store with location store = Store( name=\"Downtown Coffee\", address=\"123 Main St, San Francisco, CA\", location=GeoPoint(longitude=-122.4194, latitude=37.7749) ) await store.insert() # Find stores within 5km nearby_ids = await IndexManager.find_by_geo_radius( redis_client=redis, document_class=Store, field_name=\"location\", longitude=-122.4200, latitude=37.7750, radius=5, unit=\"km\" ) # Get full store documents nearby_stores = await Store.get_many(nearby_ids) for store in nearby_stores: print(f\"{store.name} - {store.address}\")","title":"Quick Start"},{"location":"tutorial/geo-spatial-indexing/#geopoint-type","text":"","title":"GeoPoint Type"},{"location":"tutorial/geo-spatial-indexing/#creating-geopoint","text":"from beanis import GeoPoint # Standard format location = GeoPoint(longitude=-122.4194, latitude=37.7749) # From dict location = GeoPoint(**{\"longitude\": -122.4194, \"latitude\": 37.7749}) # Access values print(f\"Lat: {location.latitude}, Lon: {location.longitude}\")","title":"Creating GeoPoint"},{"location":"tutorial/geo-spatial-indexing/#validation","text":"GeoPoint automatically validates coordinates: # Valid ranges # Longitude: -180 to 180 # Latitude: -90 to 90 # This raises ValidationError try: invalid = GeoPoint(longitude=200, latitude=37.7) except ValueError as e: print(e) # \"Longitude must be between -180 and 180\"","title":"Validation"},{"location":"tutorial/geo-spatial-indexing/#radius-queries","text":"","title":"Radius Queries"},{"location":"tutorial/geo-spatial-indexing/#basic-proximity-search","text":"# Find all stores within 10km nearby = await IndexManager.find_by_geo_radius( redis_client=redis, document_class=Store, field_name=\"location\", longitude=-122.4194, latitude=37.7749, radius=10, unit=\"km\" # Options: 'm', 'km', 'mi', 'ft' ) # Returns list of document IDs print(f\"Found {len(nearby)} stores\")","title":"Basic Proximity Search"},{"location":"tutorial/geo-spatial-indexing/#query-with-distances","text":"Get results sorted by distance with actual distance values: # Find stores with distances results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=Store, field_name=\"location\", longitude=-122.4194, latitude=37.7749, radius=10, unit=\"km\" ) # Returns list of (doc_id, distance) tuples for store_id, distance in results: store = await Store.get(store_id) print(f\"{store.name}: {distance:.2f} km away\")","title":"Query with Distances"},{"location":"tutorial/geo-spatial-indexing/#supported-distance-units","text":"# Meters await IndexManager.find_by_geo_radius(..., radius=1000, unit=\"m\") # Kilometers (default) await IndexManager.find_by_geo_radius(..., radius=10, unit=\"km\") # Miles await IndexManager.find_by_geo_radius(..., radius=5, unit=\"mi\") # Feet await IndexManager.find_by_geo_radius(..., radius=5000, unit=\"ft\")","title":"Supported Distance Units"},{"location":"tutorial/geo-spatial-indexing/#real-world-example-food-delivery-service","text":"Let's build a complete food delivery radius checker: from beanis import Document, GeoPoint from beanis.odm.indexes import IndexedField, IndexManager from typing_extensions import Annotated from typing import List, Optional from pydantic import BaseModel # Models class DeliveryZone(BaseModel): name: str max_radius_km: float class Restaurant(Document): name: str cuisine: str location: Annotated[GeoPoint, IndexedField()] delivery_zones: List[DeliveryZone] is_open: bool class Settings: name = \"restaurants\" class DeliveryAddress(BaseModel): location: GeoPoint formatted_address: str # Business logic class DeliveryService: def __init__(self, redis_client): self.redis = redis_client async def find_available_restaurants( self, address: DeliveryAddress, max_distance_km: float = 10 ) -> List[tuple[Restaurant, float]]: \"\"\" Find restaurants that deliver to given address Returns list of (restaurant, distance) sorted by distance \"\"\" # Find all restaurants within max distance results = await IndexManager.find_by_geo_radius_with_distance( redis_client=self.redis, document_class=Restaurant, field_name=\"location\", longitude=address.location.longitude, latitude=address.location.latitude, radius=max_distance_km, unit=\"km\" ) # Fetch restaurant details and filter by delivery zones available = [] for restaurant_id, distance in results: restaurant = await Restaurant.get(restaurant_id) # Skip if closed if not restaurant.is_open: continue # Check if address is in any delivery zone for zone in restaurant.delivery_zones: if distance <= zone.max_radius_km: available.append((restaurant, distance)) break # Sort by distance available.sort(key=lambda x: x[1]) return available async def check_delivery_available( self, restaurant_id: str, address: DeliveryAddress ) -> tuple[bool, Optional[float]]: \"\"\" Check if restaurant delivers to address Returns (is_available, distance_km) \"\"\" restaurant = await Restaurant.get(restaurant_id) if not restaurant or not restaurant.is_open: return (False, None) # Calculate distance results = await IndexManager.find_by_geo_radius_with_distance( redis_client=self.redis, document_class=Restaurant, field_name=\"location\", longitude=address.location.longitude, latitude=address.location.latitude, radius=max(zone.max_radius_km for zone in restaurant.delivery_zones), unit=\"km\" ) # Find this restaurant in results for doc_id, distance in results: if doc_id == restaurant_id: # Check if in any delivery zone for zone in restaurant.delivery_zones: if distance <= zone.max_radius_km: return (True, distance) return (False, distance) return (False, None) async def get_delivery_estimate( self, restaurant: Restaurant, distance_km: float ) -> dict: \"\"\" Calculate delivery time and fee based on distance \"\"\" # Base delivery time: 20 minutes + 3 min per km delivery_time_min = 20 + (distance_km * 3) # Delivery fee: $2.99 base + $0.50 per km delivery_fee = 2.99 + (distance_km * 0.50) return { \"restaurant_name\": restaurant.name, \"distance_km\": round(distance_km, 2), \"estimated_delivery_min\": round(delivery_time_min), \"delivery_fee\": round(delivery_fee, 2) } # Usage example async def main(): from redis.asyncio import Redis from beanis import init_beanis redis = Redis(decode_responses=True) await init_beanis(database=redis, document_models=[Restaurant]) # Create sample restaurants restaurants = [ Restaurant( name=\"Pizza Palace\", cuisine=\"Italian\", location=GeoPoint(longitude=-122.4194, latitude=37.7749), delivery_zones=[ DeliveryZone(name=\"Downtown\", max_radius_km=5), DeliveryZone(name=\"Extended\", max_radius_km=10) ], is_open=True ), Restaurant( name=\"Sushi Express\", cuisine=\"Japanese\", location=GeoPoint(longitude=-122.4100, latitude=37.7850), delivery_zones=[ DeliveryZone(name=\"Local\", max_radius_km=3) ], is_open=True ), Restaurant( name=\"Burger Joint\", cuisine=\"American\", location=GeoPoint(longitude=-122.4300, latitude=37.7650), delivery_zones=[ DeliveryZone(name=\"Wide\", max_radius_km=15) ], is_open=True ) ] for restaurant in restaurants: await restaurant.insert() # User's delivery address user_address = DeliveryAddress( location=GeoPoint(longitude=-122.4150, latitude=37.7800), formatted_address=\"456 Market St, San Francisco, CA\" ) # Find available restaurants service = DeliveryService(redis) available = await service.find_available_restaurants(user_address) print(f\"Restaurants delivering to {user_address.formatted_address}:\\n\") for restaurant, distance in available: estimate = await service.get_delivery_estimate(restaurant, distance) print(f\"{estimate['restaurant_name']} ({restaurant.cuisine})\") print(f\" Distance: {estimate['distance_km']} km\") print(f\" Delivery time: ~{estimate['estimated_delivery_min']} min\") print(f\" Delivery fee: ${estimate['delivery_fee']}\") print() await redis.aclose() # Run import asyncio asyncio.run(main()) Output: Restaurants delivering to 456 Market St, San Francisco, CA: Pizza Palace (Italian) Distance: 0.73 km Delivery time: ~22 min Delivery fee: $3.36 Sushi Express (Japanese) Distance: 1.12 km Delivery time: ~23 min Delivery fee: $3.55 Burger Joint (American) Distance: 2.21 km Delivery time: ~27 min Delivery fee: $4.10","title":"Real-World Example: Food Delivery Service"},{"location":"tutorial/geo-spatial-indexing/#more-use-cases","text":"","title":"More Use Cases"},{"location":"tutorial/geo-spatial-indexing/#store-locator","text":"class Store(Document): name: str address: str phone: str location: Annotated[GeoPoint, IndexedField()] store_hours: dict class Settings: name = \"stores\" async def find_nearest_store(user_lat: float, user_lon: float, limit: int = 5): \"\"\"Find nearest stores to user\"\"\" results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=Store, field_name=\"location\", longitude=user_lon, latitude=user_lat, radius=50, # Within 50km unit=\"km\" ) # Get top N nearest nearest = results[:limit] stores_with_distance = [] for store_id, distance in nearest: store = await Store.get(store_id) stores_with_distance.append({ \"store\": store, \"distance_km\": distance, \"distance_mi\": distance * 0.621371 # Convert to miles }) return stores_with_distance","title":"Store Locator"},{"location":"tutorial/geo-spatial-indexing/#real-time-vehicle-tracking","text":"from datetime import datetime class Vehicle(Document): vehicle_id: str driver_name: str location: Annotated[GeoPoint, IndexedField()] last_updated: datetime is_available: bool class Settings: name = \"vehicles\" async def find_nearest_available_driver( pickup_location: GeoPoint, max_distance_km: float = 5 ) -> Optional[tuple[Vehicle, float]]: \"\"\"Find nearest available driver for ride-hailing\"\"\" results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=Vehicle, field_name=\"location\", longitude=pickup_location.longitude, latitude=pickup_location.latitude, radius=max_distance_km, unit=\"km\" ) # Find first available driver for vehicle_id, distance in results: vehicle = await Vehicle.get(vehicle_id) if vehicle.is_available: return (vehicle, distance) return None async def update_vehicle_location(vehicle_id: str, new_location: GeoPoint): \"\"\"Update driver location in real-time\"\"\" vehicle = await Vehicle.get(vehicle_id) vehicle.location = new_location vehicle.last_updated = datetime.utcnow() await vehicle.save() # Geo index automatically updated","title":"Real-Time Vehicle Tracking"},{"location":"tutorial/geo-spatial-indexing/#geo-fencing-zone-detection","text":"class GeofenceZone(Document): name: str center: Annotated[GeoPoint, IndexedField()] radius_km: float zone_type: str # \"delivery\", \"restricted\", \"premium\" class Settings: name = \"geofence_zones\" async def check_if_in_zone( location: GeoPoint, zone_type: Optional[str] = None ) -> List[GeofenceZone]: \"\"\"Check if location is within any geofence zones\"\"\" # Query with large radius to get all potential zones results = await IndexManager.find_by_geo_radius_with_distance( redis_client=redis, document_class=GeofenceZone, field_name=\"center\", longitude=location.longitude, latitude=location.latitude, radius=100, # Max zone size unit=\"km\" ) zones_containing_location = [] for zone_id, distance in results: zone = await GeofenceZone.get(zone_id) # Check if location is within zone's radius if distance <= zone.radius_km: if zone_type is None or zone.zone_type == zone_type: zones_containing_location.append(zone) return zones_containing_location","title":"Geo-Fencing / Zone Detection"},{"location":"tutorial/geo-spatial-indexing/#performance-tips","text":"","title":"Performance Tips"},{"location":"tutorial/geo-spatial-indexing/#1-choose-appropriate-radius","text":"Smaller radius = faster queries: # Fast: ~0.19ms nearby = await find_by_geo_radius(..., radius=1, unit=\"km\") # Still fast: ~0.23ms nearby = await find_by_geo_radius(..., radius=10, unit=\"km\") # Slower: ~0.27ms (more results to process) nearby = await find_by_geo_radius(..., radius=50, unit=\"km\")","title":"1. Choose Appropriate Radius"},{"location":"tutorial/geo-spatial-indexing/#2-batch-queries-when-possible","text":"# \u274c Slow: Multiple round-trips for user in users: nearby = await find_by_geo_radius(user.location, ...) # \u2705 Better: Batch processing user_locations = [user.location for user in users] # Process in batches or use async gather","title":"2. Batch Queries When Possible"},{"location":"tutorial/geo-spatial-indexing/#3-cache-frequent-queries","text":"from functools import lru_cache @lru_cache(maxsize=1000) def get_cached_nearby_stores(lat: float, lon: float, radius: int): # Round coordinates to reduce cache misses lat_rounded = round(lat, 3) lon_rounded = round(lon, 3) # ... query logic","title":"3. Cache Frequent Queries"},{"location":"tutorial/geo-spatial-indexing/#4-use-distance-only-when-needed","text":"# If you only need IDs (7% faster) ids = await find_by_geo_radius(...) # If you need distances for sorting/display results = await find_by_geo_radius_with_distance(...)","title":"4. Use Distance Only When Needed"},{"location":"tutorial/geo-spatial-indexing/#limitations","text":"","title":"Limitations"},{"location":"tutorial/geo-spatial-indexing/#cannot-combine-with-other-indexes","text":"Geo queries are separate from other index queries: # \u274c Cannot do this in one query # \"Find Italian restaurants within 5km\" # \u2705 Do this instead: # 1. Find by geo proximity nearby_ids = await find_by_geo_radius(..., radius=5) # 2. Filter by other criteria nearby_restaurants = await Restaurant.get_many(nearby_ids) italian = [r for r in nearby_restaurants if r.cuisine == \"Italian\"]","title":"Cannot Combine with Other Indexes"},{"location":"tutorial/geo-spatial-indexing/#one-geopoint-per-document","text":"Each document can only have one geo-indexed location: # \u274c Multiple geo indexes not supported class Business(Document): main_location: Annotated[GeoPoint, IndexedField()] warehouse_location: Annotated[GeoPoint, IndexedField()] # Won't work well # \u2705 Use separate documents class BusinessLocation(Document): business_id: str location_type: str # \"main\" or \"warehouse\" location: Annotated[GeoPoint, IndexedField()]","title":"One GeoPoint Per Document"},{"location":"tutorial/geo-spatial-indexing/#benchmark-results","text":"Based on our comprehensive benchmarks: Metric Value Notes Query time (avg) 0.2ms Sub-millisecond Query time (P95) 0.21ms Very consistent Insert overhead 90-120% ~2x slower with geo index Distance overhead 7% Minimal cost Scalability O(log N) Time barely increases Dataset tested: 10,000 stores, 25km radius queries","title":"Benchmark Results"},{"location":"tutorial/geo-spatial-indexing/#comparison-with-mongodb","text":"Feature MongoDB (2dsphere) Beanis (Redis GEO) Query time 5-20ms 0.2ms (10-100x faster) Index size Larger Smaller (sorted set) Complex queries \u2705 $near + filters \u26a0\ufe0f Filter after query Distance units \u2705 All units \u2705 m, km, mi, ft Polygon queries \u2705 Supported \u274c Radius only Max distance Unlimited Practical: ~500km","title":"Comparison with MongoDB"},{"location":"tutorial/geo-spatial-indexing/#best-practices","text":"Always validate coordinates - GeoPoint does this automatically Use appropriate radius - Start small, increase if needed Cache frequent locations - User's home, work addresses Update locations efficiently - Geo index updates automatically on save Consider data distribution - Works best with evenly distributed points Monitor query times - Should stay under 1ms for most use cases","title":"Best Practices"},{"location":"tutorial/geo-spatial-indexing/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"tutorial/geo-spatial-indexing/#queries-return-no-results","text":"# Check if documents have geo indexes members = await redis.zrange(\"idx:Store:location\", 0, -1) print(f\"Indexed stores: {len(members)}\") # Verify location is saved correctly store = await Store.get(store_id) print(f\"Location: {store.location}\") # Check radius is reasonable # 1 degree \u2248 111km, so 500km = ~4.5 degrees","title":"Queries Return No Results"},{"location":"tutorial/geo-spatial-indexing/#slow-insert-performance","text":"# If geo indexing is not needed for all documents: class Store(Document): location: GeoPoint # No index # Manually index only important stores await IndexManager.add_to_index( redis_client, Store, store.id, \"location\", store.location, \"geo\" )","title":"Slow Insert Performance"},{"location":"tutorial/geo-spatial-indexing/#next-steps","text":"Indexes Overview - Learn about other index types Custom Encoders - Extend GeoPoint with custom fields API Reference - Full IndexManager documentation","title":"Next Steps"},{"location":"tutorial/geo-spatial-indexing/#further-reading","text":"Redis GEOADD Redis GEORADIUS Haversine Formula - Distance calculation method used by Redis","title":"Further Reading"},{"location":"tutorial/indexes/","text":"Indexes Beanis uses Redis data structures (Sorted Sets and Sets) to enable fast queries on your documents. Overview Unlike MongoDB which has a flexible indexing system, Redis indexing in Beanis is simpler but powerful: Sorted Sets - For numeric fields (support range queries) Sets - For string/categorical fields (exact match only) Geo Indexes - For geo-spatial fields (proximity queries) Hash - Document storage (no indexing) Defining Indexes Use the Indexed() type annotation to mark fields for indexing: from beanis import Document, Indexed, GeoPoint class Product(Document): name: str # Not indexed - no queries price: Indexed[float] # Indexed - range queries category: Indexed[str] # Indexed - exact match stock: int # Not indexed - no queries location: Indexed[GeoPoint] # Indexed - geo queries class Settings: name = \"products\" Numeric Indexes (Sorted Sets) Numeric fields ( int , float ) are stored in Redis Sorted Sets: class Product(Document): name: str price: Indexed(float) # Stored in sorted set stock: Indexed(int) # Stored in sorted set # Enables range queries expensive = await Product.find(Product.price > 100).to_list() affordable = await Product.find( Product.price >= 10, Product.price <= 50 ).to_list() # Stock queries low_stock = await Product.find(Product.stock < 10).to_list() Redis Structure : Key: idx:Product:price Type: Sorted Set Content: {document_id: price_value, ...} Example: idx:Product:price -> { \"prod_1\": 9.99, \"prod_2\": 15.50, \"prod_3\": 25.00 } Supported Operations : - == - Exact match - > , >= - Greater than - < , <= - Less than - Range combinations Categorical Indexes (Sets) String fields are stored in Redis Sets (one set per unique value): class Product(Document): name: str category: Indexed(str) # Stored in sets brand: Indexed(str) # Stored in sets # Enables exact match queries books = await Product.find(Product.category == \"books\").to_list() nike = await Product.find(Product.brand == \"Nike\").to_list() Redis Structure : Key: idx:Product:category:books Type: Set Content: {document_id, document_id, ...} Example: idx:Product:category:books -> {\"prod_1\", \"prod_2\", \"prod_5\"} idx:Product:category:electronics -> {\"prod_3\", \"prod_4\"} Supported Operations : - == - Exact match only Not Supported : - Partial matches (use full-text search module) - Case-insensitive search - Wildcards Index Creation Indexes are created automatically when you insert documents: # Define indexed fields class Product(Document): price: Indexed(float) category: Indexed(str) # Initialize await init_beanis(database=redis_client, document_models=[Product]) # Insert - indexes created automatically product = Product( name=\"Laptop\", price=999.99, category=\"electronics\" ) await product.insert() # This creates: # 1. Hash: Product:{id} # 2. Sorted Set entry: idx:Product:price # 3. Set entry: idx:Product:category:electronics Index Updates Indexes are automatically updated when documents change: product = await Product.get(\"product_id_123\") # Update price - sorted set automatically updated await product.update(price=799.99) # Update category - sets automatically updated await product.update(category=\"computers\") # Old set entry removed, new set entry added Multiple Indexes You can index multiple fields on the same document: class Product(Document): name: str price: Indexed(float) stock: Indexed(int) category: Indexed(str) brand: Indexed(str) # Each indexed field has its own Redis structure # - idx:Product:price (sorted set) # - idx:Product:stock (sorted set) # - idx:Product:category:{value} (sets) # - idx:Product:brand:{value} (sets) Query Performance Indexed Queries (Fast) # These queries use indexes - O(log N) or O(1) products = await Product.find(Product.price < 100).to_list() books = await Product.find(Product.category == \"books\").to_list() Non-Indexed Queries (Slow) # This field is NOT indexed - requires full scan class Product(Document): name: str # Not indexed! price: Indexed(float) # This will NOT work (name not indexed): # products = await Product.find(Product.name == \"Laptop\").to_list() # You must use get_all() and filter manually: all_products = await Product.all() laptops = [p for p in all_products if p.name == \"Laptop\"] Rule : Only Indexed() fields support .find() queries! Index Storage Cost Each index uses Redis memory: class Product(Document): price: Indexed(float) # ~50 bytes per document category: Indexed(str) # ~40 bytes per document per unique value # For 10,000 products with 5 categories: # - price index: ~500 KB # - category index: ~200 KB (5 sets) # - documents: ~5-10 MB (depends on size) Best Practice : Only index fields you'll query frequently. Compound Queries Beanis supports querying multiple indexed fields: # Both fields must be indexed products = await Product.find( Product.price >= 10, Product.price <= 50, Product.category == \"electronics\" ).to_list() How it works : 1. Query price index (sorted set range) 2. Query category index (set members) 3. Intersect results (in Python) Index Limitations No Full-Text Search # Not supported - no partial matching # products = await Product.find(Product.name.contains(\"Lap\")).to_list() # Use Redis RediSearch module for full-text search # Or filter in memory: all_products = await Product.all() matches = [p for p in all_products if \"Lap\" in p.name] No Case-Insensitive Search # Case-sensitive only books = await Product.find(Product.category == \"Books\").to_list() # Won't find \"books\" # Workaround: Store lowercase class Product(Document): category_lower: Indexed(str) @before_event(Insert, Update) async def normalize_category(self): self.category_lower = self.category.lower() # Query lowercase books = await Product.find(Product.category_lower == \"books\").to_list() No Date Range Queries from datetime import datetime class Product(Document): created_at: datetime # Can't index datetime directly # Workaround: Store as timestamp (numeric) class Product(Document): created_at: datetime created_timestamp: Indexed(float) @before_event(Insert) async def set_timestamp(self): self.created_timestamp = self.created_at.timestamp() # Query by timestamp recent = await Product.find( Product.created_timestamp > datetime(2024, 1, 1).timestamp() ).to_list() Index Maintenance Automatic Cleanup Indexes are cleaned up automatically on: - Document deletion - Field updates - Document updates # Delete - removes from all indexes await product.delete_self() # Update category - removes from old set, adds to new set await product.update(category=\"new_category\") Manual Index Check Check what's in an index: # Direct Redis access (for debugging) redis_client = Product._database # Check sorted set index price_index = await redis_client.zrange(\"idx:Product:price\", 0, -1, withscores=True) print(price_index) # [(doc_id, score), ...] # Check set index books_set = await redis_client.smembers(\"idx:Product:category:books\") print(books_set) # {doc_id, doc_id, ...} Advanced Patterns Geo-Spatial Indexing Redis supports geo-spatial queries, and Beanis provides built-in support through the GeoPoint type. from beanis import Document, Indexed, GeoPoint from beanis.odm.indexes import IndexManager class Store(Document): name: str location: Indexed[GeoPoint] # Geo index class Settings: name = \"stores\" # Create a store store = Store( name=\"Downtown Store\", location=GeoPoint(longitude=-122.4, latitude=37.8) ) await store.insert() # Find nearby stores (within 10km) from beanis.odm.indexes import IndexManager nearby_ids = await IndexManager.find_by_geo_radius( redis_client=Store._database, document_class=Store, field_name=\"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" ) # Get the actual documents nearby_stores = await Store.get_many(nearby_ids) # Find nearby stores with distances nearby_with_dist = await IndexManager.find_by_geo_radius_with_distance( redis_client=Store._database, document_class=Store, field_name=\"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" ) for store_id, distance in nearby_with_dist: print(f\"Store {store_id} is {distance} km away\") Supported units : m (meters), km (kilometers), mi (miles), ft (feet) How it works : - Uses Redis GEOADD command to store locations - Uses Redis GEORADIUS for proximity queries - Automatically maintains geo index on insert/update/delete Multi-Value Indexing For list fields, consider storing as separate documents or using a workaround: # Not directly supported class Product(Document): tags: List[str] # Can't index list # Workaround 1: Store each tag as indexed field (if few tags) class Product(Document): tag1: Indexed(str) = \"\" tag2: Indexed(str) = \"\" tag3: Indexed(str) = \"\" # Workaround 2: Manual secondary index class Product(Document): tags: List[str] async def index_tags(self): \"\"\"Manually add to tag sets\"\"\" for tag in self.tags: await self._database.sadd(f\"tags:{tag}\", self.id) Best Practices Index selectively - Only fields you'll query frequently Use correct types - Numeric for ranges, strings for exact match Monitor memory - Indexes consume Redis memory Lowercase strings - For case-insensitive queries Timestamps for dates - Convert datetime to float for range queries Test performance - Profile with realistic data volumes Comparison with MongoDB Feature MongoDB Beanis (Redis) Numeric range \u2705 Compound indexes \u2705 Sorted sets Exact match \u2705 Single field \u2705 Sets Full-text search \u2705 Text indexes \u274c Need RediSearch Compound indexes \u2705 Multi-field \u26a0\ufe0f Manual intersection Geo-spatial \u2705 2dsphere \u2705 Built-in (GEORADIUS) Index creation Manual \u2705 Automatic Array indexing \u2705 Multikey \u274c Not supported Next Steps Find Operations - Query using indexes Event Hooks - Maintain custom indexes Custom Encoders - Store complex types","title":"Indexes"},{"location":"tutorial/indexes/#indexes","text":"Beanis uses Redis data structures (Sorted Sets and Sets) to enable fast queries on your documents.","title":"Indexes"},{"location":"tutorial/indexes/#overview","text":"Unlike MongoDB which has a flexible indexing system, Redis indexing in Beanis is simpler but powerful: Sorted Sets - For numeric fields (support range queries) Sets - For string/categorical fields (exact match only) Geo Indexes - For geo-spatial fields (proximity queries) Hash - Document storage (no indexing)","title":"Overview"},{"location":"tutorial/indexes/#defining-indexes","text":"Use the Indexed() type annotation to mark fields for indexing: from beanis import Document, Indexed, GeoPoint class Product(Document): name: str # Not indexed - no queries price: Indexed[float] # Indexed - range queries category: Indexed[str] # Indexed - exact match stock: int # Not indexed - no queries location: Indexed[GeoPoint] # Indexed - geo queries class Settings: name = \"products\"","title":"Defining Indexes"},{"location":"tutorial/indexes/#numeric-indexes-sorted-sets","text":"Numeric fields ( int , float ) are stored in Redis Sorted Sets: class Product(Document): name: str price: Indexed(float) # Stored in sorted set stock: Indexed(int) # Stored in sorted set # Enables range queries expensive = await Product.find(Product.price > 100).to_list() affordable = await Product.find( Product.price >= 10, Product.price <= 50 ).to_list() # Stock queries low_stock = await Product.find(Product.stock < 10).to_list() Redis Structure : Key: idx:Product:price Type: Sorted Set Content: {document_id: price_value, ...} Example: idx:Product:price -> { \"prod_1\": 9.99, \"prod_2\": 15.50, \"prod_3\": 25.00 } Supported Operations : - == - Exact match - > , >= - Greater than - < , <= - Less than - Range combinations","title":"Numeric Indexes (Sorted Sets)"},{"location":"tutorial/indexes/#categorical-indexes-sets","text":"String fields are stored in Redis Sets (one set per unique value): class Product(Document): name: str category: Indexed(str) # Stored in sets brand: Indexed(str) # Stored in sets # Enables exact match queries books = await Product.find(Product.category == \"books\").to_list() nike = await Product.find(Product.brand == \"Nike\").to_list() Redis Structure : Key: idx:Product:category:books Type: Set Content: {document_id, document_id, ...} Example: idx:Product:category:books -> {\"prod_1\", \"prod_2\", \"prod_5\"} idx:Product:category:electronics -> {\"prod_3\", \"prod_4\"} Supported Operations : - == - Exact match only Not Supported : - Partial matches (use full-text search module) - Case-insensitive search - Wildcards","title":"Categorical Indexes (Sets)"},{"location":"tutorial/indexes/#index-creation","text":"Indexes are created automatically when you insert documents: # Define indexed fields class Product(Document): price: Indexed(float) category: Indexed(str) # Initialize await init_beanis(database=redis_client, document_models=[Product]) # Insert - indexes created automatically product = Product( name=\"Laptop\", price=999.99, category=\"electronics\" ) await product.insert() # This creates: # 1. Hash: Product:{id} # 2. Sorted Set entry: idx:Product:price # 3. Set entry: idx:Product:category:electronics","title":"Index Creation"},{"location":"tutorial/indexes/#index-updates","text":"Indexes are automatically updated when documents change: product = await Product.get(\"product_id_123\") # Update price - sorted set automatically updated await product.update(price=799.99) # Update category - sets automatically updated await product.update(category=\"computers\") # Old set entry removed, new set entry added","title":"Index Updates"},{"location":"tutorial/indexes/#multiple-indexes","text":"You can index multiple fields on the same document: class Product(Document): name: str price: Indexed(float) stock: Indexed(int) category: Indexed(str) brand: Indexed(str) # Each indexed field has its own Redis structure # - idx:Product:price (sorted set) # - idx:Product:stock (sorted set) # - idx:Product:category:{value} (sets) # - idx:Product:brand:{value} (sets)","title":"Multiple Indexes"},{"location":"tutorial/indexes/#query-performance","text":"","title":"Query Performance"},{"location":"tutorial/indexes/#indexed-queries-fast","text":"# These queries use indexes - O(log N) or O(1) products = await Product.find(Product.price < 100).to_list() books = await Product.find(Product.category == \"books\").to_list()","title":"Indexed Queries (Fast)"},{"location":"tutorial/indexes/#non-indexed-queries-slow","text":"# This field is NOT indexed - requires full scan class Product(Document): name: str # Not indexed! price: Indexed(float) # This will NOT work (name not indexed): # products = await Product.find(Product.name == \"Laptop\").to_list() # You must use get_all() and filter manually: all_products = await Product.all() laptops = [p for p in all_products if p.name == \"Laptop\"] Rule : Only Indexed() fields support .find() queries!","title":"Non-Indexed Queries (Slow)"},{"location":"tutorial/indexes/#index-storage-cost","text":"Each index uses Redis memory: class Product(Document): price: Indexed(float) # ~50 bytes per document category: Indexed(str) # ~40 bytes per document per unique value # For 10,000 products with 5 categories: # - price index: ~500 KB # - category index: ~200 KB (5 sets) # - documents: ~5-10 MB (depends on size) Best Practice : Only index fields you'll query frequently.","title":"Index Storage Cost"},{"location":"tutorial/indexes/#compound-queries","text":"Beanis supports querying multiple indexed fields: # Both fields must be indexed products = await Product.find( Product.price >= 10, Product.price <= 50, Product.category == \"electronics\" ).to_list() How it works : 1. Query price index (sorted set range) 2. Query category index (set members) 3. Intersect results (in Python)","title":"Compound Queries"},{"location":"tutorial/indexes/#index-limitations","text":"","title":"Index Limitations"},{"location":"tutorial/indexes/#no-full-text-search","text":"# Not supported - no partial matching # products = await Product.find(Product.name.contains(\"Lap\")).to_list() # Use Redis RediSearch module for full-text search # Or filter in memory: all_products = await Product.all() matches = [p for p in all_products if \"Lap\" in p.name]","title":"No Full-Text Search"},{"location":"tutorial/indexes/#no-case-insensitive-search","text":"# Case-sensitive only books = await Product.find(Product.category == \"Books\").to_list() # Won't find \"books\" # Workaround: Store lowercase class Product(Document): category_lower: Indexed(str) @before_event(Insert, Update) async def normalize_category(self): self.category_lower = self.category.lower() # Query lowercase books = await Product.find(Product.category_lower == \"books\").to_list()","title":"No Case-Insensitive Search"},{"location":"tutorial/indexes/#no-date-range-queries","text":"from datetime import datetime class Product(Document): created_at: datetime # Can't index datetime directly # Workaround: Store as timestamp (numeric) class Product(Document): created_at: datetime created_timestamp: Indexed(float) @before_event(Insert) async def set_timestamp(self): self.created_timestamp = self.created_at.timestamp() # Query by timestamp recent = await Product.find( Product.created_timestamp > datetime(2024, 1, 1).timestamp() ).to_list()","title":"No Date Range Queries"},{"location":"tutorial/indexes/#index-maintenance","text":"","title":"Index Maintenance"},{"location":"tutorial/indexes/#automatic-cleanup","text":"Indexes are cleaned up automatically on: - Document deletion - Field updates - Document updates # Delete - removes from all indexes await product.delete_self() # Update category - removes from old set, adds to new set await product.update(category=\"new_category\")","title":"Automatic Cleanup"},{"location":"tutorial/indexes/#manual-index-check","text":"Check what's in an index: # Direct Redis access (for debugging) redis_client = Product._database # Check sorted set index price_index = await redis_client.zrange(\"idx:Product:price\", 0, -1, withscores=True) print(price_index) # [(doc_id, score), ...] # Check set index books_set = await redis_client.smembers(\"idx:Product:category:books\") print(books_set) # {doc_id, doc_id, ...}","title":"Manual Index Check"},{"location":"tutorial/indexes/#advanced-patterns","text":"","title":"Advanced Patterns"},{"location":"tutorial/indexes/#geo-spatial-indexing","text":"Redis supports geo-spatial queries, and Beanis provides built-in support through the GeoPoint type. from beanis import Document, Indexed, GeoPoint from beanis.odm.indexes import IndexManager class Store(Document): name: str location: Indexed[GeoPoint] # Geo index class Settings: name = \"stores\" # Create a store store = Store( name=\"Downtown Store\", location=GeoPoint(longitude=-122.4, latitude=37.8) ) await store.insert() # Find nearby stores (within 10km) from beanis.odm.indexes import IndexManager nearby_ids = await IndexManager.find_by_geo_radius( redis_client=Store._database, document_class=Store, field_name=\"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" ) # Get the actual documents nearby_stores = await Store.get_many(nearby_ids) # Find nearby stores with distances nearby_with_dist = await IndexManager.find_by_geo_radius_with_distance( redis_client=Store._database, document_class=Store, field_name=\"location\", longitude=-122.4, latitude=37.8, radius=10, unit=\"km\" ) for store_id, distance in nearby_with_dist: print(f\"Store {store_id} is {distance} km away\") Supported units : m (meters), km (kilometers), mi (miles), ft (feet) How it works : - Uses Redis GEOADD command to store locations - Uses Redis GEORADIUS for proximity queries - Automatically maintains geo index on insert/update/delete","title":"Geo-Spatial Indexing"},{"location":"tutorial/indexes/#multi-value-indexing","text":"For list fields, consider storing as separate documents or using a workaround: # Not directly supported class Product(Document): tags: List[str] # Can't index list # Workaround 1: Store each tag as indexed field (if few tags) class Product(Document): tag1: Indexed(str) = \"\" tag2: Indexed(str) = \"\" tag3: Indexed(str) = \"\" # Workaround 2: Manual secondary index class Product(Document): tags: List[str] async def index_tags(self): \"\"\"Manually add to tag sets\"\"\" for tag in self.tags: await self._database.sadd(f\"tags:{tag}\", self.id)","title":"Multi-Value Indexing"},{"location":"tutorial/indexes/#best-practices","text":"Index selectively - Only fields you'll query frequently Use correct types - Numeric for ranges, strings for exact match Monitor memory - Indexes consume Redis memory Lowercase strings - For case-insensitive queries Timestamps for dates - Convert datetime to float for range queries Test performance - Profile with realistic data volumes","title":"Best Practices"},{"location":"tutorial/indexes/#comparison-with-mongodb","text":"Feature MongoDB Beanis (Redis) Numeric range \u2705 Compound indexes \u2705 Sorted sets Exact match \u2705 Single field \u2705 Sets Full-text search \u2705 Text indexes \u274c Need RediSearch Compound indexes \u2705 Multi-field \u26a0\ufe0f Manual intersection Geo-spatial \u2705 2dsphere \u2705 Built-in (GEORADIUS) Index creation Manual \u2705 Automatic Array indexing \u2705 Multikey \u274c Not supported","title":"Comparison with MongoDB"},{"location":"tutorial/indexes/#next-steps","text":"Find Operations - Query using indexes Event Hooks - Maintain custom indexes Custom Encoders - Store complex types","title":"Next Steps"},{"location":"tutorial/initialization/","text":"Initialization Before you can use Beanis, you need to initialize it with your Redis client and document models. Basic Initialization import asyncio from redis.asyncio import Redis from beanis import Document, init_beanis class Product(Document): name: str price: float class Settings: name = \"products\" async def init(): # Create Redis client # IMPORTANT: decode_responses=True is required! client = Redis( host=\"localhost\", port=6379, db=0, decode_responses=True ) # Initialize Beanis await init_beanis(database=client, document_models=[Product]) if __name__ == \"__main__\": asyncio.run(init()) Multiple Document Models You can register multiple document models at once: from beanis import init_beanis class Product(Document): name: str price: float class User(Document): name: str email: str class Order(Document): user_id: str product_ids: List[str] await init_beanis( database=client, document_models=[Product, User, Order] ) Redis Client Configuration Basic Configuration from redis.asyncio import Redis client = Redis( host=\"localhost\", port=6379, db=0, # Redis database number (0-15) decode_responses=True, # Required! password=\"your-password\", # If Redis has auth ) Connection Pool For production, use a connection pool: from redis.asyncio import Redis, ConnectionPool pool = ConnectionPool( host=\"localhost\", port=6379, db=0, decode_responses=True, max_connections=50, ) client = Redis(connection_pool=pool) Redis URL You can also use a connection URL: client = Redis.from_url( \"redis://localhost:6379/0\", decode_responses=True ) Application Integration FastAPI Example from fastapi import FastAPI from redis.asyncio import Redis from beanis import init_beanis from contextlib import asynccontextmanager @asynccontextmanager async def lifespan(app: FastAPI): # Startup client = Redis( host=\"localhost\", port=6379, db=0, decode_responses=True ) await init_beanis(database=client, document_models=[Product, User]) yield # Shutdown await client.close() app = FastAPI(lifespan=lifespan) @app.get(\"/products\") async def get_products(): products = await Product.all() return products Important Notes Always use decode_responses=True - Beanis requires string responses from Redis Initialize once - Call init_beanis once at application startup Close connections - Always close the Redis client on shutdown Next Steps Defining Documents - Learn about document models Insert Operations - Create documents Find Operations - Query documents","title":"Initialization"},{"location":"tutorial/initialization/#initialization","text":"Before you can use Beanis, you need to initialize it with your Redis client and document models.","title":"Initialization"},{"location":"tutorial/initialization/#basic-initialization","text":"import asyncio from redis.asyncio import Redis from beanis import Document, init_beanis class Product(Document): name: str price: float class Settings: name = \"products\" async def init(): # Create Redis client # IMPORTANT: decode_responses=True is required! client = Redis( host=\"localhost\", port=6379, db=0, decode_responses=True ) # Initialize Beanis await init_beanis(database=client, document_models=[Product]) if __name__ == \"__main__\": asyncio.run(init())","title":"Basic Initialization"},{"location":"tutorial/initialization/#multiple-document-models","text":"You can register multiple document models at once: from beanis import init_beanis class Product(Document): name: str price: float class User(Document): name: str email: str class Order(Document): user_id: str product_ids: List[str] await init_beanis( database=client, document_models=[Product, User, Order] )","title":"Multiple Document Models"},{"location":"tutorial/initialization/#redis-client-configuration","text":"","title":"Redis Client Configuration"},{"location":"tutorial/initialization/#basic-configuration","text":"from redis.asyncio import Redis client = Redis( host=\"localhost\", port=6379, db=0, # Redis database number (0-15) decode_responses=True, # Required! password=\"your-password\", # If Redis has auth )","title":"Basic Configuration"},{"location":"tutorial/initialization/#connection-pool","text":"For production, use a connection pool: from redis.asyncio import Redis, ConnectionPool pool = ConnectionPool( host=\"localhost\", port=6379, db=0, decode_responses=True, max_connections=50, ) client = Redis(connection_pool=pool)","title":"Connection Pool"},{"location":"tutorial/initialization/#redis-url","text":"You can also use a connection URL: client = Redis.from_url( \"redis://localhost:6379/0\", decode_responses=True )","title":"Redis URL"},{"location":"tutorial/initialization/#application-integration","text":"","title":"Application Integration"},{"location":"tutorial/initialization/#fastapi-example","text":"from fastapi import FastAPI from redis.asyncio import Redis from beanis import init_beanis from contextlib import asynccontextmanager @asynccontextmanager async def lifespan(app: FastAPI): # Startup client = Redis( host=\"localhost\", port=6379, db=0, decode_responses=True ) await init_beanis(database=client, document_models=[Product, User]) yield # Shutdown await client.close() app = FastAPI(lifespan=lifespan) @app.get(\"/products\") async def get_products(): products = await Product.all() return products","title":"FastAPI Example"},{"location":"tutorial/initialization/#important-notes","text":"Always use decode_responses=True - Beanis requires string responses from Redis Initialize once - Call init_beanis once at application startup Close connections - Always close the Redis client on shutdown","title":"Important Notes"},{"location":"tutorial/initialization/#next-steps","text":"Defining Documents - Learn about document models Insert Operations - Create documents Find Operations - Query documents","title":"Next Steps"},{"location":"tutorial/inserting-into-the-database/","text":"Insert Documents Beanis documents behave just like Pydantic models (because they subclass pydantic.BaseModel ). Hence, a document can be created in a similar fashion to Pydantic: from typing import Optional from pydantic import BaseModel from beanis import Document class Category(BaseModel): name: str description: str class Product(Document): name: str description: Optional[str] = None price: float category: Category class Settings: name = \"products\" # Create a product chocolate = Category( name=\"Chocolate\", description=\"A preparation of roasted and ground cacao seeds.\" ) product = Product( name=\"Tony's Chocolonely\", description=\"Awesome chocolate bar\", price=5.95, category=chocolate ) Insert One To insert the document into Redis, use the insert() method: await product.insert() print(product.id) # Auto-generated UUID The document is now stored in Redis as a Hash at key Product:{id} . Insert with Custom ID You can specify a custom ID: product = Product( id=\"prod_001\", name=\"Tony's Chocolonely\", price=5.95, category=chocolate ) await product.insert() Insert with TTL Documents can expire automatically using TTL (time-to-live): # Expire after 1 hour await product.insert(ttl=3600) Insert Many For bulk inserts, use insert_many() for better performance: products = [ Product(name=f\"Product {i}\", price=float(i * 10), category=chocolate) for i in range(100) ] await Product.insert_many(products) This uses Redis pipelines for efficient batch operations. Response The insert() method returns the document with the populated id field: product = Product(name=\"New Product\", price=9.99, category=chocolate) result = await product.insert() print(result.id) # UUID automatically generated print(product.id) # Same as result.id Replace on Insert By default, if a document with the same ID already exists, insert() will overwrite it. This is different from MongoDB's behavior. # First insert product = Product(id=\"prod_001\", name=\"Original\", price=10.0, category=chocolate) await product.insert() # Second insert with same ID - replaces the first product = Product(id=\"prod_001\", name=\"Updated\", price=15.0, category=chocolate) await product.insert() # Replaces the original To check if a document exists first: if not await Product.exists(\"prod_001\"): await product.insert() Event Hooks You can use event hooks to run code before/after inserts: from beanis import Document, before_event, after_event, Insert class Product(Document): name: str price: float @before_event(Insert) async def validate_price(self): if self.price < 0: raise ValueError(\"Price cannot be negative\") @after_event(Insert) async def log_creation(self): print(f\"Created product: {self.name}\") product = Product(name=\"Test\", price=9.99) await product.insert() # Runs validate_price, then insert, then log_creation Examples Insert with Validation from pydantic import field_validator class Product(Document): name: str price: float stock: int @field_validator('price') @classmethod def price_must_be_positive(cls, v): if v < 0: raise ValueError('Price must be positive') return v product = Product(name=\"Test\", price=-10, stock=100) await product.insert() # Raises ValidationError Insert with Complex Types from typing import List from datetime import datetime class Product(Document): name: str price: float tags: List[str] created_at: datetime class Settings: name = \"products\" product = Product( name=\"Laptop\", price=999.99, tags=[\"electronics\", \"computers\"], created_at=datetime.now() ) await product.insert() Bulk Insert with Error Handling products = [Product(name=f\"Product {i}\", price=float(i)) for i in range(100)] try: await Product.insert_many(products) print(f\"Inserted {len(products)} products\") except Exception as e: print(f\"Error inserting products: {e}\") Performance Tips Use insert_many() for bulk operations - Much faster than individual inserts Set appropriate TTL - Automatically clean up old data Use pipelines - insert_many() already does this Avoid validation on insert if data is trusted - Pydantic validation can be expensive Next Steps Find Operations - Query documents Update Operations - Modify documents Delete Operations - Remove documents","title":"Inserting into the database"},{"location":"tutorial/inserting-into-the-database/#insert-documents","text":"Beanis documents behave just like Pydantic models (because they subclass pydantic.BaseModel ). Hence, a document can be created in a similar fashion to Pydantic: from typing import Optional from pydantic import BaseModel from beanis import Document class Category(BaseModel): name: str description: str class Product(Document): name: str description: Optional[str] = None price: float category: Category class Settings: name = \"products\" # Create a product chocolate = Category( name=\"Chocolate\", description=\"A preparation of roasted and ground cacao seeds.\" ) product = Product( name=\"Tony's Chocolonely\", description=\"Awesome chocolate bar\", price=5.95, category=chocolate )","title":"Insert Documents"},{"location":"tutorial/inserting-into-the-database/#insert-one","text":"To insert the document into Redis, use the insert() method: await product.insert() print(product.id) # Auto-generated UUID The document is now stored in Redis as a Hash at key Product:{id} .","title":"Insert One"},{"location":"tutorial/inserting-into-the-database/#insert-with-custom-id","text":"You can specify a custom ID: product = Product( id=\"prod_001\", name=\"Tony's Chocolonely\", price=5.95, category=chocolate ) await product.insert()","title":"Insert with Custom ID"},{"location":"tutorial/inserting-into-the-database/#insert-with-ttl","text":"Documents can expire automatically using TTL (time-to-live): # Expire after 1 hour await product.insert(ttl=3600)","title":"Insert with TTL"},{"location":"tutorial/inserting-into-the-database/#insert-many","text":"For bulk inserts, use insert_many() for better performance: products = [ Product(name=f\"Product {i}\", price=float(i * 10), category=chocolate) for i in range(100) ] await Product.insert_many(products) This uses Redis pipelines for efficient batch operations.","title":"Insert Many"},{"location":"tutorial/inserting-into-the-database/#response","text":"The insert() method returns the document with the populated id field: product = Product(name=\"New Product\", price=9.99, category=chocolate) result = await product.insert() print(result.id) # UUID automatically generated print(product.id) # Same as result.id","title":"Response"},{"location":"tutorial/inserting-into-the-database/#replace-on-insert","text":"By default, if a document with the same ID already exists, insert() will overwrite it. This is different from MongoDB's behavior. # First insert product = Product(id=\"prod_001\", name=\"Original\", price=10.0, category=chocolate) await product.insert() # Second insert with same ID - replaces the first product = Product(id=\"prod_001\", name=\"Updated\", price=15.0, category=chocolate) await product.insert() # Replaces the original To check if a document exists first: if not await Product.exists(\"prod_001\"): await product.insert()","title":"Replace on Insert"},{"location":"tutorial/inserting-into-the-database/#event-hooks","text":"You can use event hooks to run code before/after inserts: from beanis import Document, before_event, after_event, Insert class Product(Document): name: str price: float @before_event(Insert) async def validate_price(self): if self.price < 0: raise ValueError(\"Price cannot be negative\") @after_event(Insert) async def log_creation(self): print(f\"Created product: {self.name}\") product = Product(name=\"Test\", price=9.99) await product.insert() # Runs validate_price, then insert, then log_creation","title":"Event Hooks"},{"location":"tutorial/inserting-into-the-database/#examples","text":"","title":"Examples"},{"location":"tutorial/inserting-into-the-database/#insert-with-validation","text":"from pydantic import field_validator class Product(Document): name: str price: float stock: int @field_validator('price') @classmethod def price_must_be_positive(cls, v): if v < 0: raise ValueError('Price must be positive') return v product = Product(name=\"Test\", price=-10, stock=100) await product.insert() # Raises ValidationError","title":"Insert with Validation"},{"location":"tutorial/inserting-into-the-database/#insert-with-complex-types","text":"from typing import List from datetime import datetime class Product(Document): name: str price: float tags: List[str] created_at: datetime class Settings: name = \"products\" product = Product( name=\"Laptop\", price=999.99, tags=[\"electronics\", \"computers\"], created_at=datetime.now() ) await product.insert()","title":"Insert with Complex Types"},{"location":"tutorial/inserting-into-the-database/#bulk-insert-with-error-handling","text":"products = [Product(name=f\"Product {i}\", price=float(i)) for i in range(100)] try: await Product.insert_many(products) print(f\"Inserted {len(products)} products\") except Exception as e: print(f\"Error inserting products: {e}\")","title":"Bulk Insert with Error Handling"},{"location":"tutorial/inserting-into-the-database/#performance-tips","text":"Use insert_many() for bulk operations - Much faster than individual inserts Set appropriate TTL - Automatically clean up old data Use pipelines - insert_many() already does this Avoid validation on insert if data is trusted - Pydantic validation can be expensive","title":"Performance Tips"},{"location":"tutorial/inserting-into-the-database/#next-steps","text":"Find Operations - Query documents Update Operations - Modify documents Delete Operations - Remove documents","title":"Next Steps"},{"location":"tutorial/tutorial-overview/","text":"Beanis Tutorial Welcome to the Beanis tutorial! This guide will help you get started with Beanis, a Redis ODM (Object-Document Mapper) for Python. What You'll Learn This tutorial covers everything you need to build applications with Beanis and Redis: Document modeling - Define type-safe document structures CRUD operations - Create, read, update, and delete documents Indexing & queries - Fast queries using Redis Sorted Sets and Sets Event hooks - Run custom logic on document lifecycle events Best practices - Performance tips and patterns Tutorial Structure 1. Getting Started Start here if you're new to Beanis: Defining a Document Learn how to define document models with Pydantic: - Basic document structure - Field types and validation - Indexed fields for queries - Nested objects and complex types - Custom encoders for special types (NumPy, PyTorch) Start here \u2192 defining-a-document.md Initialization Set up Beanis with your Redis client: - Redis client configuration - Initialize Beanis with document models - FastAPI integration - Multiple database support Next step \u2192 init.md 2. Core Operations Learn the essential document operations: Insert Documents Create new documents in Redis: - Insert single documents - Bulk insert operations - Insert with TTL (auto-expiration) - Replace vs. insert behavior - Event hooks on insert Learn inserting \u2192 insert.md Find Documents Query documents efficiently: - Get by ID (O(1) lookup) - Range queries on numeric fields - Exact match on categorical fields - Get all documents - Pagination and sorting Learn querying \u2192 find.md Update Documents Modify existing documents: - Update specific fields - Save entire document - Atomic field operations (increment/decrement) - Update with validation - Event hooks on update Learn updating \u2192 update.md Delete Documents Remove documents from Redis: - Delete single document - Delete multiple documents - Delete all documents - TTL as alternative to deletion - Event hooks on delete Learn deleting \u2192 delete.md 3. Advanced Topics Master advanced Beanis features: Indexes Understand Redis indexing: - Numeric indexes (Sorted Sets) for range queries - Categorical indexes (Sets) for exact match - Index creation and maintenance - Query performance optimization - Index limitations Learn indexing \u2192 indexes.md Event Hooks (Actions) Run custom logic on document lifecycle: - Before/after insert hooks - Before/after update hooks - Before/after delete hooks - Common patterns (timestamps, validation, audit logs) - Hook execution order Learn hooks \u2192 actions.md Custom Encoders Serialize complex Python types to Redis: - NumPy arrays, PyTorch tensors - Custom classes and dataclasses - Binary data (images, audio) - Performance optimization - Versioning and error handling Learn custom encoders \u2192 custom-encoders.md Geo-Spatial Indexing Build location-based features with sub-millisecond queries: - Store/restaurant locators - Delivery radius validation - Real-time vehicle tracking - Geo-fencing applications - Complete delivery service example with benchmarks Learn geo-spatial \u2192 geo-spatial.md Quick Start Example from redis.asyncio import Redis from beanis import Document, Indexed, init_beanis # 1. Define a document class Product(Document): name: str price: Indexed(float) # Indexed for range queries category: Indexed(str) # Indexed for exact match class Settings: name = \"products\" # 2. Initialize async def setup(): client = Redis(decode_responses=True) await init_beanis(database=client, document_models=[Product]) # 3. Use it! async def example(): # Insert product = Product(name=\"Laptop\", price=999.99, category=\"electronics\") await product.insert() # Find by ID found = await Product.get(product.id) # Query by indexed field expensive = await Product.find(Product.price > 500).to_list() electronics = await Product.find(Product.category == \"electronics\").to_list() # Update await product.update(price=899.99) # Delete await product.delete_self() Recommended Learning Path For Beginners Defining a Document - Learn document structure Initialization - Set up Beanis Insert Documents - Create documents Find Documents - Query documents Update Documents - Modify documents Delete Documents - Remove documents For Advanced Users Start with the basics above, then explore: 7. Indexes - Optimize queries 8. Event Hooks - Lifecycle events 9. Custom Encoders - Complex type serialization 10. Geo-Spatial Indexing - Location-based features Key Concepts Redis Storage Model Beanis stores documents as Redis Hashes : Product:prod_123 -> {name: \"Laptop\", price: \"999.99\", category: \"electronics\"} Indexing Beanis automatically creates indexes for Indexed() fields: Numeric fields \u2192 Redis Sorted Set (range queries) idx:Product:price -> {prod_1: 999.99, prod_2: 1299.99, ...} String fields \u2192 Redis Set per value (exact match) idx:Product:category:electronics -> {prod_1, prod_3, ...} Type Safety Beanis uses Pydantic for validation: class Product(Document): price: float # Type-checked at runtime # This raises ValidationError: product = Product(name=\"Test\", price=\"invalid\") # \u274c Differences from MongoDB/Beanie If you're coming from MongoDB/Beanie, note these differences: Feature MongoDB/Beanie Beanis (Redis) Storage Documents (BSON) Hashes (key-value) Queries Full query language Indexed fields only Relations Link, BackLink Use embedded documents Aggregations Pipeline Not supported Transactions Multi-document Single document Full-text search Text indexes Use RediSearch module Best practice : Use embedded Pydantic models instead of document relations. Performance Tips Use batch operations - insert_many() , get_many() , delete_many() Index selectively - Only fields you'll query frequently Use TTL - Automatically expire temporary data Leverage atomic operations - increment_field() for counters Profile with realistic data - Test performance at scale Need Help? Documentation : Main Docs Getting Started : Getting Started Guide GitHub Issues : Report bugs or request features Examples : Check the tests folder for more examples What's Not Covered Features not supported in the Redis version: \u274c Relations (Link/BackLink) - Use embedded documents \u274c Migrations - Not needed (schema-less) \u274c Aggregations - Use Python for data processing \u274c Views - Not applicable to Redis \u274c Time Series - Use Redis TimeSeries module or TTL Next Steps Ready to start? Begin with Defining a Document ! Already familiar with the basics? Jump to Indexes or Event Hooks for advanced features. Happy coding! \ud83d\ude80","title":"Tutorial Overview"},{"location":"tutorial/tutorial-overview/#beanis-tutorial","text":"Welcome to the Beanis tutorial! This guide will help you get started with Beanis, a Redis ODM (Object-Document Mapper) for Python.","title":"Beanis Tutorial"},{"location":"tutorial/tutorial-overview/#what-youll-learn","text":"This tutorial covers everything you need to build applications with Beanis and Redis: Document modeling - Define type-safe document structures CRUD operations - Create, read, update, and delete documents Indexing & queries - Fast queries using Redis Sorted Sets and Sets Event hooks - Run custom logic on document lifecycle events Best practices - Performance tips and patterns","title":"What You'll Learn"},{"location":"tutorial/tutorial-overview/#tutorial-structure","text":"","title":"Tutorial Structure"},{"location":"tutorial/tutorial-overview/#1-getting-started","text":"Start here if you're new to Beanis:","title":"1. Getting Started"},{"location":"tutorial/tutorial-overview/#defining-a-document","text":"Learn how to define document models with Pydantic: - Basic document structure - Field types and validation - Indexed fields for queries - Nested objects and complex types - Custom encoders for special types (NumPy, PyTorch) Start here \u2192 defining-a-document.md","title":"Defining a Document"},{"location":"tutorial/tutorial-overview/#initialization","text":"Set up Beanis with your Redis client: - Redis client configuration - Initialize Beanis with document models - FastAPI integration - Multiple database support Next step \u2192 init.md","title":"Initialization"},{"location":"tutorial/tutorial-overview/#2-core-operations","text":"Learn the essential document operations:","title":"2. Core Operations"},{"location":"tutorial/tutorial-overview/#insert-documents","text":"Create new documents in Redis: - Insert single documents - Bulk insert operations - Insert with TTL (auto-expiration) - Replace vs. insert behavior - Event hooks on insert Learn inserting \u2192 insert.md","title":"Insert Documents"},{"location":"tutorial/tutorial-overview/#find-documents","text":"Query documents efficiently: - Get by ID (O(1) lookup) - Range queries on numeric fields - Exact match on categorical fields - Get all documents - Pagination and sorting Learn querying \u2192 find.md","title":"Find Documents"},{"location":"tutorial/tutorial-overview/#update-documents","text":"Modify existing documents: - Update specific fields - Save entire document - Atomic field operations (increment/decrement) - Update with validation - Event hooks on update Learn updating \u2192 update.md","title":"Update Documents"},{"location":"tutorial/tutorial-overview/#delete-documents","text":"Remove documents from Redis: - Delete single document - Delete multiple documents - Delete all documents - TTL as alternative to deletion - Event hooks on delete Learn deleting \u2192 delete.md","title":"Delete Documents"},{"location":"tutorial/tutorial-overview/#3-advanced-topics","text":"Master advanced Beanis features:","title":"3. Advanced Topics"},{"location":"tutorial/tutorial-overview/#indexes","text":"Understand Redis indexing: - Numeric indexes (Sorted Sets) for range queries - Categorical indexes (Sets) for exact match - Index creation and maintenance - Query performance optimization - Index limitations Learn indexing \u2192 indexes.md","title":"Indexes"},{"location":"tutorial/tutorial-overview/#event-hooks-actions","text":"Run custom logic on document lifecycle: - Before/after insert hooks - Before/after update hooks - Before/after delete hooks - Common patterns (timestamps, validation, audit logs) - Hook execution order Learn hooks \u2192 actions.md","title":"Event Hooks (Actions)"},{"location":"tutorial/tutorial-overview/#custom-encoders","text":"Serialize complex Python types to Redis: - NumPy arrays, PyTorch tensors - Custom classes and dataclasses - Binary data (images, audio) - Performance optimization - Versioning and error handling Learn custom encoders \u2192 custom-encoders.md","title":"Custom Encoders"},{"location":"tutorial/tutorial-overview/#geo-spatial-indexing","text":"Build location-based features with sub-millisecond queries: - Store/restaurant locators - Delivery radius validation - Real-time vehicle tracking - Geo-fencing applications - Complete delivery service example with benchmarks Learn geo-spatial \u2192 geo-spatial.md","title":"Geo-Spatial Indexing"},{"location":"tutorial/tutorial-overview/#quick-start-example","text":"from redis.asyncio import Redis from beanis import Document, Indexed, init_beanis # 1. Define a document class Product(Document): name: str price: Indexed(float) # Indexed for range queries category: Indexed(str) # Indexed for exact match class Settings: name = \"products\" # 2. Initialize async def setup(): client = Redis(decode_responses=True) await init_beanis(database=client, document_models=[Product]) # 3. Use it! async def example(): # Insert product = Product(name=\"Laptop\", price=999.99, category=\"electronics\") await product.insert() # Find by ID found = await Product.get(product.id) # Query by indexed field expensive = await Product.find(Product.price > 500).to_list() electronics = await Product.find(Product.category == \"electronics\").to_list() # Update await product.update(price=899.99) # Delete await product.delete_self()","title":"Quick Start Example"},{"location":"tutorial/tutorial-overview/#recommended-learning-path","text":"","title":"Recommended Learning Path"},{"location":"tutorial/tutorial-overview/#for-beginners","text":"Defining a Document - Learn document structure Initialization - Set up Beanis Insert Documents - Create documents Find Documents - Query documents Update Documents - Modify documents Delete Documents - Remove documents","title":"For Beginners"},{"location":"tutorial/tutorial-overview/#for-advanced-users","text":"Start with the basics above, then explore: 7. Indexes - Optimize queries 8. Event Hooks - Lifecycle events 9. Custom Encoders - Complex type serialization 10. Geo-Spatial Indexing - Location-based features","title":"For Advanced Users"},{"location":"tutorial/tutorial-overview/#key-concepts","text":"","title":"Key Concepts"},{"location":"tutorial/tutorial-overview/#redis-storage-model","text":"Beanis stores documents as Redis Hashes : Product:prod_123 -> {name: \"Laptop\", price: \"999.99\", category: \"electronics\"}","title":"Redis Storage Model"},{"location":"tutorial/tutorial-overview/#indexing","text":"Beanis automatically creates indexes for Indexed() fields: Numeric fields \u2192 Redis Sorted Set (range queries) idx:Product:price -> {prod_1: 999.99, prod_2: 1299.99, ...} String fields \u2192 Redis Set per value (exact match) idx:Product:category:electronics -> {prod_1, prod_3, ...}","title":"Indexing"},{"location":"tutorial/tutorial-overview/#type-safety","text":"Beanis uses Pydantic for validation: class Product(Document): price: float # Type-checked at runtime # This raises ValidationError: product = Product(name=\"Test\", price=\"invalid\") # \u274c","title":"Type Safety"},{"location":"tutorial/tutorial-overview/#differences-from-mongodbbeanie","text":"If you're coming from MongoDB/Beanie, note these differences: Feature MongoDB/Beanie Beanis (Redis) Storage Documents (BSON) Hashes (key-value) Queries Full query language Indexed fields only Relations Link, BackLink Use embedded documents Aggregations Pipeline Not supported Transactions Multi-document Single document Full-text search Text indexes Use RediSearch module Best practice : Use embedded Pydantic models instead of document relations.","title":"Differences from MongoDB/Beanie"},{"location":"tutorial/tutorial-overview/#performance-tips","text":"Use batch operations - insert_many() , get_many() , delete_many() Index selectively - Only fields you'll query frequently Use TTL - Automatically expire temporary data Leverage atomic operations - increment_field() for counters Profile with realistic data - Test performance at scale","title":"Performance Tips"},{"location":"tutorial/tutorial-overview/#need-help","text":"Documentation : Main Docs Getting Started : Getting Started Guide GitHub Issues : Report bugs or request features Examples : Check the tests folder for more examples","title":"Need Help?"},{"location":"tutorial/tutorial-overview/#whats-not-covered","text":"Features not supported in the Redis version: \u274c Relations (Link/BackLink) - Use embedded documents \u274c Migrations - Not needed (schema-less) \u274c Aggregations - Use Python for data processing \u274c Views - Not applicable to Redis \u274c Time Series - Use Redis TimeSeries module or TTL","title":"What's Not Covered"},{"location":"tutorial/tutorial-overview/#next-steps","text":"Ready to start? Begin with Defining a Document ! Already familiar with the basics? Jump to Indexes or Event Hooks for advanced features. Happy coding! \ud83d\ude80","title":"Next Steps"},{"location":"tutorial/updating-documents/","text":"Update Documents Beanis provides several ways to update documents in Redis. Update Specific Fields Use the update() method to modify specific fields: from beanis import Document, Indexed class Product(Document): name: str price: Indexed(float) stock: int class Settings: name = \"products\" # Get a product product = await Product.get(\"product_id_123\") # Update specific fields await product.update( price=7.99, stock=50 ) # Other fields remain unchanged print(product.name) # Original value preserved Save Method The save() method inserts new documents or updates existing ones: # Create new document product = Product(name=\"New Product\", price=9.99, stock=10) await product.save() # Inserts # Modify and save again product.price = 12.99 await product.save() # Updates The save() method replaces the entire document in Redis. Update Fields Directly Modify model attributes and call update() : product = await Product.get(\"product_id_123\") # Modify attributes product.price = 15.99 product.stock = 100 # Save changes await product.update(price=product.price, stock=product.stock) Atomic Field Operations Increment/Decrement Numeric Fields Use increment_field() for atomic numeric updates: # Decrement stock atomically (thread-safe) new_stock = await product.increment_field(\"stock\", -1) print(f\"Stock after sale: {new_stock}\") # Increment by positive value new_stock = await product.increment_field(\"stock\", 10) print(f\"Stock after restock: {new_stock}\") This uses Redis HINCRBY for atomic operations - perfect for inventory management, counters, etc. Set Single Field Update a single field without loading the entire document: # Update just the price field await product.set_field(\"price\", 8.99) # Get just the price field price = await product.get_field(\"price\") print(f\"Current price: {price}\") This is efficient when you only need to modify one field. Update Multiple Documents Update several documents by ID: # Update product prices products = await Product.get_many([\"id1\", \"id2\", \"id3\"]) for product in products: if product: await product.update(price=product.price * 1.1) # 10% increase For better performance with many updates, use Redis pipelines: # More efficient for bulk updates products = await Product.get_many(product_ids) # Prepare updates updates = [] for product in products: if product: product.price = product.price * 1.1 updates.append(product) # Execute in batch (uses pipeline internally) for product in updates: await product.update(price=product.price) Update with Validation Updates trigger Pydantic validation: from pydantic import field_validator class Product(Document): name: str price: float stock: int @field_validator('price') @classmethod def price_must_be_positive(cls, v): if v < 0: raise ValueError('Price must be positive') return v product = await Product.get(\"product_id_123\") # This will raise ValidationError await product.update(price=-5.0) # \u274c ValidationError! Update with Event Hooks Run custom logic before/after updates: from beanis import before_event, after_event, Update class Product(Document): name: str price: float old_price: float = 0.0 @before_event(Update) async def store_old_price(self): # Get current price before update existing = await Product.get(self.id) if existing: self.old_price = existing.price @after_event(Update) async def log_price_change(self): if self.old_price != self.price: print(f\"Price changed: ${self.old_price} -> ${self.price}\") product = await Product.get(\"product_id_123\") await product.update(price=7.99) # Output: Price changed: $5.99 -> $7.99 Update with TTL Update and set/modify expiration: # Update with new TTL await product.update(price=9.99) await product.set_ttl(3600) # Expire in 1 hour # Check TTL ttl = await product.get_ttl() print(f\"Expires in {ttl} seconds\") # Remove TTL (make permanent) await product.persist() Update Nested Objects Beanis stores nested Pydantic models as JSON strings in Redis: from pydantic import BaseModel class Category(BaseModel): name: str description: str class Product(Document): name: str price: float category: Category class Settings: name = \"products\" product = await Product.get(\"product_id_123\") # Update nested object (replaces entire category) await product.update( category=Category(name=\"Electronics\", description=\"Electronic devices\") ) # To modify nested field, recreate the object new_category = product.category new_category.description = \"Updated description\" await product.update(category=new_category) Important Notes update() replaces fields - Only specified fields are updated, others remain unchanged save() replaces entire document - All fields are overwritten Atomic operations - Use increment_field() for thread-safe numeric updates Validation always runs - Updates trigger Pydantic validation TTL is preserved - Unless explicitly changed, document TTL remains unchanged Performance Tips Use increment_field() for counters - Atomic and efficient Batch updates when possible - Reduces Redis round trips Update only changed fields - Don't call update() with all fields Use set_field() for single fields - More efficient than loading entire document Examples Inventory Management async def process_sale(product_id: str, quantity: int): \"\"\"Atomic stock update for sale\"\"\" product = await Product.get(product_id) # Check stock if product.stock < quantity: raise ValueError(\"Insufficient stock\") # Atomic decrement new_stock = await product.increment_field(\"stock\", -quantity) return new_stock # Process a sale remaining = await process_sale(\"prod_123\", quantity=5) print(f\"Stock remaining: {remaining}\") Price Updates with History from datetime import datetime from typing import List class PriceHistory(BaseModel): price: float timestamp: datetime class Product(Document): name: str price: float price_history: List[PriceHistory] = [] async def update_price(self, new_price: float): \"\"\"Update price and maintain history\"\"\" # Add current price to history self.price_history.append( PriceHistory(price=self.price, timestamp=datetime.now()) ) # Update to new price await self.update( price=new_price, price_history=self.price_history ) product = await Product.get(\"prod_123\") await product.update_price(7.99) Conditional Updates async def apply_discount(product_id: str, discount_pct: float): \"\"\"Apply discount only if price is above threshold\"\"\" product = await Product.get(product_id) if product.price >= 10.0: new_price = product.price * (1 - discount_pct / 100) await product.update(price=round(new_price, 2)) return True return False # Apply 20% discount to expensive items discounted = await apply_discount(\"prod_123\", 20.0) Next Steps Delete Operations - Remove documents Indexes - Query optimization Event Hooks - Document lifecycle events","title":"Updating documents"},{"location":"tutorial/updating-documents/#update-documents","text":"Beanis provides several ways to update documents in Redis.","title":"Update Documents"},{"location":"tutorial/updating-documents/#update-specific-fields","text":"Use the update() method to modify specific fields: from beanis import Document, Indexed class Product(Document): name: str price: Indexed(float) stock: int class Settings: name = \"products\" # Get a product product = await Product.get(\"product_id_123\") # Update specific fields await product.update( price=7.99, stock=50 ) # Other fields remain unchanged print(product.name) # Original value preserved","title":"Update Specific Fields"},{"location":"tutorial/updating-documents/#save-method","text":"The save() method inserts new documents or updates existing ones: # Create new document product = Product(name=\"New Product\", price=9.99, stock=10) await product.save() # Inserts # Modify and save again product.price = 12.99 await product.save() # Updates The save() method replaces the entire document in Redis.","title":"Save Method"},{"location":"tutorial/updating-documents/#update-fields-directly","text":"Modify model attributes and call update() : product = await Product.get(\"product_id_123\") # Modify attributes product.price = 15.99 product.stock = 100 # Save changes await product.update(price=product.price, stock=product.stock)","title":"Update Fields Directly"},{"location":"tutorial/updating-documents/#atomic-field-operations","text":"","title":"Atomic Field Operations"},{"location":"tutorial/updating-documents/#incrementdecrement-numeric-fields","text":"Use increment_field() for atomic numeric updates: # Decrement stock atomically (thread-safe) new_stock = await product.increment_field(\"stock\", -1) print(f\"Stock after sale: {new_stock}\") # Increment by positive value new_stock = await product.increment_field(\"stock\", 10) print(f\"Stock after restock: {new_stock}\") This uses Redis HINCRBY for atomic operations - perfect for inventory management, counters, etc.","title":"Increment/Decrement Numeric Fields"},{"location":"tutorial/updating-documents/#set-single-field","text":"Update a single field without loading the entire document: # Update just the price field await product.set_field(\"price\", 8.99) # Get just the price field price = await product.get_field(\"price\") print(f\"Current price: {price}\") This is efficient when you only need to modify one field.","title":"Set Single Field"},{"location":"tutorial/updating-documents/#update-multiple-documents","text":"Update several documents by ID: # Update product prices products = await Product.get_many([\"id1\", \"id2\", \"id3\"]) for product in products: if product: await product.update(price=product.price * 1.1) # 10% increase For better performance with many updates, use Redis pipelines: # More efficient for bulk updates products = await Product.get_many(product_ids) # Prepare updates updates = [] for product in products: if product: product.price = product.price * 1.1 updates.append(product) # Execute in batch (uses pipeline internally) for product in updates: await product.update(price=product.price)","title":"Update Multiple Documents"},{"location":"tutorial/updating-documents/#update-with-validation","text":"Updates trigger Pydantic validation: from pydantic import field_validator class Product(Document): name: str price: float stock: int @field_validator('price') @classmethod def price_must_be_positive(cls, v): if v < 0: raise ValueError('Price must be positive') return v product = await Product.get(\"product_id_123\") # This will raise ValidationError await product.update(price=-5.0) # \u274c ValidationError!","title":"Update with Validation"},{"location":"tutorial/updating-documents/#update-with-event-hooks","text":"Run custom logic before/after updates: from beanis import before_event, after_event, Update class Product(Document): name: str price: float old_price: float = 0.0 @before_event(Update) async def store_old_price(self): # Get current price before update existing = await Product.get(self.id) if existing: self.old_price = existing.price @after_event(Update) async def log_price_change(self): if self.old_price != self.price: print(f\"Price changed: ${self.old_price} -> ${self.price}\") product = await Product.get(\"product_id_123\") await product.update(price=7.99) # Output: Price changed: $5.99 -> $7.99","title":"Update with Event Hooks"},{"location":"tutorial/updating-documents/#update-with-ttl","text":"Update and set/modify expiration: # Update with new TTL await product.update(price=9.99) await product.set_ttl(3600) # Expire in 1 hour # Check TTL ttl = await product.get_ttl() print(f\"Expires in {ttl} seconds\") # Remove TTL (make permanent) await product.persist()","title":"Update with TTL"},{"location":"tutorial/updating-documents/#update-nested-objects","text":"Beanis stores nested Pydantic models as JSON strings in Redis: from pydantic import BaseModel class Category(BaseModel): name: str description: str class Product(Document): name: str price: float category: Category class Settings: name = \"products\" product = await Product.get(\"product_id_123\") # Update nested object (replaces entire category) await product.update( category=Category(name=\"Electronics\", description=\"Electronic devices\") ) # To modify nested field, recreate the object new_category = product.category new_category.description = \"Updated description\" await product.update(category=new_category)","title":"Update Nested Objects"},{"location":"tutorial/updating-documents/#important-notes","text":"update() replaces fields - Only specified fields are updated, others remain unchanged save() replaces entire document - All fields are overwritten Atomic operations - Use increment_field() for thread-safe numeric updates Validation always runs - Updates trigger Pydantic validation TTL is preserved - Unless explicitly changed, document TTL remains unchanged","title":"Important Notes"},{"location":"tutorial/updating-documents/#performance-tips","text":"Use increment_field() for counters - Atomic and efficient Batch updates when possible - Reduces Redis round trips Update only changed fields - Don't call update() with all fields Use set_field() for single fields - More efficient than loading entire document","title":"Performance Tips"},{"location":"tutorial/updating-documents/#examples","text":"","title":"Examples"},{"location":"tutorial/updating-documents/#inventory-management","text":"async def process_sale(product_id: str, quantity: int): \"\"\"Atomic stock update for sale\"\"\" product = await Product.get(product_id) # Check stock if product.stock < quantity: raise ValueError(\"Insufficient stock\") # Atomic decrement new_stock = await product.increment_field(\"stock\", -quantity) return new_stock # Process a sale remaining = await process_sale(\"prod_123\", quantity=5) print(f\"Stock remaining: {remaining}\")","title":"Inventory Management"},{"location":"tutorial/updating-documents/#price-updates-with-history","text":"from datetime import datetime from typing import List class PriceHistory(BaseModel): price: float timestamp: datetime class Product(Document): name: str price: float price_history: List[PriceHistory] = [] async def update_price(self, new_price: float): \"\"\"Update price and maintain history\"\"\" # Add current price to history self.price_history.append( PriceHistory(price=self.price, timestamp=datetime.now()) ) # Update to new price await self.update( price=new_price, price_history=self.price_history ) product = await Product.get(\"prod_123\") await product.update_price(7.99)","title":"Price Updates with History"},{"location":"tutorial/updating-documents/#conditional-updates","text":"async def apply_discount(product_id: str, discount_pct: float): \"\"\"Apply discount only if price is above threshold\"\"\" product = await Product.get(product_id) if product.price >= 10.0: new_price = product.price * (1 - discount_pct / 100) await product.update(price=round(new_price, 2)) return True return False # Apply 20% discount to expensive items discounted = await apply_discount(\"prod_123\", 20.0)","title":"Conditional Updates"},{"location":"tutorial/updating-documents/#next-steps","text":"Delete Operations - Remove documents Indexes - Query optimization Event Hooks - Document lifecycle events","title":"Next Steps"}]}